{"headings":["example---i-poisson-rate-parameter-with-gamma-prior","example---ii-normal-prior-for-normal-mean","example---iii-beta-prior-for-bernoullip","example---iv-generalization-of-hierarchical-bayes","example---v-gamma-prior-for-exponential-rate-parameter","exercises"],"options":{"chapters":true},"entries":[{"caption":"","order":{"section":[2,4,0,0,0,0,0],"number":3},"key":"eq-lik_nb_ex4"},{"caption":"\\mbox{MSE}_{\\theta}(\\hat{\\theta}) and \\mbox{MSE}_{\\theta}(\\hat{\\theta}_B) are plotted as a function of \\theta for different sample size n. The prior distribution of \\theta is considered to be \\mathcal{N}(\\mu=3, \\tau^2=0.5). The prior mean \\mu is indicated by a black dot. For sample size, n=4, \\mbox{MSE}_{\\theta}(\\hat{\\theta}_B) is smaller than \\mbox{MSE}_{\\theta}(\\hat{\\theta}) at all values of \\theta in a neighborhood of \\mu. As we move away from \\mu, \\hat{\\theta} is more preferable outsize the neighborhood of \\mu. However, as we increase n, MSE of both the estimators become closer in every neighborhood of \\mu (n=20), reducing the impact of prior information.","order":{"section":[2,2,0,0,0,0,0],"number":3},"key":"fig-1"},{"caption":"MSE of \\hat{\\lambda}=\\overline{X} and \\hat{\\lambda}_B at different values of \\lambda for different choices of the sample size, n. The prior distribution of \\lambda is chosen as gamma(4, \\frac{1}{2}). The prior mean is 2 and is indicated by a black dot. For small sample size n=10, the \\mbox{MSE}_{\\lambda}(\\hat{\\lambda}_B) is lower than the \\mbox{MSE}_{\\lambda}(\\hat{\\lambda}) for all true values of \\lambda in a neighborhood of 2. As \\lambda increases, after a certain value, \\mbox{MSE}_{\\lambda}(\\hat{\\lambda}) crosses the MSE of \\hat{\\lambda}. The same is observed for very small values of \\lambda. However, for large n values, the MSE of both the estimators merges, essentially the prior information becomes redundant (n = 50).","order":{"section":[2,1,0,0,0,0,0],"number":1},"key":"fig-mse_pois"},{"caption":"\\mbox{MSE}_p(\\hat{p})=\\mbox{MSE}_p(\\overline{X}) = \\frac{p(1 - p)}{n} and \\mbox{MSE}_p(\\hat{p}_B) =\\frac{np(1 - p) + \\left[\\alpha - p(\\alpha+\\beta)\\right]^2}{\\left(\\alpha+\\beta+n\\right)^2} are computed based on simulated data from a \\mbox{Bin}(n,p) distribution. The prior distribution \\pi(p) is assumed to be \\mbox{Beta}(\\alpha,\\beta). Here we consider \\alpha = \\beta=\\sqrt{\\frac{n}{4}} so that \\mbox{MSE}(\\hat{p}_B) = \\frac{n}{4\\left(n+\\sqrt{n}\\right)^2}, which is constant for all values of p (0<p<1), for fixed n. The computation was done for different sample sizes as depicted in the figure. At each value of p, average MSEs of both the estimators were computed using 1000 replications. The MSEs are plotted against different values of p. For small sample size n=4, \\hat{p_B} has more precision in estimating the true proportion than \\hat{p} for almost all p\\in (0,1) except the values closer to 0 and 1, where \\hat{p} is better. However, as sample size increases, the interval in which \\hat{p_B} is better than \\hat{p} decreases substantially. For n=400, \\hat{p_B} works better in a very small interval about 0.5.","order":{"section":[2,3,0,0,0,0,0],"number":5},"key":"fig-mse_binom"},{"caption":"A sample of size n = 5 were simulated from the Poisson distribution with parameter \\lambda =2 and the sample mean has been computed. The process has been replicated 100 times to obtain the sampling distribution of the sample meanand approximated by a kernel density estimator (magenta colour). The maximum likelihood estimate of \\lambda is denoted by magenta coloured `*â€™. Similarly exact prior density and exact posterior density functions are plotted using blue and red colour, respectively. Similarly the prior mean and posterior mean values are also marked. The depicted picture clearly verifies the theoretical calculations performed in the text.","order":{"section":[2,1,0,0,0,0,0],"number":2},"key":"fig-average"},{"caption":"","order":{"section":[2,3,0,0,0,0,0],"number":2},"key":"eq-MSE_EX3"},{"caption":"Histogram approximation of the posterior probability density of \\lambda for different posterior sample of size m. Red dot indicates the exact posterior mean (given data). Blue colored curve represents the exact posterior density function.","order":{"section":[2,5,0,0,0,0,0],"number":6},"key":"fig-Bayes_sim_gamma"},{"caption":"","order":{"section":[2,4,0,0,0,0,0],"number":4},"key":"eq-log_lik_nb_ex4"},{"caption":"The MSE of the two estimators are compared at different values of \\tau^2. For small sample size (n=4) and small \\tau^2 <\\sigma^2, \\hat{\\theta}_B performs better that \\hat{\\theta}. For large values of \\tau^2 > \\sigma^2, \\mbox{MSE}_{\\theta}(\\hat{\\theta}_B) increases. Basically, prior information becomes vague for large \\tau^2 values. As expected, for large sample size (n=20) both the estimators perform at par irrespective of the prior variance.","order":{"section":[2,2,0,0,0,0,0],"number":4},"key":"fig-2"},{"caption":"","order":{"section":[2,3,0,0,0,0,0],"number":1},"key":"eq-bayes-est-ex3"}]}