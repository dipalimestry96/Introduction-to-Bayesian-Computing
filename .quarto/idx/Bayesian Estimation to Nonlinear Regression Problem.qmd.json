{"title":"Bayesian Estimation to Nonlinear Regression Problem","markdown":{"yaml":{"format":{"html":{"html-math-method":"mathjax","include-in-header":[{"text":"<script>\nwindow.MathJax = {\n  tex: {\n    tags: 'all',\n    labels: {\n      sections: true\n    }\n  }\n};\n</script>\n"}],"number-sections":true}}},"headingText":"Bayesian Estimation to Nonlinear Regression Problem","containsRefs":false,"markdown":"\n\n\n\nLet $N(t)$ denotes the population size at time $t$. $N(t)$ is assumed to be a continuously differentiable function of $t$. Let the dynamics of the population is governed by the $\\theta$-logistic growth equation \n\\begin{equation}\\label{eq:growth}\n\\frac{\\mathrm{d}N(t)}{\\mathrm{d}t}=r_m N(t)\\left[1-\\left(\\frac{N(t)}{K}\\right)^{\\theta}\\right], ~ ~ ~ N(0)=N_0,\n\\end{equation} \nwhere $r_m$ is the intrinsic growth rate of the population, $\\theta$ is the curvature parameter depicting the shape of the per capita growth rate (PGR) and population density; $K$ is the carrying capacity of the environment. For $\\theta=1$, we get the logistic growth function, which is a linearly decreasing function of $N$. For $\\theta<1$ and $\\theta>1$, the PGR-density relationship is concave upward and concave downward, respectively. The equation \\eqref{eq:growth} is a Bernoulli type differential equation whose solution is given by the following: \n\n\\begin{equation}\nN(t)= \\frac{K}{\\left\\{1+  \\left[\\left(\\frac{K}{N_0}\\right)^{\\theta}-1 \\right]e^{-r_m t \\theta}\\right\\}^{\\frac{1}{\\theta}} }.\n\\end{equation} \n\nIt is easy to verify that as $t\\to \\infty$, $N(t) \\to K$. In ecological literature, demographic and environmental variations are main source of stochastic population changes. If the population is subject to the environmental noise alone then the corresponding diffusion equation is described in the form of It$\\hat{\\mbox{o}}$ stochastic differential equation (SDE) as \n\n\\begin{equation}\n\\mathrm{d}N(t)=rN(t)\\left[1-\\left(\\frac{N(t)}{K}\\right)^\\theta\\right]\\mathrm{d}t + \\sigma_eN(t)\\mathrm{d}W(t).\n\\end{equation}\n\nIf the dynamics of the population is subject to the demographic variation only, then the growth dynamics is governed by\n\n\\begin{equation}\\label{eq:SDE}\n\\mathrm{d}N(t) = rN(t)\\left[1-\\left(\\frac{N(t)}{K}\\right)^\\theta\\right]\\mathrm{d}t+\\sigma_d\\sqrt{N(t)}\\mathrm{d}W(t),\n\\end{equation}\n\n$\\sigma^2_d$ and $\\sigma^2_e$ are the demographic and environmental variances, respectively. $\\{W(t), t\\geq 0 \\}$ is the standard Brownian motion, so that $\\mathrm{d}W(t)\\sim \\mathcal{N}(0, \\mathrm{d}t)$. We consider the population dynamics to be subjected to environmental stochasticity only. We seek to estimate the model parameters by using the maximum likelihood estimation method. To do that we first simulate the data following the above model using some assumed values of the parameters. The equation \\eqref{eq:SDE} can be described as follows, \n\n\n\\begin{equation}\nN(t+\\Delta t) - N(t) = r N(t)\\left[1-\\left(\\frac{N(t)}{K}\\right)^\\theta\\right]\\Delta t + \\sigma_eN(t)\\left[W(t + \\Delta t) - W(t)\\right].\n\\end{equation}\n\n\nWe have $\\{t_0,t_1,\\cdots,t_n\\}$ be $n+1$ time points with $N(t_0) = X_0$, which is fixed. $W(t+\\Delta t)- W(t)$ is the Brownian motion with mean $0$ and variance $\\Delta t$. The simulation is carried out in the following way: The conditional distribution of $N(t +\\Delta t)$ given $N(t)$ is given by a normal distribution with mean $N(t) + rN(t)\\left[1-\\left(\\frac{N(t)}{K}\\right)^\\theta\\right]\\Delta t$ and variance $\\sigma_e^2N(t)^2\\Delta t$ and notionally can be written as follows: \n\n\\begin{equation}\nN(t + \\Delta t) -N(t)|N(t) \\sim \\mathcal{N}\\left(rN(t)\\left[1- \\left(\\frac{N(t)}{K}\\right)^{\\theta}\\right]\\Delta t, \\sigma_e^2 N(t)^2 \\Delta t\\right).\n\\end{equation}\n\nWe fixed $t_0 = 0$ and $\\Delta t =0.1$. Let $N_{t_0},N_{t_1},\\dots,N_{t_n}$ are the $n+1$ population sizes are generated using the above rule. The likelihood function $\\mathcal{L}(\\boldsymbol{\\beta})$ of the sample $N_0$, $N_1$, $N_2$, $\\cdots$, $N_n$ is the joint density of the observations treated as a function of the parameter $\\boldsymbol{\\beta}$. The method of maximum likelihood provides as estimate of $\\boldsymbol{\\beta}$ any value $\\hat{\\boldsymbol{\\beta}}$ which maximizes $\\mathcal{L}$. Note that $\\{N_{t_i}\\}_{i=0}^{n}$ are not independent but the differences are assumed to be independent, So the likelihood function can be written as \n\n\\begin{eqnarray*}\n\\mathcal{L} &=& f\\left(N_1,N_2,\\dots,N_n|N_0; r,K,\\theta,\\sigma_e\\right)\\\\\n&=& f\\left(N_1,N_2,\\dots,N_n|N_0;\\boldsymbol{\\beta}\\right)\\\\\n&=& f\\left(N_n|N_{n-1},\\dots,N_0;\\boldsymbol{\\beta}\\right)f\\left(N_{n-1}|N_{n-2},\\dots,N_0;\\tilde{\\beta}\\right)\\dots f\\left(N_1|N_0;\\tilde{\\beta}\\right)\\\\\n&=&\\prod_{i=1}^{n} f\\left(N_i|N_{i-1}\\dots,N_0;\\boldsymbol{\\beta}\\right)\\\\\n&=& \\prod_{i=1}^{n}f\\left(N_i|N_{i-1};\\beta\\right)\\\\\n&=& \\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi} \\sigma_eN_{i-1} {\\sqrt{\\Delta t_i}}}e^{-\\frac{1}{2\\sigma^2_e}\\Sigma_{i=1}^{n}\\left(\\frac{N_i-\\mu_{N_{i-1}}}{N_{i-1}}\\right)^2}\\\\\n&=& \\frac{1}{\\sqrt{2\\pi}^n(\\sigma_e)^n(\\prod_{i=1}^{n}N_{i-1} {\\sqrt{\\Delta t_i}})}e^{-\\frac{1}{2\\sigma^2_e}\\Sigma_{i=1}^{n}\\left(\\frac{N_i-\\mu_{N_{i-1}}}{N_{i-1}}\\right)^2}\n\\end{eqnarray*}\n\nwhere $\\mu_{N_i} =N(t_i) + r N(t_i)\\left[1-\\left(\\frac{N(t_i)}{K}\\right)^\\theta\\right]\\Delta t$. The log-likelihood function is given by\n\n```{=tex}\n\\begin{eqnarray}\nl(\\beta)&=&\\log \\mathcal{L}(\\beta)\\\\\n&=& -n\\ln \\sqrt{2\\pi}-n\\ln \\sigma_e-\\ln \\prod_{i=1}^{n}(N_{i-1}\\sqrt{\\Delta t_i})-\\frac{1}{2 \\sigma^2_e}\\sum_{i=1}^{n}\\left(\\frac{N_i - \\mu_{N_{i-1}}}{N_{i-1}}\\right)^2\\\\\n&=& -n \\ln (\\sqrt{2\\pi})- n \\ln \\sigma_e- \\sum_{i=1}^{n}\\ln \\sqrt{\\Delta t_i}-\\sum_{i=1}^{n}\\ln (N_{i-1})-\\frac{1}{2\\sigma^2_e}\\sum_{i=1}^{n}\\left[\\frac{N_i-N_{i-1}-r N_{i-1}\\left[1-\\left(\\frac{N_{i-1}}{K}\\right)^\\theta\\right]\\Delta t_i}{N_{i-1}}\\right]^2\n\\end{eqnarray}\n```\n\n## Estimation of Parameters {#sec-estimation_of_parameters}\n\nConsider $\\{ N_1,N_2,\\cdots,N_{n+1}\\}$ be the observed time series data. The absolute change in population size at time $t$ is approximated as\n$$\\frac{\\mathrm{d}N(t)}{\\mathrm{d}t} \\approx \\frac{N(t+\\Delta t)-N(t)}{\\Delta t}.$$ \nWe assume the yearly changes in population size, so that $\\Delta t=1$. The relative changes in population sizes is given by \n\n\\begin{equation}\nR(t) = \\frac{1}{N}\\frac{\\mathrm{d}N(t)}{\\mathrm{d}t} = \\frac{\\mathrm{d}}{\\mathrm{d}t} \\log N \\approx \\log N(t+1)- \\log N(t).\n\\end{equation} \n\nSo, for a given population time series data of size $n+1$, we have $n$ many observed per capita growth rates $\\{R(1),R(2),\\cdots,R(n)\\}$. We would like to investigate the dynamics of the population when it is subject to environmental stochastic perturbations alone. The following stochastic differential equation has been used to describe to population changes that is exposed to environmental variability. \n\n\\begin{equation}\\label{eq:theta logistic}\n\\mathrm{d}N = r_m N(t) \\left[ 1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right]\\mathrm{d}t + \\sigma_{e} N(t)\\mathrm{d}W(t) ,\n\\end{equation}\n\nwhere $\\sigma_{e}$ represents the intensity of the stochastic perturbation and $W(t)$ is the one dimensional Brownian motion which satisfies $\\mathrm{d}W(t) \\approx W(t+\\mathrm{d}t)-W(t) \\sim \\mathcal{N}(0,\\mathrm{d}t)$. Our goal is to estimate the parameters of the stochastic model by employing Bayesian statistical methods. First we compute the likelihood function of the model parameters given the population size. It is to be noted that the likelihood function can be written either by conditional on the population size $\\{N_t\\}$ or on the growth rates $\\{R_t\\}$. We can use the either observations on $\\{R_t\\}$ or $\\{N_t\\}$ to obtain the posterior distribution of the model parameters. Now, we shall write down the likelihood function given the population time series. Using the equation \\eqref{eq:theta logistic} and utilizing the distributional assumptions, we see that the absolute changes in the population size, conditional on the current size, is normally distributed. So we obtain the following (assuming $\\Delta t =1$):\n\n\\begin{align}\nN(t+1)-N(t)|N(t) &\\sim \\mathcal{N}\\left(r_m N(t) \\left[ 1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right], \\sigma_{e}^2 N(t)^2 \\right) \\notag \\\\\n\\frac{N(t+1)-N(t)}{N(t)}| N(t) &\\sim \\mathcal{N}\\left(r_m  \\left[ 1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right], \\sigma_{e}^2 \\right) \\notag \\\\\nN(i+1)-N(i) &\\sim \\mathcal{N}\\left(r_m N(i)\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta} \\right], \\sigma_{e}^2 N(i)^2 \\right) \\notag \\\\\nN(i+1)|N(i) &\\sim \\mathcal{N}\\left(N(i) + r_m N(i)\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta} \\right], \\sigma_{e}^2 N(i)^2 \\right) \\label{eq:N_i}\n\\end{align}\n\nWe can estimate the parameters using (\\ref{eq:N_i}) using the method of maximum likelihood. Therefore by using (\\ref{eq:N_i}), our goal is to estimate parameters $r_m$, $\\theta$, $K$, $\\sigma_e^2$. If we use $R_t$ values, then the following equation will be used for computation of the likelihood. \n\n\\begin{equation}\nR(t)|N(t)  \\sim \\mathcal{N}\\left(r_m\\left[1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right],\\sigma_{e} ^2\\right)  \\label{eq:R_T},\n\\end{equation} so that the log-likelihood function will be, \n\n\\begin{equation}\\label{eq:likelihood}\n\\mbox{likelihood} = -\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log(\\sigma_e^2)-\\frac{1}{2\\sigma_e^2}\\sum_{i=1}^{n}\\left\\lbrace R(i)- r_m\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta}\\right] \\right\\rbrace^2 .\n\\end{equation}\n\nTo draw inference about $r_m$, $\\theta$, $K$ and $\\sigma_e^2$, we consider inverted gamma prior for variance term $\\sigma_e^2$, gamma prior for $\\theta$, normal prior for $K$ and normal prior for $r_m$. Then the complete Bayesian model for this data can be written as, \n\n\\begin{align*}\nN(i+1)|N(i),r_m,\\theta,K,\\sigma_e^2 &\\sim \\mathcal{N}\\left(N(i) + r_m N(i)\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta} \\right], \\sigma_{e}^2 N(i)^2 \\right) \\\\\nr_m|\\alpha_0,\\beta_0 &\\sim \\mathcal{N}(\\alpha_0,\\beta_0)\\\\\nK| \\mu_K,\\sigma_K^2 &\\sim \\mathcal{N}(\\mu_K,\\sigma_K^2)\\\\\n\\theta|\\alpha_1,\\beta_1 &\\sim \\mathcal{G}(\\alpha_1,\\beta_1)  \\\\\n\\sigma_e^2|\\alpha_2,\\beta_2 &\\sim \\mathcal{IG}(\\alpha_2,\\beta_2),\n\\end{align*}\n\nwhere $\\alpha_0,\\alpha_1,\\alpha_2,\\beta_0,\\beta_1,\\beta_2,\\mu_K$, and $\\sigma_K^2$ are hyper-parameters and assumed to be known. As the starting population size is known, $P(N(1)=N_0)=1$. We take $N(1)=N_0$ as the starting population size and assumed to be a fixed quantity. The joint posterior distribution of $r_m,\\theta,K,\\sigma_e^2$ can be written as, \n\n\\begin{equation*}\nf(r_m,\\theta,K,\\sigma_e ^2|\\boldsymbol{N})=\\frac{f(\\boldsymbol{N},r_m,\\theta,K,\\sigma_e ^2)}{f_{\\boldsymbol{N}}(\\boldsymbol{N})},\n\\end{equation*} where $\\boldsymbol{N}=(N(1),N(2),\\cdots,N(n+1))'$ represents the data and $f_{\\boldsymbol{N}}(\\boldsymbol{N})$ is the marginal distribution of $\\boldsymbol{N}$.\n\n\\begin{equation}\\label{eq:27}\nf\\left(r_m,\\theta,K,\\sigma_e^2|\\boldsymbol{N}\\right) = \\frac{f\\left(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2\\right)\\cdot f\\left(r_m,\\theta,K,\\sigma_e^2\\right)}{f(\\boldsymbol{N})}.\n\\end{equation} We assume that $r_m,\\theta,K,\\sigma_e^2$ are independent variables, therefore $f(r_m,\\theta,K,\\sigma_e ^2)$ can be written as, \n\n\\begin{equation}\nf\\left(r_m,\\theta,K,\\sigma_e^2\\right)=f(r_m)\\cdot f(\\theta) \\cdot f(K) \\cdot f\\left(\\sigma_e^2\\right).\n\\end{equation} So, equation (\\ref{eq:27}) becomes, \n\\begin{eqnarray*}\nf\\left(r_m,\\theta,K,\\sigma_e^2|\\boldsymbol{N}\\right) &\\propto& f\\left(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2\\right)\\cdot f\\left(r_m,\\theta,K,\\sigma_e^2\\right) \\\\\n&\\propto& f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e ^2)\\cdot f(r_m)\\cdot f(\\theta) \\cdot f(K) \\cdot f(\\sigma_e ^2)~~[\\mbox{independence of priors}]  \\\\\n&\\propto&  \\left[\\prod_{i=1}^{n} f\\left(N(i+1)|N(i)\\right)\\right]\\cdot f(r_m)\\cdot f(\\theta) \\cdot f(K) \\cdot f(\\sigma_e ^2).\n\\end{eqnarray*}\n\nThe right hand side of the above expression is product of the likelihood and the prior, which is nothing but unnormalized joint posterior distribution of parameters. Since, the distribution does not follow some common known distribution, we use the Gibbs sampling method that simulates samples from the conditional distribution. So we generate random sample from conditional posterior distribution of each parameters. However, it is observed that in this case also the conditional posterior does not follow any known distribution. So, we use grid approximation algorithm to approximate the posterior probability distribution.\n\nLet $\\boldsymbol{\\beta} = \\left(r_m,\\theta, K, \\sigma_e^2\\right)$. Since, $N(i)$'s are not independent, so the likelihood can be written as,\n\n\\begin{eqnarray*}\n\\mbox{likelihood} &=& f\\left(N(1),N(2),\\cdots , N(n+1) ; \\beta \\right)   \\\\\n&=& f\\left(N(n+1)|N(n) ; \\beta \\right)\\cdot f\\left(N(n)|N(n-1) ; \\beta \\right) \\cdots f\\left(N(2)|N(1) ; \\beta \\right)\\cdot f\\left(N(1) ; \\beta \\right) \\\\\n&=& \\prod_{i=1}^{n} f\\left(N(i+1)|N(i) ; \\beta \\right)\\cdot f\\left(N(1); \\beta \\right).\n\\end{eqnarray*}\n\nWe assume that the initial population size to be known so that $f\\left(N(1);\\beta \\right) = 1$. \n\n\\begin{eqnarray*}\n\\mbox{likelihood} &=& \\prod_{i=1}^{n} f\\left(N(i+1)|N(i) ; \\beta \\right) \\\\\n&=& \\prod_{i=1}^{n}\\left[\\frac{1}{\\sqrt[]{2\\pi}\\sigma_e^2 N(i)} e^{-\\frac{\\left\\lbrace N(i+1)-N(i)- r_m N(i)\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta} \\right]\\right\\rbrace^2} {2\\sigma_e^2 N(i)^2}}\\right]\\\\\n&=&\\prod_{i=1}^{n} \\left[\\frac{1}{\\sqrt[]{2\\pi}\\sigma_e^2 N(i)} e^{{-\\frac{1}{2\\sigma_e^2}\\left\\lbrace \\frac{N(i+1)-N(i)}{N(i)}-r_m\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta}\\right] \\right\\rbrace}^{\\theta} }\\right]\\\\\n&=& \\frac{1}{(\\sqrt[]{2\\pi})^n}\\frac{1}{(\\sigma_e)^n}\\frac{1}{\\prod_{i=1}^{n} N(i)} e^{-\\frac{1}{2\\sigma_e^2}\\sum_{i=1}^{n}\\left\\lbrace R(i)- r_m\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta}\\right]  \\right\\rbrace^2}.\n\\end{eqnarray*}\n\nThe log-likelihood can be given as, \n\n\\begin{equation}\n\\mbox{log-likelihood} = -\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log(\\sigma_e^2)-\\sum_{i=1}^{n}\\log(N(i))-\\frac{1}{2\\sigma_e^2}\\sum_{i=1}^{n}\\left\\lbrace R(i)- r_m\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta}\\right] \\right\\rbrace^2.\n\\end{equation}\n\nTo compute the conditional posterior following process has been carried out,\n\n\\begin{eqnarray*}\nf(r_m|\\theta,K,\\sigma_e^2,\\boldsymbol{N})&=&\\frac{f(\\boldsymbol{N},r_m,\\theta,K,\\sigma_e^2)}{f(\\theta,K,\\sigma_e^2,\\boldsymbol{N})}\\\\\n&=& \\frac{f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2)\\cdot f(r_m,\\theta,K,\\sigma_e^2)}{f(\\boldsymbol{N},\\theta,K,\\sigma_e^2)}\\\\\n&=&\\frac{f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2)\\cdot\\pi(r_m)\\pi(\\theta)\\pi(K)\\pi(\\sigma_e^2)}{f(\\boldsymbol{N}|\\theta,K,\\sigma_e^2)\\cdot f(\\theta,K,\\sigma_e^2)}\\\\\n&=&\\frac{f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2)\\cdot \\pi(r_m)\\cdot \\pi(\\theta)\\cdot \\pi(K)\\cdot\\pi(\\sigma_e^2)}{f(\\boldsymbol{N}|\\theta,K,\\sigma_e^2)\\cdot \\pi(\\theta)\\cdot \\pi(K)\\cdot\\pi(\\sigma_e^2)}\\\\\n&=&\\frac{f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2)\\cdot \\pi(r_m)}{f(\\boldsymbol{N}|\\theta,K,\\sigma_e^2)}.\n\\end{eqnarray*} Let $m=f(\\boldsymbol{N}|\\theta,K,\\sigma_e^2)$, be the joint marginal probability density function of the data. \\begin{eqnarray}\nf(r_m|\\theta,K,\\sigma_e^2,\\boldsymbol{N}) &\\propto& f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2)\\cdot \\pi(r_m) \\notag\\\\\n&\\propto& \\prod_{i=1}^{n} f(N_{i+1}|N_i ; r_m,\\theta,K,\\sigma_e^2)\\cdot  \\pi(r_m).\n\\end{eqnarray} \nSimilarly, \n\n\\begin{eqnarray}\nf(\\theta|r_m,K,\\sigma_e^2,\\boldsymbol{N})&\\propto& \\prod_{i=1}^{n} f(N_{i+1}|N_i ; r_m,\\theta,K,\\sigma_e^2)\\cdot  \\pi(\\theta)\\\\\nf(K|r_m,\\theta,\\sigma_e^2,\\boldsymbol{N})&\\propto& \\prod_{i=1}^{n} f(N_{i+1}|N_i ; r_m,\\theta,K,\\sigma_e^2)\\cdot  \\pi(K)\\\\\nf(\\sigma_e^2|r_m,\\theta,K,\\boldsymbol{N}) &\\propto& \\prod_{i=1}^{n} f(N_{i+1}|N_i ; r_m,\\theta,K,\\sigma_e^2)\\cdot  \\pi(\\sigma_e^2).\n\\end{eqnarray}\n\nIn Bayesian linear regression we already verify that the variance (specially $\\phi$) after observed data follows $\\mathcal{IG}\\left( \\mbox{shape} = \\alpha+\\frac{n}{2}, \\mbox{rate} =\\frac{1}{2} \\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1x_i\\right)^2 + \\gamma\\right)$. Similarly here we expect posterior distribution of variance($\\sigma_e^2$) as follows:\n\n\\begin{equation}\\label{eq:post_sigma_sq}\n\\sigma_e^2|\\boldsymbol{N} \\sim \\mathcal{IG}\\left( \\mbox{shape} = \\alpha_2 +\\frac{n}{2}, \\mbox{rate} =\\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i- r_m \\left\\lbrace 1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right\\rbrace\\right)^2+\\beta_2\\right).\n\\end{equation}\n\n:::{.callout-note}\nIf the prior distribution $\\pi(r_m)$ is considered to be $\\mbox{gammma}(\\alpha, \\beta)$ which takes only positive values, then task of estimation of a declining population may be underestimated.\n:::\n\n:::{.callout-note}\nTo reduce  the  computational time a correct choice of priors is required. In this case, since $K$ is a property of the environment, it should not change drastically. Posterior sample of k should not vary too much away from the prior mean of $K$.\n:::\n\n:::{.callout-note}\nIf the prior distribution $\\pi(r_m)$ is considered to be $\\text{gamma}(\\alpha, \\beta)$ which takes only positive values, then the task of estimation of a declining population may be underestimated.\n:::\n\n:::{.callout-note}\nTo reduce the computational time, a correct choice of priors is required. In this case, since $K$ is a property of the environment, it should not change drastically. The posterior sample of $K$ should not vary too much from the prior mean of $K$.\n:::\n\nThe above structure of the posterior density function of $\\sigma_e^2$ given in the equation \\eqref{eq:post_sigma_sq} is verified by the simulation for the simulated data.\n\n## Simulation study\n\n### Data Generation\n\nTime series data of population size is generated using $r_m=0.5,~~ \\theta=0.5,~~K=50,~~\\sigma_e^2=0.0025$. The initial population size is set as $N_0 = 23$ and the length of the simulated series is $n=30$. The full Bayesian model is described as follows: \n\n\\begin{align}\nR(t)|N(t) &\\sim  \\mathcal{N}\\left(r_m\\left[1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right],\\sigma_{e} ^2\\right)\\label{eq:logistic model} \\\\\nr_m|\\alpha_0,\\beta_0 &\\sim \\mathcal{N}(\\alpha_0,\\beta_0)\\notag\\\\\nK| \\mu_K,\\sigma_K^2 &\\sim  \\mathcal{N}(\\mu_K,\\sigma_K^2)\\notag\\\\\n\\theta|\\alpha_1,\\beta_1 &\\sim \\mathcal{G}(\\alpha_1,\\beta_1)\\notag\\\\\n\\sigma_e^2|\\alpha_2,\\beta_2 &\\sim \\mathcal{IG}(\\alpha_2,\\beta_2).\\notag\n\\end{align}\n\n```{r , eval=TRUE, cache=TRUE}\n#| label: fig-simulated_data\n#| fig-cap: \"Plot of simulated data for the study of Nonlinear regression generated using the model given in the equation (\\\\ref{eq:logistic model}). The parameters values are fixed as $r_m=0.5, \\\\theta=0.5, K=50, \\\\sigma_e^2=0.0025$, and initial population size taken as $N_0=23$, generated data of length $n=30$.\"\n\n# parameters values for simulating the data\nrm = 0.5 # intrinsic growth rate\ntheta = 0.5 # parameter for strength of density dependence\nK = 50 # carrying capacity of the environment\nsig_2e = 0.0025 # variance of the environmental perturbation\nn = 30 # length of the time series = n+1\nN1 = 23 # initial population size\n\nset.seed(471)\nN = numeric(n + 1) # storage the simulated time series\nN[1] = N1 # initial population size.\nfor(i in 1:(length(N)-1)){\n  r = rnorm(n =1, mean = rm*(1-(N[i]/K)^theta), sd = sqrt(sig_2e)) #   simulating growth rate\n  N[i+1] = N[i]*exp(r) # simulate next generation\n}\n#print(N) # printing population size\nplot(1:(n+1), N, type= \"p\", lwd=2, col = \"red\", pch = 19)\n\nfun_logistic = function(x){ # Deterministic solution of the generalized logistic equation\nK/(1+((K/N1)^theta-1 )*exp(-rm*x*theta) )^(1/theta)\n}\ncurve(fun_logistic, 0, n+1, add= TRUE, lwd=2)\n```\n\n### Define Prior distributions\nFor generated time series, we want to generate values from posterior of each parameter$(r_m,K,\\theta,\\sigma_e^2)$. For simulation purpose, we first define the prior functions for the prior distribution given in the equation (\\ref{eq:logistic model}) by considering the values of hyper-parameters as $\\beta_0=1, \\sigma_K^2=3, \\alpha_1=1, \\beta_1=1, \\alpha_2=1, \\beta_2= 0.0001$. Prior mean of $K$ and $r_m$ that is $\\mu_K$ and $\\alpha_0$ are obtained by fitting the logistic growth equation using nonlinear least squares method ($\\texttt{nls}$, in $\\texttt{R}$). The package $\\texttt{invgamma}$ [@invgamma] has been used for generating random numbers from the inverted gamma distribution.\n\n```{r , eval=FALSE, cache=TRUE}\nprior_mean_rm = coef(fit)[1]\nprior_sd_rm = 1\nfun_prior_rm = function(x){ # Prior specification for r_m\n  dnorm(x, prior_mean_rm, prior_sd_rm)\n}\n\nprior_mean_K = coef(fit)[2]\nprior_sd_K = 3\nfun_prior_K = function(x){ # Prior specification for K\n  dnorm(x, prior_mean_K, prior_sd_K)\n}\n\nalpha_1 = 1\nbeta_1 = 1\nfun_prior_theta = function(x){ # Prior specification for theta\n  dgamma(x, shape = alpha_1, rate = beta_1)\n}\n\nlibrary(invgamma)\nalpha_2 = 1\nbeta_2 = 0.0001\nfun_prior_sig_2e = function(x){ # Prior specification for inverted gama\n  dinvgamma(x, shape = alpha_2, rate = beta_2)\n}\n```\n\nAs mentioned in the @sec-estimation_of_parameters, the exact form of the conditional posterior distribution is not known, so grid approximation has been utilized to approximate the conditional posterior density function. In the following code, we have created grid within an interval specified for each parameter. This initial choice is important as this act as a support for the density function. Finer the grid is more accurate is the posterior approximation.\n\n```{r , eval=FALSE, cache=TRUE}\n# Specification of grid values for approximating the posterior\nstep = 0.01\ngrid_rm = seq(from = -3, to =3, by = step)\ngrid_K = seq(from = 40, to = 60, by = step)\ngrid_theta = seq(from = 0, to =10, by = step)\ngrid_sig_2e = seq(from = 0.0001 , to =0.01, by = 0.00001)\n```\n\n### Likelihood function\n\nIn the following code we define the likelihood function. Note that we have directly computed the log-likelihood\nvalues to avoid truncation error.\n\n```{r , eval=FALSE, cache=TRUE}\nlikelihood = function(data, param){\n  rm = param[1]\n  K = param[2]\n  theta = param[3]\n  sig_2e = param[4]\n\n  x = data[,1] # population size\n  y = data[,2] # per capita growth rates\n\n  log_lik = -n*log(sqrt(2*pi)) - (n/2)*log(sig_2e) - sum(((y-rm*(1-(x/K)^theta))^2/(2*sig_2e)))\n  return(exp(log_lik))\n}\n\n```\n\n### Calculation of Posterior distribution\n\nFirst of all we have set 100000 as a number Markov Chain samples to be generated. Then we have fixed the initial values of the parameters and defined the arrays for storing the posterior values of the parameters in $\\texttt{R}$ as follows:\n\n```{r , eval=FALSE, cache=TRUE}\niter = 100000 # Length of the chain to be simulated\npost_rm = rep(NA, iter) # posterior values of r_m\npost_K = rep(NA, iter) # posterior values of K\npost_theta = rep(NA, iter) # posterior values of theta\npost_sig_2e = rep(NA, iter) # posterior values of simga_2e\n\nparam = c(0.2, 40, 1, 0.02) # starting of the chain\n\n# initialization of the chain\npost_rm[1] = param[1]\npost_K[1] = param[2]\npost_theta[1] = param[3]\npost_sig_2e[1] = param[4]\n```\n\nNext we carried out the Gibbs sampling and grid approximation of the posterior distribution. At each Gibbs sampling step (outer loop), the conditional posterior has been approximated by grid approximation (four inner loop for four parameters).\n\n```{r , eval=FALSE, cache=TRUE}\nfor(i in 2:iter){ # loop for iteration of Gibbs sampling\n  # section for rm\n  tmp_post_rm = numeric(length = length(grid_rm))\n  for(j in 1:length(grid_rm)){\n      param = c(grid_rm[j], post_K[i-1], post_theta[i-1], post_sig_2e[i-1])\n      tmp_post_rm[j] = likelihood(data = data, param = param)*fun_prior_rm(grid_rm[j])\n    }\n  prob = tmp_post_rm/sum(tmp_post_rm)\n  post_rm[i] = sample(grid_rm, size = 1, prob = prob)\n\n  # section for K\n  tmp_post_K = numeric(length = length(grid_K))\n  for(j in 1:length(grid_K)){\n    param = c(post_rm[i], grid_K[j], post_theta[i-1], post_sig_2e[i-1])\n    tmp_post_K[j] = likelihood(data = data, param = param)*fun_prior_K(grid_K[j])\n    }\n  prob = tmp_post_K/sum(tmp_post_K)\n  post_K[i] = sample(grid_K, size = 1, prob = prob)\n\n  # section for theta\n  tmp_post_theta = numeric(length = length(grid_theta))\n  for(j in 1:length(grid_theta)){\n    param = c(post_rm[i], post_K[i], grid_theta[j], post_sig_2e[i-1])\n    tmp_post_theta[j] = likelihood(data = data, param = param)*fun_prior_theta(grid_theta[j])\n    }\n  prob = tmp_post_theta/sum(tmp_post_theta)\n\n  post_theta[i] = sample(grid_theta, size = 1, prob = prob)\n\n  # section for sig_2e\n  tmp_post_sig_2e = numeric(length = length(grid_sig_2e))\n  for(j in 1:length(grid_sig_2e)){\n    param = c(post_rm[i], post_K[i], post_theta[i], grid_sig_2e[j])\n    tmp_post_sig_2e[j] = likelihood(data = data, param = param)*fun_prior_sig_2e(grid_sig_2e[j])\n    }\n  prob = tmp_post_sig_2e/sum(tmp_post_sig_2e)\n  post_sig_2e[i] = sample(grid_sig_2e, size = 1, prob = prob)\n\n} # loop ends for Gibbs iteration\n```\n\nBy executing the above code, we have obtained the 1,00,000 sample values from the posterior density of $r_m, K, \\theta$, and $\\sigma_e^2$. We stored them in the local directory on system for the further analysis.\n\n```{r , eval=FALSE, cache=TRUE}\npost_param_vals = data.frame(post_rm,post_K,post_theta,post_sig_2e)\nsetwd(\"specify path here\")\nfilename = paste0(\"output\",seed, \".txt\")\nwrite.table(post_param_vals, file = filename, col.names = TRUE)\n```\n\nTrace plot is a key diagnostic tool used in Bayesian statistics to assess the behavior and convergence of MCMC chains. So we have plotted the traceplot of posterior values obtained using Grid and Gibbs approximation using following code:\n\n```{r , eval=FALSE, cache=TRUE}\nfilename = paste0(\"output\",seed, \".txt\")\npost_vals = read.table(file = filename, header = TRUE)\n\n#traceplot\npar(mfrow=c(2,2))\nplot(post_vals$post_rm, type = \"l\", main = bquote(\"Trace plot of\"~ r[m]),col = \"red\")\nplot(post_vals$post_K, type = \"l\", main = bquote(\"Trace plot of\"~ K),col = \"red\")\nplot(post_vals$post_theta, type = \"l\", main = bquote(\"Trace plot of\"~ theta),col = \"red\")\nplot(post_vals$post_sig_2e, type = \"l\", main = bquote(\"Trace plot of\"~ sigma[e]^2),col = \"red\")\n```\n\nWe have kept initial half i.e 50,000 sample values as a burn-in period for each parameter. After the burn-in period from remaining 50,000 sample posterior values, using appropriate step for thinning (using auto-correlation function, $\\texttt{acf}) we have reduced the autocorrelation between the successive samples to get reliable estimate of parameters. Next we have plot the histogram of posterior distribution of all parameters using following code and depicted in the @fig-posterior.\n\n```{r , eval=FALSE, cache=TRUE}\ncut = 0.5 # burn-in chain (first 50,000 values as burn-in period)\n\n# for rm\nu = post_vals$post_rm[ceiling(iter*cut):iter] # after burn in period\nindex = seq(1,length(u), by = 20) # thining\npost_rm_thinning = u[index] # values after thinning\nacf(post_rm_thinning,main = bquote(\"acf of posterior \"~rm)) # autocorrelation\npost_interval_rm = quantile(post_rm_thinning, c(2.5, 97.5)/100) #credible interval for rm\nhist(post_rm_thinning, probability = TRUE,main = bquote(\"Posterior of \"~ r[m]),\nxlab = expression(r[m]),col = \"grey\", breaks = 40)\narrows(post_interval_rm[1], 0, post_interval_rm[2], 0, lwd=3, angle = 90,col = \"red\", code = 1)\narrows(post_interval_rm[2], 0, post_interval_rm[1], 0, lwd=3, angle = 90,col = \"red\", code = 1)\nlegend(\"topright\",legend = c(\"Credible Interval\"),lwd = 3, col = \"red\",lty = 1,cex = 1, bty = \"n\")\n\n# for K\nu = post_vals$post_K[ceiling(iter*cut):iter] # after burn in period\nindex = seq(1,length(u), by = 10) # thining\npost_K_thinning = u[index] # values after thinning\nacf(post_K_thinning,main = bquote(\"acf of posterior \"~K)) # autocorrelation\npost_interval_K = quantile(post_K_thinning, c(2.5, 97.5)/100) #credible interval for K\nhist(post_K_thinning, probability = TRUE,main = bquote(\"Posterior of \"~ K),\nxlab = expression(K),col = \"grey\", breaks = 40)\narrows(post_interval_K[1], 0, post_interval_K[2], 0, lwd=3, angle = 90, col = \"red\", code = 1)\narrows(post_interval_K[2], 0, post_interval_K[1], 0, lwd=3, angle = 90, col = \"red\", code = 1)\nlegend(\"topright\",legend = c(\"Credible Interval\"),lwd = 3, col = \"red\",lty = 1,cex = 1, bty = \"n\")\n\n# for theta\nu = post_vals$post_theta[ceiling(iter*cut):iter] # after burn in period\nindex = seq(1,length(u), by = 20) # thining\npost_theta_thinning = u[index] # values after thinning\nacf(post_theta_thinning,main = bquote(\"acf of posterior \"~theta)) # autocorrelation\npost_interval_theta = quantile(post_theta_thinning, c(2.5, 97.5)/100) #credible interval for theta\nhist(post_theta_thinning, probability = TRUE,main = bquote(\"Posterior of \"~ theta),xlab = expression(theta),col = \"grey\", breaks = 40)\narrows(post_interval_theta[1], 0, post_interval_theta[2], 0, lwd=3, angle = 90, col = \"red\", code = 1)\narrows(post_interval_theta[2], 0, post_interval_theta[1], 0, lwd=3, angle = 90, col = \"red\", code = 1)\nlegend(\"topright\",legend = c(\"Credible Interval\"),lwd = 3, col = \"red\",lty = 1,cex = 1, bty = \"n\")\n\n# for sigma_e^2\nu = post_vals$post_sig_2e[ceiling(iter*cut):iter] # after burn in period\nindex = seq(1,length(u), by = 1) # thining\npost_sig_2e_thinning = u[index] # values after thinning\nacf(post_sig_2e_thinning,main = bquote(\"acf of posterior \"~sigma[e]^2))\npost_interval_sig_2e = quantile(post_sig_2e_thinning, c(2.5, 97.5)/100)\n#credible interval for sig_2e\nhist(post_sig_2e_thinning, probability = TRUE,main = bquote(\"Posterior of \"~ sigma[e]^2),xlab = expression(sigma[e]^2),col = \"grey\", breaks = 40, ylim = c(0,800))\narrows(post_interval_sig_2e[1], 0, post_interval_sig_2e[2], 0, lwd=3, angle = 90, col = \"red\", code = 1)\narrows(post_interval_sig_2e[2], 0, post_interval_sig_2e[1], 0, lwd=3, angle = 90, col = \"red\", code = 1)\nlegend(\"topright\",legend = c(\"Credible Interval\"),lwd = 3, col = \"red\",lty = 1,cex = 1, bty = \"n\")\n```\n\nFor $\\sigma_e^2$, due to conjugacy the posterior density is predictable, but good approximation would depend on the estimate of the shape parameter which involves other parameters. Here we show what choice of the point estimate for $r_m, K$ and $\\theta$ need to be used (@fig-4) and we observe that the posterior median values best approximate the posterior density function of $\\sigma_e^2$.\n\n\n::: {#fig-posterior layout-ncol=2}\n\n![](./Chapter 4_Bayesian Estimation to Nonlinear Regression Problem_Figures/hist_rm.png){#fig-1}\n\n![](./Chapter 4_Bayesian Estimation to Nonlinear Regression Problem_Figures/hist_K.png){#fig-2}\n\n![](./Chapter 4_Bayesian Estimation to Nonlinear Regression Problem_Figures/hist_theta.png){#fig-3}\n\n![](./Chapter 4_Bayesian Estimation to Nonlinear Regression Problem_Figures/hist_sigma_sq_with_3_fitted_curve.png){#fig-4}\n\n\nPosterior distribution of the parameters of theta-logistic growth equation. From the histogram of $\\sigma_e^2$, it is evident that posterior median is more appropriate choice to obtain a point estimate of $r_m$, $K$ and $\\theta$. Blue lines indicate the 95\\% of credible intervals.\n:::","srcMarkdownNoYaml":"\n\n\n# Bayesian Estimation to Nonlinear Regression Problem\n\nLet $N(t)$ denotes the population size at time $t$. $N(t)$ is assumed to be a continuously differentiable function of $t$. Let the dynamics of the population is governed by the $\\theta$-logistic growth equation \n\\begin{equation}\\label{eq:growth}\n\\frac{\\mathrm{d}N(t)}{\\mathrm{d}t}=r_m N(t)\\left[1-\\left(\\frac{N(t)}{K}\\right)^{\\theta}\\right], ~ ~ ~ N(0)=N_0,\n\\end{equation} \nwhere $r_m$ is the intrinsic growth rate of the population, $\\theta$ is the curvature parameter depicting the shape of the per capita growth rate (PGR) and population density; $K$ is the carrying capacity of the environment. For $\\theta=1$, we get the logistic growth function, which is a linearly decreasing function of $N$. For $\\theta<1$ and $\\theta>1$, the PGR-density relationship is concave upward and concave downward, respectively. The equation \\eqref{eq:growth} is a Bernoulli type differential equation whose solution is given by the following: \n\n\\begin{equation}\nN(t)= \\frac{K}{\\left\\{1+  \\left[\\left(\\frac{K}{N_0}\\right)^{\\theta}-1 \\right]e^{-r_m t \\theta}\\right\\}^{\\frac{1}{\\theta}} }.\n\\end{equation} \n\nIt is easy to verify that as $t\\to \\infty$, $N(t) \\to K$. In ecological literature, demographic and environmental variations are main source of stochastic population changes. If the population is subject to the environmental noise alone then the corresponding diffusion equation is described in the form of It$\\hat{\\mbox{o}}$ stochastic differential equation (SDE) as \n\n\\begin{equation}\n\\mathrm{d}N(t)=rN(t)\\left[1-\\left(\\frac{N(t)}{K}\\right)^\\theta\\right]\\mathrm{d}t + \\sigma_eN(t)\\mathrm{d}W(t).\n\\end{equation}\n\nIf the dynamics of the population is subject to the demographic variation only, then the growth dynamics is governed by\n\n\\begin{equation}\\label{eq:SDE}\n\\mathrm{d}N(t) = rN(t)\\left[1-\\left(\\frac{N(t)}{K}\\right)^\\theta\\right]\\mathrm{d}t+\\sigma_d\\sqrt{N(t)}\\mathrm{d}W(t),\n\\end{equation}\n\n$\\sigma^2_d$ and $\\sigma^2_e$ are the demographic and environmental variances, respectively. $\\{W(t), t\\geq 0 \\}$ is the standard Brownian motion, so that $\\mathrm{d}W(t)\\sim \\mathcal{N}(0, \\mathrm{d}t)$. We consider the population dynamics to be subjected to environmental stochasticity only. We seek to estimate the model parameters by using the maximum likelihood estimation method. To do that we first simulate the data following the above model using some assumed values of the parameters. The equation \\eqref{eq:SDE} can be described as follows, \n\n\n\\begin{equation}\nN(t+\\Delta t) - N(t) = r N(t)\\left[1-\\left(\\frac{N(t)}{K}\\right)^\\theta\\right]\\Delta t + \\sigma_eN(t)\\left[W(t + \\Delta t) - W(t)\\right].\n\\end{equation}\n\n\nWe have $\\{t_0,t_1,\\cdots,t_n\\}$ be $n+1$ time points with $N(t_0) = X_0$, which is fixed. $W(t+\\Delta t)- W(t)$ is the Brownian motion with mean $0$ and variance $\\Delta t$. The simulation is carried out in the following way: The conditional distribution of $N(t +\\Delta t)$ given $N(t)$ is given by a normal distribution with mean $N(t) + rN(t)\\left[1-\\left(\\frac{N(t)}{K}\\right)^\\theta\\right]\\Delta t$ and variance $\\sigma_e^2N(t)^2\\Delta t$ and notionally can be written as follows: \n\n\\begin{equation}\nN(t + \\Delta t) -N(t)|N(t) \\sim \\mathcal{N}\\left(rN(t)\\left[1- \\left(\\frac{N(t)}{K}\\right)^{\\theta}\\right]\\Delta t, \\sigma_e^2 N(t)^2 \\Delta t\\right).\n\\end{equation}\n\nWe fixed $t_0 = 0$ and $\\Delta t =0.1$. Let $N_{t_0},N_{t_1},\\dots,N_{t_n}$ are the $n+1$ population sizes are generated using the above rule. The likelihood function $\\mathcal{L}(\\boldsymbol{\\beta})$ of the sample $N_0$, $N_1$, $N_2$, $\\cdots$, $N_n$ is the joint density of the observations treated as a function of the parameter $\\boldsymbol{\\beta}$. The method of maximum likelihood provides as estimate of $\\boldsymbol{\\beta}$ any value $\\hat{\\boldsymbol{\\beta}}$ which maximizes $\\mathcal{L}$. Note that $\\{N_{t_i}\\}_{i=0}^{n}$ are not independent but the differences are assumed to be independent, So the likelihood function can be written as \n\n\\begin{eqnarray*}\n\\mathcal{L} &=& f\\left(N_1,N_2,\\dots,N_n|N_0; r,K,\\theta,\\sigma_e\\right)\\\\\n&=& f\\left(N_1,N_2,\\dots,N_n|N_0;\\boldsymbol{\\beta}\\right)\\\\\n&=& f\\left(N_n|N_{n-1},\\dots,N_0;\\boldsymbol{\\beta}\\right)f\\left(N_{n-1}|N_{n-2},\\dots,N_0;\\tilde{\\beta}\\right)\\dots f\\left(N_1|N_0;\\tilde{\\beta}\\right)\\\\\n&=&\\prod_{i=1}^{n} f\\left(N_i|N_{i-1}\\dots,N_0;\\boldsymbol{\\beta}\\right)\\\\\n&=& \\prod_{i=1}^{n}f\\left(N_i|N_{i-1};\\beta\\right)\\\\\n&=& \\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi} \\sigma_eN_{i-1} {\\sqrt{\\Delta t_i}}}e^{-\\frac{1}{2\\sigma^2_e}\\Sigma_{i=1}^{n}\\left(\\frac{N_i-\\mu_{N_{i-1}}}{N_{i-1}}\\right)^2}\\\\\n&=& \\frac{1}{\\sqrt{2\\pi}^n(\\sigma_e)^n(\\prod_{i=1}^{n}N_{i-1} {\\sqrt{\\Delta t_i}})}e^{-\\frac{1}{2\\sigma^2_e}\\Sigma_{i=1}^{n}\\left(\\frac{N_i-\\mu_{N_{i-1}}}{N_{i-1}}\\right)^2}\n\\end{eqnarray*}\n\nwhere $\\mu_{N_i} =N(t_i) + r N(t_i)\\left[1-\\left(\\frac{N(t_i)}{K}\\right)^\\theta\\right]\\Delta t$. The log-likelihood function is given by\n\n```{=tex}\n\\begin{eqnarray}\nl(\\beta)&=&\\log \\mathcal{L}(\\beta)\\\\\n&=& -n\\ln \\sqrt{2\\pi}-n\\ln \\sigma_e-\\ln \\prod_{i=1}^{n}(N_{i-1}\\sqrt{\\Delta t_i})-\\frac{1}{2 \\sigma^2_e}\\sum_{i=1}^{n}\\left(\\frac{N_i - \\mu_{N_{i-1}}}{N_{i-1}}\\right)^2\\\\\n&=& -n \\ln (\\sqrt{2\\pi})- n \\ln \\sigma_e- \\sum_{i=1}^{n}\\ln \\sqrt{\\Delta t_i}-\\sum_{i=1}^{n}\\ln (N_{i-1})-\\frac{1}{2\\sigma^2_e}\\sum_{i=1}^{n}\\left[\\frac{N_i-N_{i-1}-r N_{i-1}\\left[1-\\left(\\frac{N_{i-1}}{K}\\right)^\\theta\\right]\\Delta t_i}{N_{i-1}}\\right]^2\n\\end{eqnarray}\n```\n\n## Estimation of Parameters {#sec-estimation_of_parameters}\n\nConsider $\\{ N_1,N_2,\\cdots,N_{n+1}\\}$ be the observed time series data. The absolute change in population size at time $t$ is approximated as\n$$\\frac{\\mathrm{d}N(t)}{\\mathrm{d}t} \\approx \\frac{N(t+\\Delta t)-N(t)}{\\Delta t}.$$ \nWe assume the yearly changes in population size, so that $\\Delta t=1$. The relative changes in population sizes is given by \n\n\\begin{equation}\nR(t) = \\frac{1}{N}\\frac{\\mathrm{d}N(t)}{\\mathrm{d}t} = \\frac{\\mathrm{d}}{\\mathrm{d}t} \\log N \\approx \\log N(t+1)- \\log N(t).\n\\end{equation} \n\nSo, for a given population time series data of size $n+1$, we have $n$ many observed per capita growth rates $\\{R(1),R(2),\\cdots,R(n)\\}$. We would like to investigate the dynamics of the population when it is subject to environmental stochastic perturbations alone. The following stochastic differential equation has been used to describe to population changes that is exposed to environmental variability. \n\n\\begin{equation}\\label{eq:theta logistic}\n\\mathrm{d}N = r_m N(t) \\left[ 1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right]\\mathrm{d}t + \\sigma_{e} N(t)\\mathrm{d}W(t) ,\n\\end{equation}\n\nwhere $\\sigma_{e}$ represents the intensity of the stochastic perturbation and $W(t)$ is the one dimensional Brownian motion which satisfies $\\mathrm{d}W(t) \\approx W(t+\\mathrm{d}t)-W(t) \\sim \\mathcal{N}(0,\\mathrm{d}t)$. Our goal is to estimate the parameters of the stochastic model by employing Bayesian statistical methods. First we compute the likelihood function of the model parameters given the population size. It is to be noted that the likelihood function can be written either by conditional on the population size $\\{N_t\\}$ or on the growth rates $\\{R_t\\}$. We can use the either observations on $\\{R_t\\}$ or $\\{N_t\\}$ to obtain the posterior distribution of the model parameters. Now, we shall write down the likelihood function given the population time series. Using the equation \\eqref{eq:theta logistic} and utilizing the distributional assumptions, we see that the absolute changes in the population size, conditional on the current size, is normally distributed. So we obtain the following (assuming $\\Delta t =1$):\n\n\\begin{align}\nN(t+1)-N(t)|N(t) &\\sim \\mathcal{N}\\left(r_m N(t) \\left[ 1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right], \\sigma_{e}^2 N(t)^2 \\right) \\notag \\\\\n\\frac{N(t+1)-N(t)}{N(t)}| N(t) &\\sim \\mathcal{N}\\left(r_m  \\left[ 1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right], \\sigma_{e}^2 \\right) \\notag \\\\\nN(i+1)-N(i) &\\sim \\mathcal{N}\\left(r_m N(i)\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta} \\right], \\sigma_{e}^2 N(i)^2 \\right) \\notag \\\\\nN(i+1)|N(i) &\\sim \\mathcal{N}\\left(N(i) + r_m N(i)\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta} \\right], \\sigma_{e}^2 N(i)^2 \\right) \\label{eq:N_i}\n\\end{align}\n\nWe can estimate the parameters using (\\ref{eq:N_i}) using the method of maximum likelihood. Therefore by using (\\ref{eq:N_i}), our goal is to estimate parameters $r_m$, $\\theta$, $K$, $\\sigma_e^2$. If we use $R_t$ values, then the following equation will be used for computation of the likelihood. \n\n\\begin{equation}\nR(t)|N(t)  \\sim \\mathcal{N}\\left(r_m\\left[1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right],\\sigma_{e} ^2\\right)  \\label{eq:R_T},\n\\end{equation} so that the log-likelihood function will be, \n\n\\begin{equation}\\label{eq:likelihood}\n\\mbox{likelihood} = -\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log(\\sigma_e^2)-\\frac{1}{2\\sigma_e^2}\\sum_{i=1}^{n}\\left\\lbrace R(i)- r_m\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta}\\right] \\right\\rbrace^2 .\n\\end{equation}\n\nTo draw inference about $r_m$, $\\theta$, $K$ and $\\sigma_e^2$, we consider inverted gamma prior for variance term $\\sigma_e^2$, gamma prior for $\\theta$, normal prior for $K$ and normal prior for $r_m$. Then the complete Bayesian model for this data can be written as, \n\n\\begin{align*}\nN(i+1)|N(i),r_m,\\theta,K,\\sigma_e^2 &\\sim \\mathcal{N}\\left(N(i) + r_m N(i)\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta} \\right], \\sigma_{e}^2 N(i)^2 \\right) \\\\\nr_m|\\alpha_0,\\beta_0 &\\sim \\mathcal{N}(\\alpha_0,\\beta_0)\\\\\nK| \\mu_K,\\sigma_K^2 &\\sim \\mathcal{N}(\\mu_K,\\sigma_K^2)\\\\\n\\theta|\\alpha_1,\\beta_1 &\\sim \\mathcal{G}(\\alpha_1,\\beta_1)  \\\\\n\\sigma_e^2|\\alpha_2,\\beta_2 &\\sim \\mathcal{IG}(\\alpha_2,\\beta_2),\n\\end{align*}\n\nwhere $\\alpha_0,\\alpha_1,\\alpha_2,\\beta_0,\\beta_1,\\beta_2,\\mu_K$, and $\\sigma_K^2$ are hyper-parameters and assumed to be known. As the starting population size is known, $P(N(1)=N_0)=1$. We take $N(1)=N_0$ as the starting population size and assumed to be a fixed quantity. The joint posterior distribution of $r_m,\\theta,K,\\sigma_e^2$ can be written as, \n\n\\begin{equation*}\nf(r_m,\\theta,K,\\sigma_e ^2|\\boldsymbol{N})=\\frac{f(\\boldsymbol{N},r_m,\\theta,K,\\sigma_e ^2)}{f_{\\boldsymbol{N}}(\\boldsymbol{N})},\n\\end{equation*} where $\\boldsymbol{N}=(N(1),N(2),\\cdots,N(n+1))'$ represents the data and $f_{\\boldsymbol{N}}(\\boldsymbol{N})$ is the marginal distribution of $\\boldsymbol{N}$.\n\n\\begin{equation}\\label{eq:27}\nf\\left(r_m,\\theta,K,\\sigma_e^2|\\boldsymbol{N}\\right) = \\frac{f\\left(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2\\right)\\cdot f\\left(r_m,\\theta,K,\\sigma_e^2\\right)}{f(\\boldsymbol{N})}.\n\\end{equation} We assume that $r_m,\\theta,K,\\sigma_e^2$ are independent variables, therefore $f(r_m,\\theta,K,\\sigma_e ^2)$ can be written as, \n\n\\begin{equation}\nf\\left(r_m,\\theta,K,\\sigma_e^2\\right)=f(r_m)\\cdot f(\\theta) \\cdot f(K) \\cdot f\\left(\\sigma_e^2\\right).\n\\end{equation} So, equation (\\ref{eq:27}) becomes, \n\\begin{eqnarray*}\nf\\left(r_m,\\theta,K,\\sigma_e^2|\\boldsymbol{N}\\right) &\\propto& f\\left(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2\\right)\\cdot f\\left(r_m,\\theta,K,\\sigma_e^2\\right) \\\\\n&\\propto& f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e ^2)\\cdot f(r_m)\\cdot f(\\theta) \\cdot f(K) \\cdot f(\\sigma_e ^2)~~[\\mbox{independence of priors}]  \\\\\n&\\propto&  \\left[\\prod_{i=1}^{n} f\\left(N(i+1)|N(i)\\right)\\right]\\cdot f(r_m)\\cdot f(\\theta) \\cdot f(K) \\cdot f(\\sigma_e ^2).\n\\end{eqnarray*}\n\nThe right hand side of the above expression is product of the likelihood and the prior, which is nothing but unnormalized joint posterior distribution of parameters. Since, the distribution does not follow some common known distribution, we use the Gibbs sampling method that simulates samples from the conditional distribution. So we generate random sample from conditional posterior distribution of each parameters. However, it is observed that in this case also the conditional posterior does not follow any known distribution. So, we use grid approximation algorithm to approximate the posterior probability distribution.\n\nLet $\\boldsymbol{\\beta} = \\left(r_m,\\theta, K, \\sigma_e^2\\right)$. Since, $N(i)$'s are not independent, so the likelihood can be written as,\n\n\\begin{eqnarray*}\n\\mbox{likelihood} &=& f\\left(N(1),N(2),\\cdots , N(n+1) ; \\beta \\right)   \\\\\n&=& f\\left(N(n+1)|N(n) ; \\beta \\right)\\cdot f\\left(N(n)|N(n-1) ; \\beta \\right) \\cdots f\\left(N(2)|N(1) ; \\beta \\right)\\cdot f\\left(N(1) ; \\beta \\right) \\\\\n&=& \\prod_{i=1}^{n} f\\left(N(i+1)|N(i) ; \\beta \\right)\\cdot f\\left(N(1); \\beta \\right).\n\\end{eqnarray*}\n\nWe assume that the initial population size to be known so that $f\\left(N(1);\\beta \\right) = 1$. \n\n\\begin{eqnarray*}\n\\mbox{likelihood} &=& \\prod_{i=1}^{n} f\\left(N(i+1)|N(i) ; \\beta \\right) \\\\\n&=& \\prod_{i=1}^{n}\\left[\\frac{1}{\\sqrt[]{2\\pi}\\sigma_e^2 N(i)} e^{-\\frac{\\left\\lbrace N(i+1)-N(i)- r_m N(i)\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta} \\right]\\right\\rbrace^2} {2\\sigma_e^2 N(i)^2}}\\right]\\\\\n&=&\\prod_{i=1}^{n} \\left[\\frac{1}{\\sqrt[]{2\\pi}\\sigma_e^2 N(i)} e^{{-\\frac{1}{2\\sigma_e^2}\\left\\lbrace \\frac{N(i+1)-N(i)}{N(i)}-r_m\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta}\\right] \\right\\rbrace}^{\\theta} }\\right]\\\\\n&=& \\frac{1}{(\\sqrt[]{2\\pi})^n}\\frac{1}{(\\sigma_e)^n}\\frac{1}{\\prod_{i=1}^{n} N(i)} e^{-\\frac{1}{2\\sigma_e^2}\\sum_{i=1}^{n}\\left\\lbrace R(i)- r_m\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta}\\right]  \\right\\rbrace^2}.\n\\end{eqnarray*}\n\nThe log-likelihood can be given as, \n\n\\begin{equation}\n\\mbox{log-likelihood} = -\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log(\\sigma_e^2)-\\sum_{i=1}^{n}\\log(N(i))-\\frac{1}{2\\sigma_e^2}\\sum_{i=1}^{n}\\left\\lbrace R(i)- r_m\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta}\\right] \\right\\rbrace^2.\n\\end{equation}\n\nTo compute the conditional posterior following process has been carried out,\n\n\\begin{eqnarray*}\nf(r_m|\\theta,K,\\sigma_e^2,\\boldsymbol{N})&=&\\frac{f(\\boldsymbol{N},r_m,\\theta,K,\\sigma_e^2)}{f(\\theta,K,\\sigma_e^2,\\boldsymbol{N})}\\\\\n&=& \\frac{f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2)\\cdot f(r_m,\\theta,K,\\sigma_e^2)}{f(\\boldsymbol{N},\\theta,K,\\sigma_e^2)}\\\\\n&=&\\frac{f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2)\\cdot\\pi(r_m)\\pi(\\theta)\\pi(K)\\pi(\\sigma_e^2)}{f(\\boldsymbol{N}|\\theta,K,\\sigma_e^2)\\cdot f(\\theta,K,\\sigma_e^2)}\\\\\n&=&\\frac{f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2)\\cdot \\pi(r_m)\\cdot \\pi(\\theta)\\cdot \\pi(K)\\cdot\\pi(\\sigma_e^2)}{f(\\boldsymbol{N}|\\theta,K,\\sigma_e^2)\\cdot \\pi(\\theta)\\cdot \\pi(K)\\cdot\\pi(\\sigma_e^2)}\\\\\n&=&\\frac{f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2)\\cdot \\pi(r_m)}{f(\\boldsymbol{N}|\\theta,K,\\sigma_e^2)}.\n\\end{eqnarray*} Let $m=f(\\boldsymbol{N}|\\theta,K,\\sigma_e^2)$, be the joint marginal probability density function of the data. \\begin{eqnarray}\nf(r_m|\\theta,K,\\sigma_e^2,\\boldsymbol{N}) &\\propto& f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2)\\cdot \\pi(r_m) \\notag\\\\\n&\\propto& \\prod_{i=1}^{n} f(N_{i+1}|N_i ; r_m,\\theta,K,\\sigma_e^2)\\cdot  \\pi(r_m).\n\\end{eqnarray} \nSimilarly, \n\n\\begin{eqnarray}\nf(\\theta|r_m,K,\\sigma_e^2,\\boldsymbol{N})&\\propto& \\prod_{i=1}^{n} f(N_{i+1}|N_i ; r_m,\\theta,K,\\sigma_e^2)\\cdot  \\pi(\\theta)\\\\\nf(K|r_m,\\theta,\\sigma_e^2,\\boldsymbol{N})&\\propto& \\prod_{i=1}^{n} f(N_{i+1}|N_i ; r_m,\\theta,K,\\sigma_e^2)\\cdot  \\pi(K)\\\\\nf(\\sigma_e^2|r_m,\\theta,K,\\boldsymbol{N}) &\\propto& \\prod_{i=1}^{n} f(N_{i+1}|N_i ; r_m,\\theta,K,\\sigma_e^2)\\cdot  \\pi(\\sigma_e^2).\n\\end{eqnarray}\n\nIn Bayesian linear regression we already verify that the variance (specially $\\phi$) after observed data follows $\\mathcal{IG}\\left( \\mbox{shape} = \\alpha+\\frac{n}{2}, \\mbox{rate} =\\frac{1}{2} \\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1x_i\\right)^2 + \\gamma\\right)$. Similarly here we expect posterior distribution of variance($\\sigma_e^2$) as follows:\n\n\\begin{equation}\\label{eq:post_sigma_sq}\n\\sigma_e^2|\\boldsymbol{N} \\sim \\mathcal{IG}\\left( \\mbox{shape} = \\alpha_2 +\\frac{n}{2}, \\mbox{rate} =\\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i- r_m \\left\\lbrace 1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right\\rbrace\\right)^2+\\beta_2\\right).\n\\end{equation}\n\n:::{.callout-note}\nIf the prior distribution $\\pi(r_m)$ is considered to be $\\mbox{gammma}(\\alpha, \\beta)$ which takes only positive values, then task of estimation of a declining population may be underestimated.\n:::\n\n:::{.callout-note}\nTo reduce  the  computational time a correct choice of priors is required. In this case, since $K$ is a property of the environment, it should not change drastically. Posterior sample of k should not vary too much away from the prior mean of $K$.\n:::\n\n:::{.callout-note}\nIf the prior distribution $\\pi(r_m)$ is considered to be $\\text{gamma}(\\alpha, \\beta)$ which takes only positive values, then the task of estimation of a declining population may be underestimated.\n:::\n\n:::{.callout-note}\nTo reduce the computational time, a correct choice of priors is required. In this case, since $K$ is a property of the environment, it should not change drastically. The posterior sample of $K$ should not vary too much from the prior mean of $K$.\n:::\n\nThe above structure of the posterior density function of $\\sigma_e^2$ given in the equation \\eqref{eq:post_sigma_sq} is verified by the simulation for the simulated data.\n\n## Simulation study\n\n### Data Generation\n\nTime series data of population size is generated using $r_m=0.5,~~ \\theta=0.5,~~K=50,~~\\sigma_e^2=0.0025$. The initial population size is set as $N_0 = 23$ and the length of the simulated series is $n=30$. The full Bayesian model is described as follows: \n\n\\begin{align}\nR(t)|N(t) &\\sim  \\mathcal{N}\\left(r_m\\left[1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right],\\sigma_{e} ^2\\right)\\label{eq:logistic model} \\\\\nr_m|\\alpha_0,\\beta_0 &\\sim \\mathcal{N}(\\alpha_0,\\beta_0)\\notag\\\\\nK| \\mu_K,\\sigma_K^2 &\\sim  \\mathcal{N}(\\mu_K,\\sigma_K^2)\\notag\\\\\n\\theta|\\alpha_1,\\beta_1 &\\sim \\mathcal{G}(\\alpha_1,\\beta_1)\\notag\\\\\n\\sigma_e^2|\\alpha_2,\\beta_2 &\\sim \\mathcal{IG}(\\alpha_2,\\beta_2).\\notag\n\\end{align}\n\n```{r , eval=TRUE, cache=TRUE}\n#| label: fig-simulated_data\n#| fig-cap: \"Plot of simulated data for the study of Nonlinear regression generated using the model given in the equation (\\\\ref{eq:logistic model}). The parameters values are fixed as $r_m=0.5, \\\\theta=0.5, K=50, \\\\sigma_e^2=0.0025$, and initial population size taken as $N_0=23$, generated data of length $n=30$.\"\n\n# parameters values for simulating the data\nrm = 0.5 # intrinsic growth rate\ntheta = 0.5 # parameter for strength of density dependence\nK = 50 # carrying capacity of the environment\nsig_2e = 0.0025 # variance of the environmental perturbation\nn = 30 # length of the time series = n+1\nN1 = 23 # initial population size\n\nset.seed(471)\nN = numeric(n + 1) # storage the simulated time series\nN[1] = N1 # initial population size.\nfor(i in 1:(length(N)-1)){\n  r = rnorm(n =1, mean = rm*(1-(N[i]/K)^theta), sd = sqrt(sig_2e)) #   simulating growth rate\n  N[i+1] = N[i]*exp(r) # simulate next generation\n}\n#print(N) # printing population size\nplot(1:(n+1), N, type= \"p\", lwd=2, col = \"red\", pch = 19)\n\nfun_logistic = function(x){ # Deterministic solution of the generalized logistic equation\nK/(1+((K/N1)^theta-1 )*exp(-rm*x*theta) )^(1/theta)\n}\ncurve(fun_logistic, 0, n+1, add= TRUE, lwd=2)\n```\n\n### Define Prior distributions\nFor generated time series, we want to generate values from posterior of each parameter$(r_m,K,\\theta,\\sigma_e^2)$. For simulation purpose, we first define the prior functions for the prior distribution given in the equation (\\ref{eq:logistic model}) by considering the values of hyper-parameters as $\\beta_0=1, \\sigma_K^2=3, \\alpha_1=1, \\beta_1=1, \\alpha_2=1, \\beta_2= 0.0001$. Prior mean of $K$ and $r_m$ that is $\\mu_K$ and $\\alpha_0$ are obtained by fitting the logistic growth equation using nonlinear least squares method ($\\texttt{nls}$, in $\\texttt{R}$). The package $\\texttt{invgamma}$ [@invgamma] has been used for generating random numbers from the inverted gamma distribution.\n\n```{r , eval=FALSE, cache=TRUE}\nprior_mean_rm = coef(fit)[1]\nprior_sd_rm = 1\nfun_prior_rm = function(x){ # Prior specification for r_m\n  dnorm(x, prior_mean_rm, prior_sd_rm)\n}\n\nprior_mean_K = coef(fit)[2]\nprior_sd_K = 3\nfun_prior_K = function(x){ # Prior specification for K\n  dnorm(x, prior_mean_K, prior_sd_K)\n}\n\nalpha_1 = 1\nbeta_1 = 1\nfun_prior_theta = function(x){ # Prior specification for theta\n  dgamma(x, shape = alpha_1, rate = beta_1)\n}\n\nlibrary(invgamma)\nalpha_2 = 1\nbeta_2 = 0.0001\nfun_prior_sig_2e = function(x){ # Prior specification for inverted gama\n  dinvgamma(x, shape = alpha_2, rate = beta_2)\n}\n```\n\nAs mentioned in the @sec-estimation_of_parameters, the exact form of the conditional posterior distribution is not known, so grid approximation has been utilized to approximate the conditional posterior density function. In the following code, we have created grid within an interval specified for each parameter. This initial choice is important as this act as a support for the density function. Finer the grid is more accurate is the posterior approximation.\n\n```{r , eval=FALSE, cache=TRUE}\n# Specification of grid values for approximating the posterior\nstep = 0.01\ngrid_rm = seq(from = -3, to =3, by = step)\ngrid_K = seq(from = 40, to = 60, by = step)\ngrid_theta = seq(from = 0, to =10, by = step)\ngrid_sig_2e = seq(from = 0.0001 , to =0.01, by = 0.00001)\n```\n\n### Likelihood function\n\nIn the following code we define the likelihood function. Note that we have directly computed the log-likelihood\nvalues to avoid truncation error.\n\n```{r , eval=FALSE, cache=TRUE}\nlikelihood = function(data, param){\n  rm = param[1]\n  K = param[2]\n  theta = param[3]\n  sig_2e = param[4]\n\n  x = data[,1] # population size\n  y = data[,2] # per capita growth rates\n\n  log_lik = -n*log(sqrt(2*pi)) - (n/2)*log(sig_2e) - sum(((y-rm*(1-(x/K)^theta))^2/(2*sig_2e)))\n  return(exp(log_lik))\n}\n\n```\n\n### Calculation of Posterior distribution\n\nFirst of all we have set 100000 as a number Markov Chain samples to be generated. Then we have fixed the initial values of the parameters and defined the arrays for storing the posterior values of the parameters in $\\texttt{R}$ as follows:\n\n```{r , eval=FALSE, cache=TRUE}\niter = 100000 # Length of the chain to be simulated\npost_rm = rep(NA, iter) # posterior values of r_m\npost_K = rep(NA, iter) # posterior values of K\npost_theta = rep(NA, iter) # posterior values of theta\npost_sig_2e = rep(NA, iter) # posterior values of simga_2e\n\nparam = c(0.2, 40, 1, 0.02) # starting of the chain\n\n# initialization of the chain\npost_rm[1] = param[1]\npost_K[1] = param[2]\npost_theta[1] = param[3]\npost_sig_2e[1] = param[4]\n```\n\nNext we carried out the Gibbs sampling and grid approximation of the posterior distribution. At each Gibbs sampling step (outer loop), the conditional posterior has been approximated by grid approximation (four inner loop for four parameters).\n\n```{r , eval=FALSE, cache=TRUE}\nfor(i in 2:iter){ # loop for iteration of Gibbs sampling\n  # section for rm\n  tmp_post_rm = numeric(length = length(grid_rm))\n  for(j in 1:length(grid_rm)){\n      param = c(grid_rm[j], post_K[i-1], post_theta[i-1], post_sig_2e[i-1])\n      tmp_post_rm[j] = likelihood(data = data, param = param)*fun_prior_rm(grid_rm[j])\n    }\n  prob = tmp_post_rm/sum(tmp_post_rm)\n  post_rm[i] = sample(grid_rm, size = 1, prob = prob)\n\n  # section for K\n  tmp_post_K = numeric(length = length(grid_K))\n  for(j in 1:length(grid_K)){\n    param = c(post_rm[i], grid_K[j], post_theta[i-1], post_sig_2e[i-1])\n    tmp_post_K[j] = likelihood(data = data, param = param)*fun_prior_K(grid_K[j])\n    }\n  prob = tmp_post_K/sum(tmp_post_K)\n  post_K[i] = sample(grid_K, size = 1, prob = prob)\n\n  # section for theta\n  tmp_post_theta = numeric(length = length(grid_theta))\n  for(j in 1:length(grid_theta)){\n    param = c(post_rm[i], post_K[i], grid_theta[j], post_sig_2e[i-1])\n    tmp_post_theta[j] = likelihood(data = data, param = param)*fun_prior_theta(grid_theta[j])\n    }\n  prob = tmp_post_theta/sum(tmp_post_theta)\n\n  post_theta[i] = sample(grid_theta, size = 1, prob = prob)\n\n  # section for sig_2e\n  tmp_post_sig_2e = numeric(length = length(grid_sig_2e))\n  for(j in 1:length(grid_sig_2e)){\n    param = c(post_rm[i], post_K[i], post_theta[i], grid_sig_2e[j])\n    tmp_post_sig_2e[j] = likelihood(data = data, param = param)*fun_prior_sig_2e(grid_sig_2e[j])\n    }\n  prob = tmp_post_sig_2e/sum(tmp_post_sig_2e)\n  post_sig_2e[i] = sample(grid_sig_2e, size = 1, prob = prob)\n\n} # loop ends for Gibbs iteration\n```\n\nBy executing the above code, we have obtained the 1,00,000 sample values from the posterior density of $r_m, K, \\theta$, and $\\sigma_e^2$. We stored them in the local directory on system for the further analysis.\n\n```{r , eval=FALSE, cache=TRUE}\npost_param_vals = data.frame(post_rm,post_K,post_theta,post_sig_2e)\nsetwd(\"specify path here\")\nfilename = paste0(\"output\",seed, \".txt\")\nwrite.table(post_param_vals, file = filename, col.names = TRUE)\n```\n\nTrace plot is a key diagnostic tool used in Bayesian statistics to assess the behavior and convergence of MCMC chains. So we have plotted the traceplot of posterior values obtained using Grid and Gibbs approximation using following code:\n\n```{r , eval=FALSE, cache=TRUE}\nfilename = paste0(\"output\",seed, \".txt\")\npost_vals = read.table(file = filename, header = TRUE)\n\n#traceplot\npar(mfrow=c(2,2))\nplot(post_vals$post_rm, type = \"l\", main = bquote(\"Trace plot of\"~ r[m]),col = \"red\")\nplot(post_vals$post_K, type = \"l\", main = bquote(\"Trace plot of\"~ K),col = \"red\")\nplot(post_vals$post_theta, type = \"l\", main = bquote(\"Trace plot of\"~ theta),col = \"red\")\nplot(post_vals$post_sig_2e, type = \"l\", main = bquote(\"Trace plot of\"~ sigma[e]^2),col = \"red\")\n```\n\nWe have kept initial half i.e 50,000 sample values as a burn-in period for each parameter. After the burn-in period from remaining 50,000 sample posterior values, using appropriate step for thinning (using auto-correlation function, $\\texttt{acf}) we have reduced the autocorrelation between the successive samples to get reliable estimate of parameters. Next we have plot the histogram of posterior distribution of all parameters using following code and depicted in the @fig-posterior.\n\n```{r , eval=FALSE, cache=TRUE}\ncut = 0.5 # burn-in chain (first 50,000 values as burn-in period)\n\n# for rm\nu = post_vals$post_rm[ceiling(iter*cut):iter] # after burn in period\nindex = seq(1,length(u), by = 20) # thining\npost_rm_thinning = u[index] # values after thinning\nacf(post_rm_thinning,main = bquote(\"acf of posterior \"~rm)) # autocorrelation\npost_interval_rm = quantile(post_rm_thinning, c(2.5, 97.5)/100) #credible interval for rm\nhist(post_rm_thinning, probability = TRUE,main = bquote(\"Posterior of \"~ r[m]),\nxlab = expression(r[m]),col = \"grey\", breaks = 40)\narrows(post_interval_rm[1], 0, post_interval_rm[2], 0, lwd=3, angle = 90,col = \"red\", code = 1)\narrows(post_interval_rm[2], 0, post_interval_rm[1], 0, lwd=3, angle = 90,col = \"red\", code = 1)\nlegend(\"topright\",legend = c(\"Credible Interval\"),lwd = 3, col = \"red\",lty = 1,cex = 1, bty = \"n\")\n\n# for K\nu = post_vals$post_K[ceiling(iter*cut):iter] # after burn in period\nindex = seq(1,length(u), by = 10) # thining\npost_K_thinning = u[index] # values after thinning\nacf(post_K_thinning,main = bquote(\"acf of posterior \"~K)) # autocorrelation\npost_interval_K = quantile(post_K_thinning, c(2.5, 97.5)/100) #credible interval for K\nhist(post_K_thinning, probability = TRUE,main = bquote(\"Posterior of \"~ K),\nxlab = expression(K),col = \"grey\", breaks = 40)\narrows(post_interval_K[1], 0, post_interval_K[2], 0, lwd=3, angle = 90, col = \"red\", code = 1)\narrows(post_interval_K[2], 0, post_interval_K[1], 0, lwd=3, angle = 90, col = \"red\", code = 1)\nlegend(\"topright\",legend = c(\"Credible Interval\"),lwd = 3, col = \"red\",lty = 1,cex = 1, bty = \"n\")\n\n# for theta\nu = post_vals$post_theta[ceiling(iter*cut):iter] # after burn in period\nindex = seq(1,length(u), by = 20) # thining\npost_theta_thinning = u[index] # values after thinning\nacf(post_theta_thinning,main = bquote(\"acf of posterior \"~theta)) # autocorrelation\npost_interval_theta = quantile(post_theta_thinning, c(2.5, 97.5)/100) #credible interval for theta\nhist(post_theta_thinning, probability = TRUE,main = bquote(\"Posterior of \"~ theta),xlab = expression(theta),col = \"grey\", breaks = 40)\narrows(post_interval_theta[1], 0, post_interval_theta[2], 0, lwd=3, angle = 90, col = \"red\", code = 1)\narrows(post_interval_theta[2], 0, post_interval_theta[1], 0, lwd=3, angle = 90, col = \"red\", code = 1)\nlegend(\"topright\",legend = c(\"Credible Interval\"),lwd = 3, col = \"red\",lty = 1,cex = 1, bty = \"n\")\n\n# for sigma_e^2\nu = post_vals$post_sig_2e[ceiling(iter*cut):iter] # after burn in period\nindex = seq(1,length(u), by = 1) # thining\npost_sig_2e_thinning = u[index] # values after thinning\nacf(post_sig_2e_thinning,main = bquote(\"acf of posterior \"~sigma[e]^2))\npost_interval_sig_2e = quantile(post_sig_2e_thinning, c(2.5, 97.5)/100)\n#credible interval for sig_2e\nhist(post_sig_2e_thinning, probability = TRUE,main = bquote(\"Posterior of \"~ sigma[e]^2),xlab = expression(sigma[e]^2),col = \"grey\", breaks = 40, ylim = c(0,800))\narrows(post_interval_sig_2e[1], 0, post_interval_sig_2e[2], 0, lwd=3, angle = 90, col = \"red\", code = 1)\narrows(post_interval_sig_2e[2], 0, post_interval_sig_2e[1], 0, lwd=3, angle = 90, col = \"red\", code = 1)\nlegend(\"topright\",legend = c(\"Credible Interval\"),lwd = 3, col = \"red\",lty = 1,cex = 1, bty = \"n\")\n```\n\nFor $\\sigma_e^2$, due to conjugacy the posterior density is predictable, but good approximation would depend on the estimate of the shape parameter which involves other parameters. Here we show what choice of the point estimate for $r_m, K$ and $\\theta$ need to be used (@fig-4) and we observe that the posterior median values best approximate the posterior density function of $\\sigma_e^2$.\n\n\n::: {#fig-posterior layout-ncol=2}\n\n![](./Chapter 4_Bayesian Estimation to Nonlinear Regression Problem_Figures/hist_rm.png){#fig-1}\n\n![](./Chapter 4_Bayesian Estimation to Nonlinear Regression Problem_Figures/hist_K.png){#fig-2}\n\n![](./Chapter 4_Bayesian Estimation to Nonlinear Regression Problem_Figures/hist_theta.png){#fig-3}\n\n![](./Chapter 4_Bayesian Estimation to Nonlinear Regression Problem_Figures/hist_sigma_sq_with_3_fitted_curve.png){#fig-4}\n\n\nPosterior distribution of the parameters of theta-logistic growth equation. From the histogram of $\\sigma_e^2$, it is evident that posterior median is more appropriate choice to obtain a point estimate of $r_m$, $K$ and $\\theta$. Blue lines indicate the 95\\% of credible intervals.\n:::"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"html-math-method":"mathjax","include-in-header":[{"text":"<script>\nwindow.MathJax = {\n  tex: {\n    tags: 'all',\n    labels: {\n      sections: true\n    }\n  }\n};\n</script>\n"}],"number-sections":true,"output-file":"Bayesian Estimation to Nonlinear Regression Problem.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","bibliography":["references.bib"],"theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"Bayesian Estimation to Nonlinear Regression Problem.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"documentclass":"scrreprt","includes":{"in-header":["\\usepackage{graphicx}\n\\usepackage{ragged2e}\n\\justifying\n\\usepackage{imakeidx}\n\\makeindex[intoc=true, columns=3, columnseprule=true, options=-s latex/indexstyles.ist]\n"]}},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}