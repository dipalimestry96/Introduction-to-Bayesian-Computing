<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction to Bayesian Computing - 5&nbsp; Bayesian connection to Statistical Regularization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./summary.html" rel="next">
<link href="./Bayesian Estimation to Nonlinear Regression Problem.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script>
window.MathJax = {
  tex: {
    tags: 'all',
    labels: {
      section: true
    }
  }
};
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./Bayesian connection to Statistical Regularization.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian connection to Statistical Regularization</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Introduction to Bayesian Computing</a> 
        <div class="sidebar-tools-main tools-wide">
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Introduction-to-Bayesian-Computing.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./Introduction-to-Bayesian-Computing.epub">
              <i class="bi bi-bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Illustrative Examples in practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Illustrative Examples in practice</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Bayesian Estimation for Linear Regression Problem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Bayesian Estimation for Linear Regression Problem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Bayesian Estimation to Nonlinear Regression Problem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Bayesian Estimation to Nonlinear Regression Problem</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Bayesian connection to Statistical Regularization.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian connection to Statistical Regularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian connection to Statistical Regularization</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>First we outline the concepts of linear regression. We denote the response variable by <span class="math inline">\(Y\)</span> and the set of predictor variables by <span class="math inline">\(X_1, X_2,\cdots, X_p\)</span>, <span class="math inline">\(p\)</span> being the number of predictors which are also synonymously written as explanatory variables, independent variables, covariates, regressors etc. The true relationship between the response and the predictors can be approximated by a regression function <span class="math inline">\(f\)</span>, so that the equation</p>
<p><span class="math display">\[\begin{equation}\label{eq1}
    Y = f(X_1, X_2,\cdots, X_p) + \epsilon
\end{equation}\]</span></p>
<p>is a valid statistical model for the population of interest. Usually, <span class="math inline">\(f\)</span> is a fixed but unknown function of <span class="math inline">\(X_1, X_2,\cdots, X_p\)</span> which represents the systematic variation of <span class="math inline">\(Y\)</span> explained by the predictors. The unexplained component, denoted by <span class="math inline">\(\epsilon\)</span>, is assumed to be a random error (independent of the predictors) with mean zero. This gives a measure of discrepancy of the approximation by the function <span class="math inline">\(f\)</span>.</p>
<p>Under the multiple regression set up, <span class="math inline">\(f\)</span> is replaced by a linear function of the predictors, represented as</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 X_1 + \cdots+\beta_p X_p + \epsilon,\]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> is the intercept term and <span class="math inline">\(\beta_j\)</span> gives the contributions of <span class="math inline">\(X_j\)</span> for <span class="math inline">\(j=1,\cdots,n\)</span> in explaining the variation of <span class="math inline">\(Y\)</span>. For convenience, we adopt the matrix notation and represent the data set up. We have the continuous response <span class="math inline">\(\mathbf{Y} = \left(y_1,\cdots,y_n\right)' \in \mathbb{R}^n\)</span> and the <span class="math inline">\(n\)</span> data values <span class="math inline">\(\left(x_{ij}\right)_{i=1}^n\)</span> are available on each <span class="math inline">\(X_j\)</span>, <span class="math inline">\(j=1,2,\cdots, p\)</span>. The regression equation in matrix form written as</p>
<p><span class="math display">\[\begin{equation}\label{eq2}
    \mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{e}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{X}\)</span> is an <span class="math inline">\(n\times (p+1)\)</span> design matrix with entries in the first column being 1 (if intercept included) otherwise it is <span class="math inline">\(n\times p\)</span> order matrix with <span class="math inline">\(x_{ij}\)</span> denotes the <span class="math inline">\(i\)</span>th observation corresponding to the <span class="math inline">\(j\)</span>th variable. <span class="math inline">\(\boldsymbol{\beta}\)</span> be the vector of regression coefficients of order <span class="math inline">\((p+1)\times 1\)</span> and <span class="math inline">\(\mathbf{e} = (e_1,e_2,\cdots,e_n)' \in \mathbb{R}^n\)</span> is the vector of errors. By using the least squares theory, the residual sum of squares,</p>
<p><span class="math display">\[\mathbf{e'e} = \sum_{i=1}^n\left(y_i - \beta_0 - \sum_{j=1}^p\beta_j x_{ij}\right)^2 =\mathbf{RSS}(\boldsymbol{\beta}),\]</span></p>
<p>is minimized with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>. Solving the normal equations, the estimates of <span class="math inline">\(\boldsymbol{\beta}\)</span> is obtained as <span class="math inline">\(\hat{\boldsymbol{\beta}} = \mathbf{\left(X'X\right)^{-1}X'Y}\)</span>. Using the notion of <span class="math inline">\(l_2-\mbox{norm}\)</span>, one can write as</p>
<p><span class="math display">\[\begin{equation}\label{est_beta}
    \hat{\boldsymbol{\beta}} = \mbox{argmin}_{\boldsymbol{\beta}}\left(\|\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}\|_2^2\right),
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\|\mathbf{u}\|_2^2 = \sum_{i=1}^n u_i^2\)</span> for a vector <span class="math inline">\(\mathbf{u} \in \mathbb{R}^n\)</span>. <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is an unbiased and consistent estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>. The normal equations for the above minimization problem is given by</p>
<p><span class="math display">\[\mathbf{\left(X'X\right)}\boldsymbol{\beta} = \mathbf{X'Y}.\]</span></p>
<p>If <span class="math inline">\(\mathbf{\left(X'X\right)}\)</span> has determinant zero, then the unique solution for the system of equations can not be obtained. If the matrix is ill-conditioned that is the determinant is very small, then variance for estimated coefficients are very large making the estimates unreliable. This is a common case in many real life data sets where high degree of correlation exits between two variables. If any particular predictor can be closely approximated by a linear combination of two or more other predictors, then also the matrix <span class="math inline">\(\mathbf{\left(X'X\right)}\)</span> is nearly singular. Such a situation is called multicollinearity and must be taken care of before any modeling assignment.</p>
<p>To tackle the multicollinearity problem, various shrinkage methods have been proposed in the literature. Ridge regression deals with solving the normal equations of the form</p>
<p><span class="math display">\[\left(\mathbf{X'X}+\lambda \mathbf{I}\right)\boldsymbol{\beta} = \mathbf{X'Y},\]</span></p>
<p>where <span class="math inline">\(\lambda\ge 0\)</span> is called the shrinkage parameter. Because of the additional parameter <span class="math inline">\(\lambda\)</span> the coefficient estimates <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> has lower variance but they are no longer unbiased. Basically, instead of minimizing the residual sum of squares, ridge regression minimizes a slightly different quantity, given by <span class="math inline">\(\mathbf{RSS}(\boldsymbol{\beta})+ \lambda \sum_{j=1}^p \beta_j^2\)</span>. The coefficient estimates can be written using <span class="math inline">\(l_2-\mbox{norm}\)</span> as</p>
<p><span class="math display">\[\begin{equation}\label{est_beta_ridge}
    \hat{\boldsymbol{\beta}_{\lambda}^R} = \mbox{argmin}_{\boldsymbol{\beta}} \left(\|\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}\|_2^2 + \lambda \|\boldsymbol{\beta}\|_2^2\right).
\end{equation}\]</span></p>
<p>The term <span class="math inline">\(\lambda \sum_{j=1}^p \beta_j^2\)</span>, referred as shrinkage penalty, is small when <span class="math inline">\(\beta_1,\cdots, \beta_p\)</span> are close to zero. For <span class="math inline">\(\lambda = 0\)</span>, the procedure is equivalent to the ordinary least squares (OLS) regression. For <span class="math inline">\(\lambda \rightarrow \infty\)</span>, the influence of shrinkage penalty increases and the components of <span class="math inline">\(\hat{\boldsymbol{\beta}_{\lambda}^R}\)</span> will approach zero and its successful implementation is guaranteed by the bias-variance tradeoff.</p>
<p>Model selection methods such as forward, backward and mixed selection give the model that involves a subset of all the predictors. These techniques can be conveniently performed using R software for statistical computing. The packages <span class="math inline">\(\texttt{ISLR}\)</span> <span class="citation" data-cites="ISLR">(<a href="references.html#ref-ISLR" role="doc-biblioref">James et al. 2021</a>)</span>, <span class="math inline">\(\texttt{leaps}\)</span> <span class="citation" data-cites="leaps">(<a href="references.html#ref-leaps" role="doc-biblioref">Fortran code by Alan Miller 2024</a>)</span>, <span class="math inline">\(\texttt{MASS}\)</span> <span class="citation" data-cites="MASS">(<a href="references.html#ref-MASS" role="doc-biblioref">Venables and Ripley 2002</a>)</span> can be used for this purpose and different criteria such as <span class="math inline">\(R^2\)</span>, AIC etc. can be utilized for the model selection. Unlike these methods, ridge regression includes all <span class="math inline">\(p\)</span> predictors in the model; the penalty term reduces many coefficients to very small values, but will not set them exactly equal to zero. This is challenging for interpretation of the results in high dimension when the number of predictors is very large. This shortcoming is overcome by the method of lasso regression by minimizing the quantity <span class="math inline">\(\mathbf{RSS}(\boldsymbol{\beta})+ \lambda \sum_{j=1}^p|\beta_j|\)</span>, which considers an <span class="math inline">\(l_1-\)</span>norm penalty instead of <span class="math inline">\(l_2-\)</span>norm penalty. Thus using <span class="math inline">\(l_1-\mbox{norm}\)</span> notation, the coefficient estimates can be written as</p>
<p><span class="math display">\[\begin{equation}\label{est_beta_lasso}
    \hat{\boldsymbol{\beta}_{\lambda}^L} = \mbox{argmin}_{\boldsymbol{\beta}} \left(\|\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}\|_2^2 + \lambda \|\boldsymbol{\beta}\|_1\right)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\|\mathbf{u}\|_1 = \sum_{i=1}^n |u_i|\)</span> for a vector <span class="math inline">\(\mathbf{u} \in \mathbb{R}^n\)</span>. Because of the <span class="math inline">\(l_1\)</span> penalty some of the coefficients are forced to be equal to zero for large <span class="math inline">\(\lambda\)</span>. Thus, like the best subset selection methods, lasso also provides variable selection method. More precisely the lasso regression falls between best subset selection method and the ridge regression method and has some nice statistical properties from both techniques.</p>
<p>Ridge regression has a close connection to Bayesian linear regression. Bayesian linear regression assumes that the parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma^2\)</span> (known) to be the random variables, while at the same time considering <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span> as fixed. We shall show that if we assume the the prior distribution for <span class="math inline">\(\boldsymbol{\beta}\)</span> as follows: <span class="math inline">\(\beta_1, \beta_2,\cdots,\beta_p\)</span> are independent and identically distributed according to a normal distribution of mean zero and variance <span class="math inline">\(c\)</span>, then the posterior distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> is also normal and the ridge regression estimate is both the mean and the mode for <span class="math inline">\(\boldsymbol{\beta}\)</span> under this posterior distribution. We start with the linear regression setting again so that, the exact calculations follows naturally:</p>
<p><span class="math display">\[\begin{equation}
  Y_{i} = \beta_{0} + \sum_{j=1}^p\beta_{j}X_{ij}+\epsilon_{i},~~~i = 1,2,\cdots,n.
\end{equation}\]</span></p>
<p><span class="math inline">\(\epsilon_{i}\sim \mathcal{N}(0,\sigma^2),~ i=1,2,\cdots,n\)</span>, <span class="math inline">\(\hat{\boldsymbol{\beta}} = (\hat{\beta_1},\cdots,\hat{\beta_p})'\)</span>, <span class="math inline">\(\mbox{E}_{\boldsymbol{\beta}}\hat{\boldsymbol{\beta}} = \boldsymbol{\beta}\)</span>, <span class="math inline">\(\mbox{Var}_{\boldsymbol{\beta}}(\hat{\boldsymbol{\beta}}) = \sigma^2 (\mathbf{X}'\mathbf{X})^{-1}\)</span>, where <span class="math inline">\(\hat{\beta} =(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}\)</span>, <span class="math inline">\(\mathbf{Y} = (y_1,y_2,\cdots,y_n)'\)</span>. In matrix notation, we write it as</p>
<p><span class="math display">\[\mathbf{Y} = \beta_0 \mathbf{1}_n + \mathbf{X}\boldsymbol{\beta} + \mathbf{e},\]</span></p>
<p>where <span class="math inline">\(\mathbf{1}_n\)</span> is a vector of order <span class="math inline">\(n\times 1\)</span> whose each entry is equal to 1. Note that we do not have any prior distribution on the intercept term <span class="math inline">\(\beta_0\)</span>. Since <span class="math inline">\(\beta_{j}\)</span>’s (<span class="math inline">\(j=1,2,\cdots,p\)</span>) are independent and identically distributed normal random variables with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(c\)</span>, the the joint probability density function can be easily written as a multivariate normal distribution as follows:</p>
<p><span class="math display">\[\begin{equation}
\pi(\boldsymbol{\beta}) = \left(\frac{1}{\sqrt{2\pi c}}\right)^p e^{-\frac{\boldsymbol{\beta}'\boldsymbol{\beta}}{2c}}, ~~\boldsymbol{\beta}\in \mathbb{R}^p,
\end{equation}\]</span></p>
<p>and the marginal prior densities are given by <span class="math inline">\(\pi(\beta_j) = \frac{1}{\sqrt{2\pi c}}e^{-\frac{\beta_j^2}{2c}},~ j =1,2,\cdots,n\)</span>. Using a Hierarchical Bayesian set up the regression model can be written as</p>
<p><span class="math display">\[\begin{eqnarray*}
Y_i|\boldsymbol{\beta} &amp;\sim&amp; \mathcal{N}\left(\beta_0 + \sum_{j=1}^p\beta_{j}x_{ij},\sigma^2\right), i = 1,2,\cdots,n,~\mbox{independent},\\
\boldsymbol{\beta} &amp;\sim &amp; \pi(\boldsymbol{\beta}).
\end{eqnarray*}\]</span></p>
<p>The posterior distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> is given by,</p>
<p><span class="math display">\[\begin{eqnarray*}
\pi(\boldsymbol{\beta}|y) = \frac{f(\boldsymbol{\beta},y)}{f_{\mathbf{Y}}(y)} = \frac{f(y|\boldsymbol{\beta})\pi(\boldsymbol{\beta})}{f_{\mathbf{Y}}(y)},
\end{eqnarray*}\]</span></p>
<p>The marginal distribution of <span class="math inline">\(\mathbf{Y}\)</span> is given by,</p>
<p><span class="math display">\[\begin{eqnarray}\label{eq:one}
f_{\mathbf{Y}}(y) &amp;=&amp; \int_{\mathbb{R}^p} f(y|\boldsymbol{\beta}) \pi(\boldsymbol{\beta})\mathrm{d}\boldsymbol{\beta} \\
&amp;=&amp;\int_{\mathbb{R}^p}\frac{1}{(\sqrt{2\pi}\sigma)^n}e^{-\frac{\sum_{i=1}^n\left(y_{i}-\beta_0-\sum_{j=1}^p\beta_{j}x_{ij}\right)^2}{2\sigma^2}}\frac{1}{(\sqrt{2\pi c})^p}e^{-\frac{\sum_{j=1}^p\beta_{j}^2}{2c}} \mathrm{d}\boldsymbol{\beta} \notag \\
&amp;=&amp;\frac{1}{(\sqrt{2\pi}\sigma)^n}\frac{1}{(\sqrt{2\pi c})^p}\int_{\mathbb{R}^p} e^{-\frac{1}{2}\left[\frac{(y-\beta_0 \mathbf{1}_n - \mathbf{X}\boldsymbol{\beta})'(y-\beta_0 \mathbf{1}_n - \mathbf{X}\boldsymbol{\beta})}{\sigma^2}+\frac{\boldsymbol{\beta}'\boldsymbol{\beta}}{c}\right]} \mathrm{d}\boldsymbol{\beta}
\end{eqnarray}\]</span> Now consider the exponent part in the previous equation and simplify it, <span class="math display">\[\begin{eqnarray}
2\times \mbox{Exponent}&amp;= &amp;\frac{(\mathbf{Y}-\beta_0\mathbf{1}_n-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\beta_0\mathbf{1}_n-\mathbf{X}\boldsymbol{\beta})}{\sigma^2} + \frac{\boldsymbol{\beta}'\boldsymbol{\beta}}{c}\\
&amp;=&amp;\frac{(\mathbf{Y}'-\beta_0\mathbf{1}_n'-\boldsymbol{\beta}'\mathbf{X}')(\mathbf{Y}-\beta_0\mathbf{1}_n-\mathbf{X}\boldsymbol{\beta}) }{\sigma^2} + \frac{\boldsymbol{\beta}' \boldsymbol{\beta}}{c} \notag \\
&amp;=&amp; \frac{\mathbf{Y}'\mathbf{Y}-2\beta_0\mathbf{1}_n'\mathbf{Y}-2\boldsymbol{\beta}'\mathbf{X}'\mathbf{Y}+\beta_0^2+2\boldsymbol{\beta}'\mathbf{X}'\beta_0\mathbf{1}_n+\boldsymbol{\beta}'\mathbf{X}'\mathbf{X}\boldsymbol{\beta}}{\sigma^2} + \frac{\boldsymbol{\beta}'\boldsymbol{\beta}}{c} \notag \\
&amp;=&amp;\frac{\mathbf{Y}'\mathbf{Y}}{\sigma^2}-\frac{2\beta_0\mathbf{1}_n'\mathbf{Y}}{\sigma^2}+\frac{\beta_0^2}{\sigma^2}+\boldsymbol{\beta}'\left(\frac{\mathbf{X}'\mathbf{X}}{\sigma^2}+\frac{1}{c}\mathbf{I}_p\right)\beta -2\boldsymbol{\beta}'\mathbf{X}'\left(\frac{\mathbf{Y}}{\sigma^2}-\frac{\beta_0\mathbf{1}_n}{\sigma^2}\right)\label{eq:two}
\end{eqnarray}\]</span></p>
<p>We write the above expression in the form <span class="math inline">\((\boldsymbol{\beta}-\mu)'\Sigma^{-1}(\boldsymbol{\beta}-\mu) +\)</span> constant containing term <span class="math inline">\(\mathbf{Y}\)</span> and <span class="math inline">\(\mathbf{X}'s\)</span>. To do that we utilize the following expression for the purpose of matching similar terms: <span class="math display">\[(\mathbf{X}-\mu)'\Sigma^{-1}(\mathbf{X}-\mu) =\mathbf{X}'\Sigma^{-1}\mathbf{X}-2\mathbf{X}'\Sigma^{-1}\mu+\mu'\Sigma^{-1}\mu.\]</span> Therefore by equation <span class="math inline">\(\eqref{eq:one}\)</span></p>
<p><span class="math display">\[\begin{eqnarray}
&amp; &amp;\frac{(\mathbf{Y}-\beta_0\mathbf{1}_n-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\beta_0\mathbf{1}_n-\mathbf{X}\boldsymbol{\beta})}{\sigma^2} + \frac{\boldsymbol{\beta}'\boldsymbol{\beta}}{c} \\
&amp;=&amp; \boldsymbol{\beta}'\left(\frac{\mathbf{X}'\mathbf{X}}{\sigma^2}+\frac{1}{c}\mathbf{I}_p\right)\boldsymbol{\beta}-2\boldsymbol{\beta}'\left(\frac{\mathbf{X}'\mathbf{X}}{\sigma^2}+\frac{1}{c}\mathbf{I}_p\right)\left(\frac{\mathbf{X}'\mathbf{X}}{\sigma^2}+\frac{1}{c}\mathbf{I}_p\right)^{-1} \left(\frac{\mathbf{X}'\mathbf{Y}}{\sigma^2}-\frac{\mathbf{X}'\beta_0\mathbf{1}_n}{\sigma^2}\right)+ \notag \\
&amp;\ &amp;\left[\left(\frac{\mathbf{X}'\mathbf{X}}{\sigma^2}+\frac{1}{c}\mathbf{I}_p\right)^{-1}\left(\frac{\mathbf{X}'\mathbf{Y}}{\sigma^2}-\frac{\mathbf{X}'\beta_0\mathbf{1}_n}{\sigma^2}\right)\right]'\left(\frac{\mathbf{X}'\mathbf{X}}{\sigma^2}+\frac{1}{c}\mathbf{I}_p\right)  \notag \\
&amp;\ &amp; \left[\left(\frac{\mathbf{X}'\mathbf{X}}{\sigma^2}+\frac{1}{c}\mathbf{I}_p\right)^{-1}\left(\frac{\mathbf{X}'\mathbf{Y}}{\sigma^2}-\frac{\mathbf{X}'\beta_0\mathbf{1}_n}{\sigma^2}\right)\right] \notag \\
&amp;\ &amp; -\left[\left(\frac{\mathbf{X}'\mathbf{X}}{\sigma^2}+\frac{1}{c}\mathbf{I}_p\right)^{-1}\left(\frac{\mathbf{X}'\mathbf{Y}}{\sigma^2}-\frac{\mathbf{X}'\beta_0\mathbf{1}_n}{\sigma^2}\right)\right]'\left(\frac{\mathbf{X}'\mathbf{X}}{\sigma^2}+\frac{1}{c}\mathbf{I}_p\right)  \notag \\
&amp;\ &amp; \left[\left(\frac{\mathbf{X}'\mathbf{X}}{\sigma^2}+\frac{1}{c}\mathbf{I}_p\right)^{-1}\left(\frac{\mathbf{X}'\mathbf{Y}}{\sigma^2}-\frac{\mathbf{X}'\beta_0\mathbf{1}_n}{\sigma^2}\right)\right] \notag \\
&amp;\ &amp; + \frac{\mathbf{Y}'\mathbf{Y}}{\sigma^2}-\frac{2\beta_0\mathbf{1}_n'\mathbf{Y}}{\sigma^2}+\frac{\beta_0^2}{\sigma^2}. \label{eq:four}
\end{eqnarray}\]</span> Now, let <span class="math display">\[\begin{equation*}
\Sigma^{-1}=\sigma^2\left(\mathbf{X}'\mathbf{X}+\frac{\sigma^2}{c}\mathbf{I}_p\right)~~~\mbox{and}~~~\mu = \Sigma\mathbf{X}'\left(\mathbf{Y}-\beta_0\mathbf{1}_n\right),
\end{equation*}\]</span></p>
<p>then the Exponent becomes,</p>
<p><span class="math display">\[\begin{eqnarray}
2 \times \mbox{Exponent}&amp;=&amp;\boldsymbol{\beta}'\Sigma^{-1}\boldsymbol{\beta}-2\boldsymbol{\beta}'\Sigma^{-1}\mu + \mu'\Sigma^{-1}\mu -\mu'\Sigma^{-1}\mu + \frac{\mathbf{Y}'\mathbf{Y}}{\sigma^2}-\frac{2\beta_0\mathbf{1}_n'\mathbf{Y}}{\sigma^2}+\frac{\beta_0^2}{\sigma^2} \notag \\
&amp;=&amp;(\boldsymbol{\beta}-\mu)'\Sigma^{-1}(\boldsymbol{\beta}-\mu)-\mu'\Sigma^{-1}\mu + \frac{\mathbf{Y}'\mathbf{Y}}{\sigma^2}-\frac{2\beta_0\mathbf{1}_n'\mathbf{Y}}{\sigma^2}+\frac{\beta_0^2}{\sigma^2}. \label{eq:five}
\end{eqnarray}\]</span></p>
<p>Then equation <span class="math inline">\(\eqref{eq:one}\)</span> becomes,</p>
<p><span class="math display">\[\begin{eqnarray}
f_{\mathbf{Y}}(y) &amp;=&amp;\left(\sqrt{2\pi}\right)^p|\Sigma|^{p/2}\frac{1}{\left(\sqrt{2\pi}\sigma\right)^n}\frac{1}{\left(\sqrt{2\pi c}\right)^p}e^{-\frac{\mathbf{Y}'\mathbf{Y}}{2\sigma^{2}}}e^{\frac{\beta_0\mathbf{1}_n'\mathbf{Y}}{2\sigma^{2}}}e^{\frac{\beta_0^2}{2\sigma^{2}}}e^{\frac{\mu'\Sigma^{-1}\mu}{2}} \int_{\mathbb{R}^p}\frac{1}{(\sqrt{2\pi})^p}\frac{1}{|\Sigma|^{p/2}}e^{-\frac{1}{2}(\boldsymbol{\beta}-\mu)'\Sigma^{-1}(\boldsymbol{\beta}-\mu)} \mathrm{d}\boldsymbol{\beta} \notag \\
&amp;=&amp;\left(\sqrt{2\pi}\right)^p|\Sigma|^{p/2}\frac{1}{\left(\sqrt{2\pi}\sigma\right)^n}\frac{1}{\left(\sqrt{2\pi c}\right)^p}e^{-\frac{\mathbf{Y}'\mathbf{Y}}{2\sigma^{2}}}e^{\frac{\beta_0\mathbf{1}_n'\mathbf{Y}}{2\sigma^{2}}}e^{\frac{\beta_0^2}{2\sigma^{2}}}e^{\frac{\mu'\Sigma^{-1}\mu}{2}}.\label{eq:six}
\end{eqnarray}\]</span></p>
<p>Note that the integrand is the density function of <span class="math inline">\(\mbox{MVN}_p(\mu,\Sigma)\)</span>, hence integral out to be <span class="math inline">\(1\)</span>, where,</p>
<p><span class="math display">\[\begin{equation*}
\mu = \left(\frac{\mathbf{X}'\mathbf{X}}{\sigma^2}+\frac{1}{c}\mathbf{I}_p\right)^{-1}\mathbf{X}'\left(\frac{\mathbf{Y}-\beta_0\mathbf{1}_n}{\sigma^2}\right)~~~\mbox{and}~~~ \Sigma = \sigma^2\left(\mathbf{X}'\mathbf{X}+\frac{\sigma^2}{c} \mathbf{I}_p\right)^{-1}.
\end{equation*}\]</span></p>
<p>The posterior distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> is,</p>
<p><span class="math display">\[\begin{eqnarray*}
\phi(\boldsymbol{\beta}|y)&amp;=&amp; \frac{f(\boldsymbol{\beta},y)}{f_{\mathbf{Y}}(y)} \\
&amp;=&amp; \frac{f(y|\boldsymbol{\beta})\pi(\boldsymbol{\beta})}{f_{\mathbf{Y}}(y)} \\
&amp;=&amp; \frac{\frac{1}{(\sqrt{2\pi}\sigma)^n}e^{-\frac{(\mathbf{Y}-\beta_0\mathbf{1}_n-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\beta_0\mathbf{1}_n-\mathbf{X}\boldsymbol{\beta})}{\sigma^2} + \frac{\boldsymbol{\beta}'\beta}{c}}\frac{1}{(\sqrt{2\pi c})^p}e^{-\frac{\boldsymbol{\beta}' \boldsymbol{\beta}}{2c}}}{\left(\sqrt{2\pi}\right)^p|\Sigma|^{\frac{p}{2}}\frac{1}{\left(\sqrt{2\pi}\sigma\right)^n}\frac{1}{\left(\sqrt{2\pi c}\right)^p}e^{-\frac{\mathbf{Y}'\mathbf{Y}}{2\sigma^{2}}}e^{\frac{\beta_0\mathbf{1}_n\mathbf{Y}}{2\sigma^{2}}}e^{\frac{\beta_0^2}{2\sigma^2}}e^{\frac{\mu'\Sigma^{-1}\mu}{2}}} \\
&amp;=&amp;\frac{1}{\left(\sqrt{2\pi}\right)^p}\frac{1}{|\Sigma|^{p/2}}e^{-\frac{1}{2}(\boldsymbol{\beta}-\mu)'\Sigma^{-1}(\boldsymbol{\beta}-\mu)}.
\end{eqnarray*}\]</span></p>
<p>So the posterior mean of <span class="math inline">\(\boldsymbol{\beta}\)</span> is given by</p>
<p><span class="math display">\[\begin{eqnarray*}
\mbox{E}(\boldsymbol{\beta}|y)
&amp;=&amp;\hat{\boldsymbol{\beta}}_B \\
&amp;=&amp;\left(\mathbf{X}'\mathbf{X} + \frac{\sigma^2}{c}\mathbf{I}_p\right)^{-1}\mathbf{X}'(\mathbf{Y}-\beta_0\mathbf{1}_n) \label{eq:seven}
\end{eqnarray*}\]</span></p>
<p>The main utility of ridge regression is to choose the predictors. We can start with the regression model in the following form:</p>
<p><span class="math display">\[\begin{eqnarray*}
\tilde{\mathbf{y}} = \mathbf{X}\boldsymbol{\beta} + \epsilon
\end{eqnarray*}\]</span></p>
<p>where <span class="math inline">\(\tilde{\mathbf{y}} = \mathbf{Y}-\bar{y}\mathbf{1}_n\)</span> and <span class="math inline">\(\bar{y} = \frac{1}{n}\sum_{i=1}^{n}y_i\)</span>. Then</p>
<p><span class="math display">\[\begin{equation*}
\hat{\boldsymbol{\beta}}_B = \left(\mathbf{X}'\mathbf{X} + \frac{\sigma^2}{c}\mathbf{I}_p\right)^{-1}\mathbf{X}'\tilde{\mathbf{y}}.
\end{equation*}\]</span></p>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-leaps" class="csl-entry" role="listitem">
Fortran code by Alan Miller, Thomas Lumley based on. 2024. <em>Leaps: Regression Subset Selection</em>. <a href="https://CRAN.R-project.org/package=leaps">https://CRAN.R-project.org/package=leaps</a>.
</div>
<div id="ref-ISLR" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. <em>ISLR: Data for an Introduction to Statistical Learning with Applications in r</em>. <a href="https://CRAN.R-project.org/package=ISLR">https://CRAN.R-project.org/package=ISLR</a>.
</div>
<div id="ref-MASS" class="csl-entry" role="listitem">
Venables, W. N., and B. D. Ripley. 2002. <em>Modern Applied Statistics with s</em>. Fourth. New York: Springer. <a href="https://www.stats.ox.ac.uk/pub/MASS4/">https://www.stats.ox.ac.uk/pub/MASS4/</a>.
</div>
</div>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./Bayesian Estimation to Nonlinear Regression Problem.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Bayesian Estimation to Nonlinear Regression Problem</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./summary.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Conclusion</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Introduction to Bayesian Computing was written by Dipali Vasudev Mestry <a href="mailto:dipalimestry96@gmail.com">(dipalimestry96@gmail.com)</a>, and Amiya Ranjan Bhowmick <a href="mailto:amiyaiitb@gmail.com">(amiyaiitb@gmail.com)</a>/ <a href="mailto:ar.bhowmick@ictmumbai.edu.in">(ar.bhowmick@ictmumbai.edu.in)</a>..</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This book was built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>



</body></html>