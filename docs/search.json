[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Bayesian Computing",
    "section": "",
    "text": "Preface\nBayesian statistical methods are becoming increasingly popular across all branches of science. With the rapid development of statistical software, Bayesian computations are now accessible to researchers across various domains. These methods are routinely used by practitioners in both industry and academia. However, with the growing availability of software and packages, the fundamental understanding of Bayesian estimation is sometimes compromised.\nIn this document, we aim to bridge that gap by offering a clear understanding of the Bayesian principles through practical examples and case studies. By simulating data and comparing Bayesian estimates with likelihood-based estimates, we focus on the core concept of bias-variance decomposition of the mean square error (MSE) when evaluating estimators.\nThis work is the outcome of a series of lectures delivered by the author, Amiya Ranjan Bhowmick, during the summer vacation of 2018 at the Institute of Chemical Technology (ICT), Mumbai. This document would be incomplete without acknowledging the influence of two exceptional books: Statistical Inference (Casella and Berger 2002) and All of Statistics (Wasserman 2004). These texts provided the foundation for much of the material discussed, and it would have been impossible to develop this work without their insights. We studied many examples and exercises from these books and have put our understanding into words here. And, certainly mistake is a part of our life. We would be grateful if the mistakes are informed to us.\n\n\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical Inference. Second. Duxbury Advanced Series. India Edition: Cengage Learning.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in Statistical Inference. New York: Springer-Verlag New York, Inc."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Bayesian statistics has emerged as a powerful tool in the realm of statistical analysis, providing a flexible framework for making inferences under uncertainty. The foundation of Bayesian methods dates back to the \\(18^\\text{th}\\) century when Reverend Thomas Bayes proposed a theorem for updating beliefs with new evidence. This approach offers a probabilistic paradigm that allows the incorporation of prior knowledge, leading to more informed decision-making.\nBayesian analysis, rooted in Bayesian statistical methods, aims to make inferences about parameters. Parameters are the quantities we seek to estimate or infer from our data. They represent the underlying characteristics or properties of the system or process under investigation. For instance, in a model predicting the probability of species extinction, the parameter of interest might be the actual likelihood of such an extinction event occurring. Mathematically, the parameter(s) is denoted by \\(\\theta\\).\nAt the heart of Bayesian analysis are three fundamental components: prior, likelihood, and posterior. These elements work together to form a cohesive system that informs probabilistic reasoning. Let us understand the what exactly these terms are in Bayesian statistics;\n\nPrior\nThe prior distribution or simply prior is a probability distribution that represents our initial beliefs or knowledge about the parameters before observing any data. It reflects what we think the values of the parameters could be based on prior experience, expert knowledge, or sometimes, a deliberate choice to remain neutral. Priors can take many forms, from non-informative or vague, which show that we have little or no prior knowledge about the parameter, to informative, which incorporate strong prior knowledge. In this book we denote the prior distribution of parameter \\(\\theta\\) by \\(\\pi(\\theta)\\).\n\n\nLikelihood\nThe likelihood function is derived from the data and describes the probability of the observed data given the model parameters. It reflects how well different parameter values explain the observed data. In Bayesian inference, the likelihood is combined with the prior to update our beliefs about the parameters.\nMathematically, if we denote the parameter of interest by \\(\\theta\\) and the observed data by \\(y\\), the likelihood function is given by \\(p(y|\\theta)\\), which measures the plausibility of \\(\\theta\\) given the data.\n\n\nPosterior\nThe posterior distribution denoted by \\(p(\\theta|y)\\) is the result of updating the prior with the observed data through the likelihood. It is the core output of a Bayesian analysis and represents our updated beliefs about the parameter(s) after accounting for the data.\nUsing Bayes’ Theorem, the posterior is expressed as: \\[ p(\\theta|y) = \\frac{p(y|\\theta)\\pi(\\theta)}{p(y)},\\] where \\(p(\\theta|y)\\) is the posterior, \\(p(y|\\theta)\\) is likelihood, \\(\\pi(\\theta)\\) is the prior, and \\(p(y)\\) is the marginal likelihood, which normalize the posterior distribution. The posterior reflects both the information contained in the prior and the evidence from the data, thus offering a balanced view of parameter uncertainty.\n\n\nImproper prior\nIn some cases, a prior distribution \\(\\pi(\\theta)\\) may not integrate to one, which is referred to as an improper prior: \\[\n\\int_{-\\infty}^{\\infty} \\pi(\\theta) \\, d\\theta = \\infty\n\\] These priors are often used when we want a non-informative or vague prior, such as in situations where no clear prior knowledge exists. Improper priors can sometimes lead to improper posteriors, but when handled correctly, they can still be useful, particularly when the likelihood is highly informative.\nFor example, a flat prior over an infinite range is an improper prior because it doesn’t integrate to a finite value. However, it is often employed when the intention is to let the data dominate the inference. Let us understand it through one example;\nA common example of an improper prior is the over the entire real line:\n\\[\\pi(\\theta) = 1\\]\nfor all \\(\\theta \\in \\mathbb{R}\\). This prior is “improper” because:\n\\[ \\int_{-\\infty}^{\\infty} \\pi(\\theta) \\, d\\theta = \\int_{-\\infty}^{\\infty} 1 \\, d\\theta = \\infty. \\] For the posterior distribution to be proper, the integral of the posterior distribution over all possible values of \\(\\theta\\) must be finite and equal to one:\n\\[ \\int_{-\\infty}^{\\infty} \\pi(\\theta | y) \\, d\\theta = 1. \\]\nIf the likelihood function \\(P(y | \\theta)\\) is such that it ensures this condition, then the improper prior can be used effectively.\n\n\nConjugate prior\nGiven a likelihood function \\(p(y \\mid \\theta)\\), if the posterior distribution \\(p(\\theta \\mid y)\\) belongs to the same family of probability distributions as the prior distribution \\(p(\\theta)\\), then the prior and posterior are referred to as conjugate distributions with respect to that likelihood function. In this case, the prior is called a conjugate prior for the likelihood function \\(p(y \\mid \\theta)\\).\nIn this introductory chapter, we have explored the foundational concepts of Bayesian statistics, including the role of parameters, prior distributions, likelihoods, and the posterior distribution. We have also introduced the idea of improper and conjugate priors. With these core principles in mind, we are now ready to move from theory to practice."
  },
  {
    "objectID": "Illustrative Examples in practice.html#example---i-poisson-rate-parameter-with-gamma-prior",
    "href": "Illustrative Examples in practice.html#example---i-poisson-rate-parameter-with-gamma-prior",
    "title": "2  Illustrative Examples in practice",
    "section": "2.1 Example - I (Poisson rate parameter with gamma prior)",
    "text": "2.1 Example - I (Poisson rate parameter with gamma prior)\nLet \\(X_1, X_2,\\cdots, X_n\\) be a random sample of size \\(n\\) from a population following Poisson(\\(\\lambda\\)) distribution. Suppose \\(\\lambda\\) have a gamma(\\(\\alpha\\), \\(\\beta\\)) distribution, which is the conjugate family for Poisson. So, the statistical model has the following hierarchy:\n\\[\\begin{eqnarray*}\n% \\nonumber to remove numbering (before each equation)\n  X_i|\\lambda &\\sim & \\mbox{Poisson}(\\lambda),~~i=1, 2,\\cdots,n, \\\\\n  \\lambda &\\sim & \\mbox{gamma}(\\alpha,\\beta).\n\\end{eqnarray*}\\]\nBased on the observed sample, we are interested to estimate the mean of the population, that is the value of \\(\\lambda\\). The prior distribution of \\(\\lambda\\), \\(\\pi(\\lambda)\\), is given by\n\\[\\begin{equation*}\n    \\pi(\\lambda) = \\frac{\\lambda^{\\alpha-1}e^{-\\frac{\\lambda}{\\beta}}}{\\Gamma(\\alpha) \\beta^{\\alpha}},~~\\lambda&gt;0,\n\\end{equation*}\\]\nwhere \\(\\alpha\\) and \\(\\beta\\) are positive constants. First, we shall obtain the posterior distribution of \\(\\lambda\\). If there were no prior information available about the parameter \\(\\lambda\\), then we could use the sample mean \\(\\overline{X}\\) to estimate it. However, the exact sampling distribution of \\(\\overline{X}\\) is not known in this case. Therefore, we begin with a statistic \\(Y=\\sum_{i=1}^nX_i\\), whose sampling distribution is known to be Poisson(\\(n\\lambda\\)) (denoted by \\(f(y|\\lambda)\\)). The posterior distribution, the conditional distribution of \\(\\lambda\\) given the sample, \\(X_1, X_2,\\cdots, X_n\\), that is, given \\(Y=y\\), is\n\\[\\begin{equation}\n    \\pi(\\lambda|y) = \\frac{f(y|\\lambda)\\pi(\\lambda)}{m(y)},\n\\end{equation}\\]\nwhere \\(m(y)\\) is the marginal distribution of \\(Y\\) and can be calculated as follows;\n\\[\\begin{eqnarray*}\n% \\nonumber to remove numbering (before each equation)\n  m(y) &=& \\int_0^{\\infty}f(y|\\lambda)\\pi(\\lambda)\\mathrm{d}\\lambda \\\\\n   &=& \\int_0^{\\infty}\\frac{e^{-n\\lambda}(n\\lambda)^y}{y!}\\cdot \\frac{\\lambda^{\\alpha-1}e^{-\\frac{\\lambda}{\\beta}}}{\\Gamma(\\alpha)\\beta^{\\alpha}}\\mathrm{d}\\lambda \\\\\n&=& \\frac{n^y}{y!\\Gamma(\\alpha)\\beta^{\\alpha}}\\int_0^{\\infty}e^{-\\lambda\\left(n+\\frac{1}{\\beta}\\right)}\\lambda^{y+\\alpha -1}\\mathrm{d}\\lambda\\\\\n&=& \\frac{n^y \\Gamma(y+\\alpha)}{y!\\Gamma(\\alpha)\\beta^{\\alpha}\\left(y+\\frac{1}{\\beta}\\right)^{y+\\alpha}}\\int_0^{\\infty}\\frac{e^{-\\lambda\\left(n+\\frac{1}{\\beta}\\right)}\\lambda^{y+\\alpha-1}\\left(n+\\frac{1}{\\beta}\\right)^{y+\\alpha}}{\\Gamma(y+\\alpha)}\\mathrm{d}\\lambda\\\\\n&=& \\frac{n^y \\Gamma(y+\\alpha)}{y!\\Gamma(\\alpha)\\beta^{\\alpha}\\left(n+\\frac{1}{\\beta}\\right)^{y+\\alpha}},~~y=0,1,2,\\cdots.\n\\end{eqnarray*}\\]\nIn the above, the integrand is the kernel of the \\(\\mbox{gamma}\\left(y+\\alpha, \\left(n+\\frac{1}{\\beta}\\right)^{-1}\\right)\\) density function, hence integrated out to be 1. Now, the posterior density \\(\\pi(\\lambda|y)\\) is given by \\[\\begin{eqnarray*}\n% \\nonumber to remove numbering (before each equation)\n  \\pi(\\lambda|y) &=& \\frac{e^{-n\\lambda}(n\\lambda)^y}{y!}\\cdot \\frac{\\lambda^{\\alpha-1}e^{-\\frac{\\lambda}{\\beta}}}{\\Gamma(\\alpha)\\beta^{\\alpha}}\\ \\cdot\\frac{y!\\Gamma(\\alpha)\\beta^{\\alpha}\\left(n+\\frac{1}{\\beta}\\right)^{y+\\alpha}}{n^y \\Gamma(y+\\alpha)}\\\\\n& = & \\frac{\\left(n + \\frac{1}{\\beta}\\right)^{y+\\alpha}e^{-\\lambda\\left(n+\\frac{1}{\\beta}\\right)}\\lambda^{y+\\alpha-1}}{\\Gamma(y+\\alpha)}, ~~0&lt;\\lambda &lt;\\infty.\n\\end{eqnarray*}\\] Hence, as expected, the posterior distribution of \\(\\lambda\\), \\[\\lambda|y \\sim \\mbox{gamma}\\left(y+\\alpha, \\left(n+\\frac{1}{\\beta}\\right)^{-1}\\right).\\] It also verifies the claim that the gamma(\\(\\alpha\\),\\(\\beta\\)) is the conjugate family for Poisson. A closure look in the above calculations reveals that the steps can be heavily reduced, in fact the explicit expression for \\(m(y)\\) is not at all required. Since, it does not depend of \\(\\lambda\\), it is a constant, that makes the integral \\(\\int_0^\\infty \\pi(\\lambda|y)\\mathrm{d}\\lambda\\) to be equal to 1, so that it becomes a valid probability density function. Thus, appearance of the posterior density in the integrand of the marginal is not a magic. We shall use this posterior distribution of \\(\\lambda\\) to make statements about the parameter \\(\\lambda\\). The mean of the posterior can be used as a point estimate of \\(\\lambda\\). So, the Bayesian estimator of \\(\\lambda\\) (call it \\(\\hat{\\lambda}_B\\)) is given by \\(E(\\lambda|Y)\\). Because of the gamma density, we avoid the computation of the integral and directly write as: \\[\\begin{equation*}\n    \\hat{\\lambda}_B = \\frac{y+\\alpha}{n+\\frac{1}{\\beta}} = \\frac{\\beta(y+\\alpha)}{n\\beta+1}.\n\\end{equation*}\\] Let us investigate the structure of the Bayesian estimate of \\(\\lambda\\). If no data were available, we are forced to use the information from the prior distribution (only). \\(\\pi(\\lambda)\\) has the mean \\(\\alpha \\beta\\), which would be our best estimate of \\(\\lambda\\). If no prior information were available, we would use \\(Y/n\\) (sample mean \\(\\overline{X}\\)) to estimate \\(\\lambda\\) and draw the conclusion based on the sample values only. Now, the beautiful part is that the Bayesian estimator combines all of these information (if available). We can write \\(\\hat{\\lambda}_B\\) in the following way. \\[\\begin{equation*}\n    \\hat{\\lambda}_B = \\left(\\frac{n\\beta}{n\\beta+1}\\right)\\left(\\frac{Y}{n}\\right) + \\left(\\frac{1}{n\\beta+1}\\right)\\left(\\alpha \\beta\\right).\n\\end{equation*}\\] Thus \\(\\hat{\\lambda}_B\\) is a linear combination of the prior mean and the sample mean. The weights are determined by the values of \\(\\alpha\\), \\(\\beta\\) and \\(n\\).\nSuppose that we increase the sample size such that \\(n \\to \\infty\\). Then \\(\\frac{n\\beta}{n\\beta+1} \\to 1\\) and \\(\\frac{1}{n\\beta+1} \\to 0\\), and \\(\\hat{\\lambda}_B \\to \\overline{X}\\). The idea is that as we increase the sample size, the data histogram closely approximates the population distribution itself. As if we have got enough knowledge about the population itself (because of a large number of observations). Hence, the prior information becomes less relevant and the value of the sample mean dominates the Bayesian estimate of \\(\\lambda\\). When the size of the sample is small, then it is wise to utilize the prior information about the population parameter. Hence, the term \\(\\left(\\frac{\\alpha \\beta}{n\\beta+1}\\right)\\) contributes significantly in the final estimate. By weak law of large numbers \\(\\overline{X} \\to \\lambda\\) in probability as \\(n\\to \\infty\\), so \\(\\hat{\\lambda}_B \\to \\theta\\) in probability, proving that \\(\\hat{\\lambda}_B\\) is consistent estimator for \\(\\lambda\\). Another interesting point is that, for any finite \\(n\\), \\(\\hat{\\lambda}_B\\) is a biased estimator of \\(\\lambda\\), with \\(\\mbox{Bias}_{\\lambda}(\\hat{\\lambda}_B) = \\frac{\\alpha \\beta -\\lambda}{n\\beta + 1} \\to 0\\) as \\(n\\to \\infty\\). \\(\\hat{\\lambda}_B\\) is asymptotically unbiased.\nWe end up with two estimators for the parameter \\(\\lambda\\), viz. Bayesian estimate, \\(\\hat{\\lambda}_B\\) and Maximum likelihood estimator \\(\\hat{\\lambda} = \\overline{X}\\). It is natural to ask which estimator should we prefer? The efficiency of an estimator is computed by the Mean Square Error (MSE) which measures the average squared difference between the estimator and the parameter. In the present situation, the MSE of \\(\\hat{\\lambda}\\) is\n\\[\\begin{equation*}\n    \\mbox{E}_{\\lambda}(\\hat{\\lambda}-\\lambda)^2 = \\mbox{Var}_{\\lambda}(\\overline{X}) = \\frac{\\lambda}{n}.\n\\end{equation*}\\] Since, \\(\\overline{X}\\) is an unbiased estimator of \\(\\lambda\\), the MSE of \\(\\overline{X}\\) is equal to its variance. Now, given \\(Y=\\sum_{i=1}^nX_i\\), the MSE of the Bayesian estimator of \\(\\lambda\\), \\(\\hat{\\lambda}_B = \\frac{\\beta(Y+\\alpha)}{n\\beta +1}\\), is\n\\[\\begin{eqnarray*}\n% \\nonumber to remove numbering (before each equation)\n  \\mbox{E}_{\\lambda}(\\hat{\\lambda}_B-\\lambda)^2 &=& \\mbox{Var}_{\\lambda}\\hat{\\lambda}_B + (\\mbox{Bias}_{\\lambda} \\hat{\\lambda}_B)^2\\\\\n   &=& \\mbox{Var}_{\\lambda}\\left(\\frac{\\beta(Y+\\alpha)}{n\\beta +1}\\right) + \\left(\\mbox{E}_{\\lambda}\\left(\\frac{\\beta(Y+\\alpha)}{n\\beta +1}\\right)-\\lambda\\right)^2\\\\\n   &=& \\frac{\\beta^2 n\\lambda}{(n\\beta+1)^2} + \\frac{(\\alpha \\beta -\\lambda)^2}{(n\\beta+1)^2}.\n\\end{eqnarray*}\\]\nFor fixed \\(n\\), the above quantity will be minimum if \\(\\alpha\\beta = \\lambda\\), that is the prior mean is equal to the true value. However, in general, MSE is a function of the parameter. So, it is highly unlikely that we would end up with a single estimator which is the best for all parameter values. In general the MSEs of two estimators cross each other. This demonstrates that one estimator is better with respect to another estimator in only a portion of the parameter space. Now let us delve deep further in comparing the above two estimators. If \\(\\lambda = \\alpha \\beta\\), then for large \\(n\\), \\[\\mbox{MSE}_{\\lambda}(\\hat{\\lambda}_B)  = \\frac{\\beta^2 n\\lambda}{(n\\beta+1)^2} = \\frac{\\beta^2n\\lambda}{n^2\\beta^2\\left(1+\\frac{1}{n\\beta}\\right)^2} \\approx \\frac{\\lambda}{n}.\\] Thus, if the prior is chosen in a such way (choice of \\(\\alpha\\) and \\(\\beta\\)) so that the prior mean is close to the true value, then both the estimators have same variance approximately, for large \\(n\\). This observation is depicted in Figure 2.1. We consider the values of \\(\\alpha\\) and \\(\\beta\\) to be \\(4\\) and \\(\\frac{1}{2}\\), respectively, so that the prior mean is \\(\\alpha \\beta =2\\). It is clear from the figure that if prior mean is close to the true value, then \\(\\hat{\\lambda}_B\\) performs better than \\(\\hat{\\lambda}\\). If the prior mean is much away from the true value, then \\(\\hat{\\lambda}\\) performs better than \\(\\hat{\\lambda}_B\\). However, for large sample size (\\(n\\to \\infty\\)) both the estimators have same MSE.\n\n\n\nFigure 2.1: MSE of \\(\\hat{\\lambda}=\\overline{X}\\) and \\(\\hat{\\lambda}_B\\) at different values of \\(\\lambda\\) for different choices of the sample size, \\(n\\). The prior distribution of \\(\\lambda\\) is chosen as gamma(4, \\(\\frac{1}{2}\\)). The prior mean is 2 and is indicated by a black dot. For small sample size \\(n=10\\), the \\(\\mbox{MSE}_{\\lambda}(\\hat{\\lambda}_B)\\) is lower than the \\(\\mbox{MSE}_{\\lambda}(\\hat{\\lambda})\\) for all true values of \\(\\lambda\\) in a neighborhood of \\(2\\). As \\(\\lambda\\) increases, after a certain value, \\(\\mbox{MSE}_{\\lambda}(\\hat{\\lambda})\\) crosses the MSE of \\(\\hat{\\lambda}\\). The same is observed for very small values of \\(\\lambda\\). However, for large \\(n\\) values, the MSE of both the estimators merges, essentially the prior information becomes redundant (\\(n = 50\\)).\n\n\nWe have seen that the posterior mean is a linear combination of prior mean and the sample mean. Basically, the Bayesian estimate lies between the sample mean and prior mean. That is, the linear combination is in fact a convex combination. This can also be better visualized if we plot the three distributions in a single plot window, viz. the data histogram, prior distribution, posterior distribution (Figure 2.2).\nR Code for Figure 2.1\n\npar(mfrow=c(1,3))\nalpha = 4; beta = 1/2;\nlambda = seq(0.1, 5, length.out = 50)\nn_vals = c(10, 25, 50)\nfor(n in n_vals){\n  mse_lambda_cap = lambda/n\n  mse_lambdaB_cap = (beta^2*n*lambda + (alpha*beta-lambda)^2)/(n*beta+1)^2\n  plot(lambda, mse_lambda_cap, col = \"red\", lwd=3, lty=1, type=\"l\",\n       xlab = expression(lambda), cex.lab = 1.2, ylab = \"MSE\", main = paste(\"n = \", n), cex.main = 1.5, cex.lab = 1.5)\n  lines(lambda, mse_lambdaB_cap, col = \"blue\", lwd=3, lty=1)\n  legend = c( expression(paste(\"MSE\"[lambda], (hat(lambda)))),\n              expression(paste(\"MSE\"[lambda], (hat(lambda)[B]))))\n  legend(\"topleft\",legend = legend, lwd = c(3,3),col = c(\"red\",\"blue\"),\n         lty = c(1,1), cex = 1, bty = \"n\")\n  points(alpha*beta, 0, lwd=2, pch=20, cex=2)\n}\n\n\n\n\nFigure 2.2: A sample of size \\(n = 5\\) were simulated from the Poisson distribution with parameter \\(\\lambda =2\\) and the sample mean has been computed. The process has been replicated 100 times to obtain the sampling distribution of the sample meanand approximated by a kernel density estimator (magenta colour). The maximum likelihood estimate of \\(\\lambda\\) is denoted by magenta coloured `*’. Similarly exact prior density and exact posterior density functions are plotted using blue and red colour, respectively. Similarly the prior mean and posterior mean values are also marked. The depicted picture clearly verifies the theoretical calculations performed in the text.\n\n\nR Code for Figure 2.2\n\npar(mfrow=c(1,1))\n\n# sampling distribution of sample mean\nn = 5                                       # sample size\nlambda = 2                                  # true mean values\nset.seed(123)                               # reporducibility of simualtion\nrep = 100                                   # number of replication\nmean_vals = numeric(length = rep)\nfor(i in 1:rep){\n    mean_vals[i] = mean(rpois(n = n, lambda = lambda))\n}\nplot(density(mean_vals), lwd=3, col = \"magenta\",  xlim = c(0, 11), ylim =c(0,.8), main = \"\", \n    xlab = expression(lambda), cex.lab = 1.4, lty = 2)\n\n# Prior density of lambda\nalpha = 18; beta = 1/3                      # hyperparameters\nprior_density = function(x){                # prior density\n    exp(-x/beta)*x^(alpha-1)/(beta^alpha * gamma(alpha))\n}\ncurve(prior_density(x), 0, 13, col = \"red\", lwd=3, add = TRUE, lty = 2)\npoints(alpha*beta, 0, lwd=2, col =\"red\", pch = \"*\", cex=2)\n\ny = sum(rpois(n = n, lambda = lambda))      # sufficient statistic\nposterior_density = function(x){            # posterior density\n    (n+1/beta)^(y+alpha)*exp(-x*(n+1/beta))*x^(y+alpha-1)/gamma(y+alpha)\n}\ncurve(posterior_density(x), col = \"blue\", lwd=3, add = TRUE, lty = 2)\npoints(beta*(y+alpha)/(n*beta + 1),0, lwd=2, col =\"blue\", pch = \"*\", cex=2)     # posterior mean\npoints(y/n, 0, lwd=2, col =\"magenta\", pch = \"*\", cex=2)\nlegend(8,0.8, c(expression(f[bar(X)](x)), expression(pi(lambda)),\n                expression(paste(pi,\"(\",lambda, \"|\", y,\")\" ))),\nlwd=rep(3,3), col = c(\"magenta\", \"red\", \"blue\"),  bty=\"n\", cex = 1.3, lty = rep(2, 3))"
  },
  {
    "objectID": "Illustrative Examples in practice.html#example---ii-normal-prior-for-normal-mean",
    "href": "Illustrative Examples in practice.html#example---ii-normal-prior-for-normal-mean",
    "title": "2  Illustrative Examples in practice",
    "section": "2.2 Example - II (Normal prior for normal mean)",
    "text": "2.2 Example - II (Normal prior for normal mean)\nSuppose we observe a sample of size 1, \\(X \\sim \\mathcal{N}(\\theta, \\sigma^2)\\) and suppose that the prior distribution of \\(\\theta\\) is \\(\\mathcal{N}(\\mu, \\tau^2)\\). We assume that the quantities, \\(\\sigma^2\\), \\(\\mu\\) and \\(\\tau^2\\) are all known. We are interested to obtain the posterior distribution of \\(\\theta\\). The prior distribution is given as;\n\\[\\begin{equation*}\n\\pi(\\theta) = \\frac{1}{\\tau \\sqrt{2\\pi}}e^{-\\frac{(\\theta-\\mu)^2}{2\\tau^2}}, ~~-\\infty&lt;\\mu, \\theta &lt;\\infty,~0 &lt;\\tau&lt;\\infty.\n\\end{equation*}\\]\nThe posterior density function of \\(\\theta\\) is given as follows;\n\\[\\begin{eqnarray*}\n    \\pi(\\theta|x) &=& \\frac{f(x|\\theta)\\pi(\\theta)}{\\int_{-\\infty}^{\\infty}f(x|\\theta)\\pi(\\theta)\\mathrm{d}\\theta}\\\\\n                &=& \\frac{\\frac{1}{\\sigma \\tau (\\sqrt{2\\pi})^2}\\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\frac{x-\\theta}{\\sigma}\\right)^2 + \\left(\\frac{\\theta-\\mu}{\\tau}\\right)^2\\right]\\right\\}}{{\\int_{-\\infty}^{\\infty}f(x|\\theta)\\pi(\\theta)\\mathrm{d}\\theta}}.\n\\end{eqnarray*}\\]\nThe exponent can be expressed as\n\\[\\begin{equation*}\n    \\frac{\\sigma^2 +\\tau^2}{\\sigma^2\\tau^2}\\cdot \\left[\\left(\\theta - \\frac{x\\tau^2 + \\mu \\sigma^2}{\\sigma^2 + \\tau^2}\\right)^2 + \\frac{\\tau^2 x^2 + \\mu^2 \\sigma^2}{\\sigma^2 + \\tau^2} - \\left(\\frac{x\\tau^2 + \\mu^2\\sigma^2}{\\sigma^2 + \\tau^2}\\right)^2\\right].\n\\end{equation*}\\]\nNote that, all the terms except containing the expression of \\(\\theta\\) will be cancelled with the denominator, resulting the posterior density of \\(\\theta\\) given as follows;\n\\[\\begin{equation*}\n    \\pi(\\theta|x) = \\frac{\\sqrt{\\sigma^2 +\\tau^2}}{\\sigma\\tau\\sqrt{2\\pi}}\\exp\\left[-\\frac{\\sigma^2 +\\tau^2}{2 \\sigma^2 \\tau^2}\\left(\\theta - \\frac{x\\tau^2 + \\mu \\sigma^2}{\\sigma^2 + \\tau^2}\\right)^2\\right].\n\\end{equation*}\\]\nThe posterior distribution of \\(\\theta\\) is normal, showing that the normal family is its own conjugate when indexed by the mean (\\(\\theta\\)). The posterior mean and variance of \\(\\theta\\) are as follows;\n\\[\\begin{equation*}\n% \\nonumber to remove numbering (before each equation)\n  \\mbox{E}(\\theta|x) =  \\frac{x\\tau^2 + \\mu \\sigma^2}{\\sigma^2 + \\tau^2},~~~~~\\mbox{and}~~~~ \\mbox{Var}(\\theta|x) = \\frac{\\sigma^2 \\tau^2}{\\sigma^2 +\\tau^2}.\n\\end{equation*}\\]\nAs discussed earlier, \\(\\mbox{E}(\\theta|x)\\) is a point of estimate of \\(\\theta\\), thus the Bayesian estimator of \\(\\theta\\) based on a single sample is given by\n\\[\\begin{equation*}\n    \\hat{\\theta}_B = \\frac{X\\tau^2 + \\mu \\sigma^2}{\\sigma^2 + \\tau^2} = \\left(\\frac{\\tau^2}{\\sigma^2 + \\tau^2}\\right)X + \\left(\\frac{\\sigma^2}{\\sigma^2 + \\tau^2}\\right)\\mu.\n\\end{equation*}\\]\nAgain, the Bayesian estimate of \\(\\theta\\) is a linear combination of the prior mean and the sample value (which is in fact the estimate of \\(\\theta\\), as the size of the sample is 1). We shall treat this problem based on a sample of size \\(n\\) and obtain the Bayesian estimator of \\(\\theta\\) using it. A tedious calculation is required to obtain the posterior distribution. However, that will help us to obtain the the distribution of \\(\\theta|\\overline{X}\\) easily. Suppose that we draw a sample \\(X_1, X_2,\\cdots,X_n\\) of size \\(n\\) from \\(\\mathcal{N}(\\theta,\\sigma^2)\\), then the sample mean \\(\\overline{X} \\sim \\mathcal{N}(\\theta, \\sigma^2/n)\\). The same calculation will follow with \\(X\\) would be replaced by \\(\\overline{X}\\) and \\(\\sigma^2\\) will be replaced by \\(\\frac{\\sigma^2}{n}\\), respectively. Then the posterior distribution of \\(\\theta\\) is normal, with mean and variance given by\n\\[\\begin{equation*}\n    \\mbox{E}(\\theta|\\overline{x}) =  \\frac{\\overline{x}n\\tau^2 + \\mu \\sigma^2}{\\sigma^2 + n\\tau^2},~~~~~~~~~ \\mbox{Var}(\\theta|\\overline{x}) = \\frac{\\sigma^2 \\tau^2}{\\sigma^2 +n\\tau^2},\n\\end{equation*}\\]\nand the Bayesian estimator is given as follows;\n\\[\\begin{equation*}\n    \\hat{\\theta}_B = \\left(\\frac{n\\tau^2}{\\sigma^2 + n\\tau^2}\\right)\\overline{X} + \\left(\\frac{\\sigma^2}{\\sigma^2 + n\\tau^2}\\right)\\mu.\n\\end{equation*}\\]\nAs \\(n\\to \\infty\\), \\(\\frac{n\\tau^2}{\\sigma^2 + n\\tau^2} \\to 1\\) and \\(\\frac{\\sigma^2}{\\sigma^2 + n\\tau^2} \\to 0\\), so that \\(\\hat{\\theta}_B \\approx \\overline{X}\\). But, when \\(n\\) is small, use of prior information improves the estimate. Since, \\(\\overline{X}\\to \\theta\\) in probability as \\(n\\to \\infty\\), which follows that \\(\\hat{\\theta}_B \\to \\theta\\) in probability as \\(n\\to \\infty\\) (By Slutsky’s theorem). MSE of \\(\\hat{\\theta}_B\\) is given by\n\\[\\begin{eqnarray*}\n% \\nonumber to remove numbering (before each equation)\n  \\mbox{E}_{\\theta}(\\hat{\\theta}_B - \\theta)^2 &=& \\mbox{Var}_{\\theta}(\\hat{\\theta}_B) + \\left(\\mbox{Bias}_{\\theta}(\\hat{\\theta}_B)\\right)^2 \\\\\n  &=& \\mbox{Var}_{\\theta}\\left(\\frac{\\overline{X}n\\tau^2 + \\mu \\sigma^2}{\\sigma^2 + n\\tau^2}\\right) + \\left(\\mbox{E}_{\\theta}\\left(\\frac{\\overline{X}n\\tau^2 + \\mu \\sigma^2}{\\sigma^2 + n\\tau^2}\\right)-\\theta\\right)^2\\\\\n  &=& \\frac{\\tau^4 n\\sigma^2}{(\\sigma^2 +n\\tau^2)^2} + \\left(\\frac{(\\mu-\\theta)\\sigma^2}{\\sigma^2 +n\\tau^2}\\right)^2\\\\\n  &=& \\frac{\\sigma^2}{\\left(\\sigma^2+n\\tau^2 \\right)^2}\\cdot\\left(n\\tau^4 + (\\mu-\\theta)^2\\sigma^2\\right).\n\\end{eqnarray*}\\]\nThe estimators \\(\\hat{\\theta}_B\\) and \\(\\overline{X}\\) are compared for different sample sizes and also for different choices of the prior variance \\(\\tau^2\\) (Figure 4.2 (a) and Figure 4.2 (b)). The estimator can also be compared by using the relative efficiency, which is defined as the MSE of the two estimators. So the efficiency of \\(\\hat{\\theta}_B\\) relative to \\(\\overline{X}\\), \\(\\mbox{eff}_{\\theta}(\\hat{\\theta}_B|\\overline{X})\\), is\n\\[\\begin{equation*}\n    \\frac{\\mbox{MSE}_{\\theta}(\\hat{\\theta}_B)}{\\mbox{MSE}_{\\theta}(\\overline{X})} = \\frac{n\\tau^4 + (\\mu-\\theta)^2\\sigma^2}{\\left( \\sigma^2 + n\\tau^2\\right)^2}.\n\\end{equation*}\\] If \\(\\mu = \\theta\\), for a given sample size \\(n\\), relative efficiency is minimum and less than 1. Hence, \\(\\hat{\\theta}_B\\) is more efficient than \\(\\overline{X}\\) for any given \\(n\\). If we move \\(\\theta\\) values away from \\(\\mu\\), then \\(\\mbox{eff}_{\\theta}(\\hat{\\theta}_B|\\overline{X})\\) crosses the line \\(y=1\\). For theta values beyond that point, \\(\\overline{X}\\) is better than \\(\\hat{\\theta}_B\\). We encourage the reader to draw this picture and compare two estimators based on the relative efficiency. It is clearly understood that, essentially we are describing the same thing, only with a different graphical representation. Of course, the function \\(\\texttt{curve}\\) from \\(\\texttt{R}\\) is a very useful tool, which has been utilized throughout this material.\n\n\n\nFigure 2.3: \\(\\mbox{MSE}_{\\theta}(\\hat{\\theta})\\) and \\(\\mbox{MSE}_{\\theta}(\\hat{\\theta}_B)\\) are plotted as a function of \\(\\theta\\) for different sample size \\(n\\). The prior distribution of \\(\\theta\\) is considered to be \\(\\mathcal{N}(\\mu=3, \\tau^2=0.5)\\). The prior mean \\(\\mu\\) is indicated by a black dot. For sample size, \\(n=4\\), \\(\\mbox{MSE}_{\\theta}(\\hat{\\theta}_B)\\) is smaller than \\(\\mbox{MSE}_{\\theta}(\\hat{\\theta})\\) at all values of \\(\\theta\\) in a neighborhood of \\(\\mu\\). As we move away from \\(\\mu\\), \\(\\hat{\\theta}\\) is more preferable outsize the neighborhood of \\(\\mu\\). However, as we increase \\(n\\), MSE of both the estimators become closer in every neighborhood of \\(\\mu\\) (\\(n=20\\)), reducing the impact of prior information.\n\n\n\n\n\nFigure 2.4: The MSE of the two estimators are compared at different values of \\(\\tau^2\\). For small sample size (\\(n=4\\)) and small \\(\\tau^2 &lt;\\sigma^2\\), \\(\\hat{\\theta}_B\\) performs better that \\(\\hat{\\theta}\\). For large values of \\(\\tau^2 &gt; \\sigma^2\\), \\(\\mbox{MSE}_{\\theta}(\\hat{\\theta}_B)\\) increases. Basically, prior information becomes vague for large \\(\\tau^2\\) values. As expected, for large sample size \\((n=20)\\) both the estimators perform at par irrespective of the prior variance.\n\n\nR Code for Figure 4.2 (a) and Figure 4.2 (b)\n\nsigma = sqrt(1)     # population sd, known\nmu = 3              # prior mean value\ntau = sqrt(0.5)     # prior sd\nn_vals = c(4, 10, 20) # sample size\n\npar(mfrow=c(2,3))   # space for six plots in a single window\nfor(n in n_vals){\n  curve(sigma^2/(sigma^2 + n*tau^2)^2*(n*tau^4 + (mu-x)^2*sigma^2),\n        0, 6, col = \"blue\", lwd=3, ylab = \"MSE\", ylim=c(0,0.3),\n        main = paste(\"n = \", n), xlab = expression(theta), cex.lab = 1.5)\n  abline(h=sigma^2/n,col = \"grey\", lwd=3)\n  points(mu, 0, lwd=2.5, pch=20, cex=2)\n  text(mu+0.8, 0, expression(mu), cex = 1.5 )\n}\nlegend = c( expression(paste(\"MSE\"[theta], (hat(theta)))),\n    expression(paste(\"MSE\"[theta], (hat(theta)[B]))))\nlegend(\"topright\",legend = legend, lwd = c(3,3),col = c(\"grey\",\"blue\"),\n       lty = c(1,1), cex = 0.8, bty = \"n\")\n\npar(mfrow=c(2,3))\nn_vals = c(4, 10, 20)\nsigma = sqrt(1)\ntau_vals = sqrt(c(0.5, 1, 2))  # varying prior sd\nfor(n in n_vals){\n  for(i in 1:length(tau_vals)){\n    tau = tau_vals[i]\n    if(i == 1){\n      curve(sigma^2/(sigma^2 + n*tau^2)^2*(n*tau^4 + (mu-x)^2*sigma^2),\n            0, 6, col = i+1, lwd=3, ylab = \"MSE\", ylim=c(0,0.5), lty=i,\n            xlab = expression(theta), main = paste(\"n = \", n), cex.lab = 1.5)\n      abline(h=sigma^2/n, lwd=3, col = \"grey\")\n      points(mu, 0, lwd=3, pch=20, cex=2)\n      text(mu+0.8, 0, expression(mu), cex=1.5 )\n    }\n    else\n      curve(sigma^2/(sigma^2 + n*tau^2)^2*(n*tau^4 + (mu-x)^2*sigma^2),\n            0, 6, col = i+1, lwd=2.5, add = TRUE, lty=i)\n  }\n}\nlegend = c(expression(paste(tau^2,\"=\", 0.5)), expression(paste(tau^2,\"=\", 1)), \n        expression(paste(tau^2,\"=\", 2)))\nlegend(\"topright\", legend, col = c(2,3,4), lwd=c(3,3,3), lty=1:3, bty = \"n\")"
  },
  {
    "objectID": "Illustrative Examples in practice.html#example---iii-beta-prior-for-bernoullip",
    "href": "Illustrative Examples in practice.html#example---iii-beta-prior-for-bernoullip",
    "title": "2  Illustrative Examples in practice",
    "section": "2.3 Example - III (Beta prior for Bernoulli(\\(p\\)))",
    "text": "2.3 Example - III (Beta prior for Bernoulli(\\(p\\)))\nLet \\(X_1, X_2,\\cdots, X_n\\) be iid Bernoulli(\\(p\\)). Then \\(Y = \\sum_{i=1}^n X_i\\) is binomial(\\(n,p\\)). We assume the prior distribution of \\(p\\) is Beta(\\(\\alpha, \\beta\\)). Then the joint distribution of \\(Y\\) and \\(p\\) is given by\n\\[\\begin{eqnarray*}\n% \\nonumber to remove numbering (before each equation)\n  f(y,p) &=& \\left[\\binom{n}{y}p^y(1-p)^{n-y}\\right]\\left[\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}p^{\\alpha-1}(1-p)^{\\beta-1}\\right] \\\\\n   &=&  \\binom{n}{y}\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}p^{y+\\alpha-1}(1-p)^{n-y+\\beta -1}.\n\\end{eqnarray*}\\]\nThe marginal pdf of \\(Y\\) is\n\\[\\begin{equation*}\n    f(y) = \\int_0^1f(y,p)\\mathrm{d}p = \\binom{n}{y}\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}\\frac{\\Gamma(y + \\alpha) \\Gamma(n-y+\\beta)}{\\Gamma(n + \\alpha + \\beta)},\n\\end{equation*}\\]\nwhich is the beta-binomial distribution. The posterior distribution of \\(p\\) is given as follows;\n\\[\\begin{equation*}\n    \\pi(p|y) = \\frac{f(y,p)}{f(y)} = \\frac{\\Gamma(n + \\alpha + \\beta)}{\\Gamma(y + \\alpha) \\Gamma(n-y+\\beta)}p^{y+\\alpha-1}(1-p)^{n-y+\\beta -1},\n\\end{equation*}\\]\nwhich is beta(\\(y+\\alpha, n-y + \\beta\\)). So, the Bayes estimator of \\(p\\) is taken as a mean of the posterior distribution under a squared error loss, and it is given as follows;\n\\[\\begin{equation*}\n    \\hat{p}_B = \\frac{y+\\alpha}{\\alpha + \\beta + n}.\n\\end{equation*}\\]\nWe can write \\(\\hat{p}_B\\) in the following way;\n\\[\\hat{p}_B = \\left(\\frac{n}{\\alpha +\\beta +n}\\right)\\left(\\frac{Y}{n}\\right) + \\left(\\frac{\\alpha + \\beta}{\\alpha + \\beta + n}\\right)\\left(\\frac{\\alpha}{\\alpha + \\beta}\\right). \\tag{2.1}\\]\nThus \\(\\hat{p}_B\\) is a linear combination of the prior mean and the sample mean. The weights are determined by the values of \\(\\alpha\\), \\(\\beta\\) and \\(n\\). In the present situation, the MSE of \\(\\hat{p}\\) is\n\\[\\mbox{E}_p(\\hat{p}-p)^2 = \\mbox{Var}((\\overline{X})) = \\frac{p(1-p)}{n}. \\tag{2.2}\\]\nSince, \\(\\overline{X}\\) is an unbiased estimator of \\(p\\), hence the MSE is equal to the variance. Now, given \\(Y = \\sum_{i=1}^{n}X_i\\), the MSE of the Bayesian estimator of \\(p\\), \\(\\hat{p}_B\\) = \\(\\frac{Y+\\alpha}{\\alpha+\\beta+n}\\) is\n\\[\\begin{eqnarray*}\n\\mbox{E}_p(\\hat{p}_B-p)^2&=& \\mbox{Var}_p(\\hat{p}_B) + (\\mbox{Bias}_p\\hat{p}_B)^2\\\\\n&=&\\mbox{Var}_p\\left(\\frac{Y+\\alpha}{\\alpha+\\beta+n}\\right) + \\left(\\mbox{E}_p\\left(\\frac{Y+\\alpha}{\\alpha+\\beta+n}\\right)-p\\right)^2\\\\\n&=&\\frac{np(1-p)}{(\\alpha+\\beta+n)^2} + \\frac{\\left(\\alpha-p(\\alpha+\\beta)\\right)^2}{(\\alpha+\\beta+n)^2}.\n\\end{eqnarray*}\\]\n\n\n\nFigure 2.5: \\(\\mbox{MSE}_p(\\hat{p})=\\mbox{MSE}_p(\\overline{X}) = \\frac{p(1 - p)}{n}\\) and \\(\\mbox{MSE}_p(\\hat{p}_B) =\\frac{np(1 - p) + \\left[\\alpha - p(\\alpha+\\beta)\\right]^2}{\\left(\\alpha+\\beta+n\\right)^2}\\) are computed based on simulated data from a \\(\\mbox{Bin}(n,p)\\) distribution. The prior distribution \\(\\pi(p)\\) is assumed to be \\(\\mbox{Beta}(\\alpha,\\beta)\\). Here we consider \\(\\alpha = \\beta=\\sqrt{\\frac{n}{4}}\\) so that \\(\\mbox{MSE}(\\hat{p}_B) = \\frac{n}{4\\left(n+\\sqrt{n}\\right)^2}\\), which is constant for all values of \\(p (0&lt;p&lt;1)\\), for fixed \\(n\\). The computation was done for different sample sizes as depicted in the figure. At each value of \\(p\\), average MSEs of both the estimators were computed using 1000 replications. The MSEs are plotted against different values of \\(p\\). For small sample size \\(n=4\\), \\(\\hat{p_B}\\) has more precision in estimating the true proportion than \\(\\hat{p}\\) for almost all \\(p\\in (0,1)\\) except the values closer to 0 and 1, where \\(\\hat{p}\\) is better. However, as sample size increases, the interval in which \\(\\hat{p_B}\\) is better than \\(\\hat{p}\\) decreases substantially. For \\(n=400\\), \\(\\hat{p_B}\\) works better in a very small interval about 0.5.\n\n\nR Code for Figure 2.5\n\npar(mfrow = c(2,3))\nn_vals = c(4, 25, 50, 100, 200, 400)  # size of the sample\nrep = 10^4   # number of replication\nP = seq(0.001, 0.99, length.out = 25) # values of the true probability\nfor(n in n_vals){\n  alpha = sqrt(n/4); beta = sqrt(n/4) # Beta(alpha, beta) to make the MSE constant\n  mse_p_cap = numeric(length = length(P))\n  mse_pB_cap = numeric(length = length(P))\n\n  for(i in 1:length(P)){\n    p_cap = numeric(rep)\n    pB_cap = numeric(rep)\n\n    for(j in 1:rep){\n      d = rbinom(n, 1, P[i])\n      p_cap[j] = sum(d)/n\n      pB_cap[j] = (sum(d) + alpha)/(alpha + beta + n )\n    }\n    mse_p_cap[i] = mean((p_cap - P[i])^2)\n    mse_pB_cap[i] = mean((pB_cap - P[i])^2)\n  }\n  ylim = c(min(mse_p_cap, mse_pB_cap), max(mse_p_cap, mse_pB_cap))\n  plot(P, mse_p_cap, col = \"red\", lwd=2, main = paste(\"n = \", n),\n       ylim = ylim, type = \"l\", ylab = \"MSE\", lty =2, xlab = expression(p))\n  lines(P, mse_pB_cap, col = \"blue\", lwd=2, lty=1)\n  legend = c(expression(paste(\"MSE\"[p], (hat(p[B])))),\n    expression(paste(\"MSE\"[p], (hat(p)))))\n  legend(\"bottomleft\", legend = legend, lwd=c(2,2), col = c(\"blue\", \"red\"),\n    bty = \"n\", lty = c(1,2), cex = 0.8)"
  },
  {
    "objectID": "Illustrative Examples in practice.html#example---iv-generalization-of-hierarchical-bayes",
    "href": "Illustrative Examples in practice.html#example---iv-generalization-of-hierarchical-bayes",
    "title": "2  Illustrative Examples in practice",
    "section": "2.4 Example - IV (Generalization of Hierarchical Bayes)",
    "text": "2.4 Example - IV (Generalization of Hierarchical Bayes)\nWe examine a generalization of the Hierarchical Bayes model. Let \\(X_1, X_2, \\cdots, X_n\\) be an observed random sample of size \\(n\\) such that \\[\\begin{equation*}\n     X_i|\\lambda_i \\sim \\mbox{Poisson}(\\lambda_i),~~~~~ i=1,2,\\cdots,n,~~\\mbox{independent}.\n\\end{equation*}\\] Suppose \\(\\lambda_i, ~i=1,2,\\cdots,n\\) have a \\(\\mathrm{gamma}(\\alpha,\\beta)\\) distribution, which is the conjugate family for Poisson. So, the hierarchical model is given by, \\[\\begin{eqnarray*}\n% \\nonumber to remove numbering (before each equation)\n  X_i|\\lambda_i & \\sim& \\mbox{Poisson}(\\lambda_i),~~~~~ i=1,2,\\cdots,n,~~\\mbox{independent}, \\\\\n  \\lambda_i &\\sim& \\mbox{gamma}(\\alpha,\\beta),~~~ i=1,2,\\cdots,n,~~\\mbox{independent},\n\\end{eqnarray*}\\] where \\(\\alpha\\) and \\(\\beta\\) are known positive constants, usually provided by the experimenter. Our interest is to estimate the \\(\\lambda_i,~i=1,2,\\cdots,n,\\) based on observed sample. The prior distribution of \\(\\lambda_i\\) is given by, \\[\\begin{equation*}\n    \\pi(\\lambda_i)=\\frac{e^{-\\frac{\\lambda_i}{\\beta}}\\lambda_i^{\\alpha -1}}{\\Gamma(\\alpha) \\beta^\\alpha},\\ i=1,2,\\cdots,n.\n\\end{equation*}\\] Suppose, we have a situation where the parameter \\(\\beta\\) is not provided by the experimenter. However, the value of \\(\\alpha\\) is known. To obtain an estimate of \\(\\lambda_i\\), we first have to estimate the parameter \\(\\beta\\). A critical point is that if we would like to estimate \\(\\beta\\), then we require independent observations from the \\(\\mbox{gamma}(\\alpha, \\beta)\\) distribution. Although \\(\\lambda_i\\)’s are there from the said distribution, but these are not observable quantities. The empirical Bayes analysis makes use of the observed sample \\(X_1, X_2,\\cdots, X_n\\) to estimate the parameters of the prior distribution. We First obtain the marginal distribution of \\(X_i\\)’s. The use of moment generating function ease the process of computing the marginal distribution greatly. Let \\(M_{X_i}(t) = \\mbox{E}(e^{tX_i})\\), be the mgf of \\(X_i,~i=1,2,\\cdots,n\\), which is\n\\[\\begin{eqnarray*}\n% \\nonumber to remove numbering (before each equation)\n  M_{X_i}(t) &=& \\mbox{E}\\left(e^{tX_i}\\right) \\\\\n   &=&  \\mbox{E}\\left[\\mbox{E}\\left(e^{tX_i}|\\lambda_i\\right)\\right]\\\\\n  &=& \\mbox{E}\\left(e^{\\lambda_i(e^t - 1)}\\right) \\\\\n  &=&  \\frac{1}{\\left[1 - \\beta(e^t-1)\\right]^{\\alpha}},~~~~~~\\left[M_{\\lambda_i}(t) = (1-\\beta t)^{-\\alpha}\\right]\\\\\n  &=& \\left(\\frac{\\frac{1}{\\beta+1}}{1 - \\frac{\\beta}{\\beta+1}e^t}\\right)^{\\alpha}.\n\\end{eqnarray*}\\]\nwhich is the mgf of Negative Binomial distribution with parameter \\(r= \\alpha\\) and \\(p=\\frac{1}{\\beta+1}\\). Recall that if \\(X\\) follows negative binomial distribution, \\(X\\sim \\mbox{NB}(r,p)\\) then mgf of \\(X\\) is given by \\(\\left(\\frac{p}{1-(1-p)e^t} \\right)^\\alpha\\) and \\(\\mbox{E}X = \\frac{r(1-p)}{p^2}\\) and \\(\\mbox{Var}(X) = \\frac{r(1-p)}{p^2}\\). So, \\(X_i \\sim \\mbox{NB}\\left( \\alpha,\\frac{1}{\\beta+1}\\right),~i=1,2,\\cdots,n\\), whose pmf of is given by,\n\\[P(X_i=x_i)=\\binom{x_i +\\alpha-1}{x_i}\\left(\\frac{1}{\\beta +1}\\right)^{\\alpha}\\left(\\frac{\\beta}{\\beta +1}\\right)^{x_i}, ~x_i\\in\\{0,1,2,\\cdots\\}.\\]\nA pleasant surprise is that we observe \\(X_1,X_2,\\cdots,X_n\\) which are marginally iid and follows \\(\\mathrm{NB}\\left(\\alpha,\\frac{1}{\\beta +1}\\right)\\). Hence, \\(\\beta\\) can be estimated by using \\(X_1,X_2,\\cdots,X_n\\). We can use the maximum likelihood estimation method to estimate \\(\\beta\\). The likelihood function \\(\\mathcal{L}(\\beta)\\) is given as follows;\n\\[\\mathcal{L}(\\beta)=\\left[\\prod_{i=1}^{n} \\binom{x_i +\\alpha-1}{x_i}\\right]\\left(\\frac{1}{\\beta +1}\\right)^{\\alpha}\\left(\\frac{\\beta}{\\beta +1}\\right)^{x_i}, \\tag{2.3}\\]\nand the corresponding log-likelihood function, \\(l(\\beta) = \\ln (\\mathcal{L}(\\beta))\\) is\n\\[\nl(\\beta)=\\sum_{i=1}^{n}\\ln \\binom{x_i +\\alpha-1}{x_i}-n\\alpha\\ln(1+\\beta)+\\sum_{i=1}^{n}x_i\\ln\\left(\\frac{\\beta}{1+\\beta}\\right).\n\\tag{2.4}\\]\nTaking first order derivative with respect to \\(\\beta\\) and making \\(\\frac{\\partial l(\\beta)}{\\partial \\beta}=0\\), we obtain the likelihood equation as\n\\[\\begin{equation*}\n\\frac{-n\\alpha}{1+\\beta} + \\left(\\sum_{i=1}^{n}x_i\\right)\\left(\\frac{1}{\\beta}-\\frac{1}{1+\\beta}\\right)=0.\n\\end{equation*}\\]\nOn simplifying we get estimator for prior parameter \\(\\beta\\) as, \\(\\hat{\\beta}=\\frac{\\bar{\\mathrm{X}}}{\\alpha}\\). It can be easily verified that \\(\\frac{\\partial^2 l(\\beta)}{\\partial \\beta^2}=\\frac{n\\alpha}{(\\beta+1)^2}+\\sum_{i=1}^{n}x_i\\left(-\\frac{1}{\\beta^2}+\\frac{1}{(1+\\beta)^2}\\right)\\) at \\(\\beta=\\hat{\\beta}\\), \\(\\frac{\\partial^2(\\beta)}{\\partial \\beta^2}\\bigg|_{\\beta=\\frac{\\bar{\\mathrm{X}}}{\\alpha}}=\\frac{-n\\alpha^3 \\bar{\\mathrm{x}}-n\\alpha^4}{ \\bar{\\mathrm{x}}(\\bar{x}+\\alpha)^2} &lt; 0\\), showing that \\(\\hat{\\beta}\\) is the MLE of \\(\\beta\\). Once the estimate of the unknown prior \\(\\beta\\) is obtained, the model becomes\n\\[\\begin{eqnarray*}\n% \\nonumber to remove numbering (before each equation)\n  X_i|\\lambda_i &\\Large\\sim& \\mbox{Poisson}(\\lambda_i), ~~~~~~i=1,2,\\cdots,n,~~\\mbox{independent}, \\\\\n  \\lambda_i &\\Large\\sim& \\mbox{gamma}(\\alpha,\\hat{\\beta}),~~~~~ i=1,2,\\cdots,n,~~\\mbox{independent}.\n\\end{eqnarray*}\\]\nThe posterior distribution that is the conditional distribution of \\(\\lambda_i\\) given the sample \\(x_i, i=1,2,\\cdots,n\\) is\n\\[\\begin{eqnarray*}\n\\pi(\\lambda_i|x_i)&=&\\frac{f(x_i|\\lambda_i)\\pi(\\lambda_i)}{f(x_i)}\\\\\n&=&\\frac{\\frac{e^{-\\lambda_i}\\lambda_i^{x_i}}{x_i!}\\frac{e^{-\\frac{\\lambda_i}{\\hat{\\beta}}}\\lambda_i^{\\alpha-1}}{\\Gamma(\\alpha) \\hat{\\beta}^\\alpha}}{\\binom{x_i +\\alpha-1}{x_i}\\left(\\frac{1}{\\hat{\\beta}+1}\\right)^\\alpha \\left(\\frac{\\hat{\\beta}}{\\hat{\\beta}+1}\\right)^{x_i}}\\\\\n&=& \\frac{\\lambda_i^{x_i+\\alpha-1}e^{-\\lambda_i\\left(1+\\frac{1}{\\hat{\\beta}}\\right)}\\left(1+\\frac{1}{\\hat{\\beta}}\\right)^{x_i+\\alpha}}{\\Gamma(x_i+\\alpha)}.\n\\end{eqnarray*}\\]\nSo, \\(\\lambda_i|x_i\\sim \\mbox{gamma}\\left(x_i+\\alpha,\\left(1+\\frac{1}{\\hat{\\beta}}\\right)^{-1}\\right)\\). The Bayes estimator of \\(\\lambda_i\\) under a squared error loss, that is the posterior mean \\(\\mbox{E}(\\lambda_i|x_i)\\) is given by,\n\\[\\begin{equation*}\n\\hat{\\lambda_i}_B=(x_i+\\alpha)\\left(1+\\frac{1}{\\hat{\\beta}}\\right)^{-1}.\n\\end{equation*}\\]\nNote that the Bayes estimate of \\(\\lambda_i\\) contains the term \\(\\hat{\\beta}\\), which was estimated from data. Hence, there is uncertainty associated with the estimate of the prior parameter \\(\\beta\\). We can estimate \\(\\mbox{Var}(\\hat{\\beta})\\) as follows:\n\\[\\mbox{Var}(\\hat{\\beta}) = \\frac{1}{\\alpha^2n^2}\\mbox{Var}_{\\beta}(\\sum_{i=1}^n X_i) = \\frac{1}{\\alpha^2n}\\mbox{Var}_{\\beta}(X_1) = \\frac{1}{\\alpha^2n}\\frac{\\alpha \\left(1-\\frac{1}{\\beta+1}\\right)}{\\left(\\frac{1}{\\beta+1}\\right)^2} = \\frac{\\beta (\\beta+1)}{n\\alpha}.\\]\nSo the estimated variance is \\(\\frac{\\hat{\\beta} (\\hat{\\beta}+1)}{n\\alpha}\\). However, it does not play any role in empirical Bayes estimation. This is a drawback of the empirical Bayes estimation. However, in hierarchical Bayes, the unknown prior parameter is replaced by a distribution. Hence the uncertainty in the hyperparameters gets included in the final estimation of the posterior mean. This is an advantage of hierarchical Bayes over empirical Bayes. The hierarchical formulation of the same problem may be stated as follows:\n\\[\\begin{eqnarray*}\n% \\nonumber to remove numbering (before each equation)\n  X_i|\\lambda_i  &\\sim &  \\mbox{Poisson}(\\lambda_i),~~~~~i=1,2,\\cdots,n,~~~\\mbox{independent} \\\\\n  \\lambda_i &\\sim & \\mbox{gamma}(\\alpha,\\beta),~~~ i=1,2,\\cdots,n,~~~\\mbox{independent}, \\\\\n  \\beta &\\sim & \\mbox{uniform}(0,\\infty)~~(\\mbox{noninformative prior}).\n\\end{eqnarray*}\\] Coming back to the problem again, the empirical Bayes estimator is given by \\[\\begin{equation*}\n\\hat{\\lambda_i}_B = \\left(\\frac{\\hat{\\beta}}{1+\\hat{\\beta}}\\right)x_i + \\left(\\frac{1}{1+\\hat{\\beta}}\\right)(\\alpha \\hat{\\beta}).\n\\end{equation*}\\]\nAgain with no surprise, \\(\\hat{\\lambda_i}_B\\) is a linear combination of the prior mean and sample mean. In above, we estimate \\(\\lambda_i\\) using only single observation form \\(\\mbox{Poission}(\\lambda_i),~ i=1,2,\\cdots,n\\). In the discussed example, we have assumed \\(\\alpha\\) to be known and \\(\\beta\\) unknown. However, it might happen that both \\(\\alpha\\) and \\(\\beta\\) are known. In such situation, both \\(\\alpha\\) and \\(\\beta\\) may be estimated from the observed samples \\(X_1,X_2,\\cdots, X_n\\), whose marginal distributions are iid \\(\\mbox{NB}\\left(\\alpha, \\frac{1}{\\beta+1}\\right)\\). The likelihood equations are given by\n\\[\\frac{\\partial l(\\alpha, \\beta)}{\\partial \\alpha} = 0, ~ \\frac{\\partial l(\\alpha, \\beta)}{\\partial \\beta}=0,\\]\nwhere the log-likelihood function \\(l(\\alpha, \\beta)\\) is same as given in the equation Equation 2.4. These maximization must be carried out using some numerical procedures, for example Newton Raphson method. The estimates and associated standard errors can be obtained using R. For example, the likelihood function can be passed in the \\(\\texttt{optim}\\) function or the function \\(\\texttt{fitdistr}\\), available in the \\(\\texttt{MASS}\\) package (Venables and Ripley 2002), may be utilized. Sample code is given below:\n\n&gt; x = rgamma(n = 50, shape = 2, rate = 3)\n&gt; library(MASS)\n&gt; fit = fitdistr(x = x, \"gamma\", lower = 0.01)\n&gt; fit$estimate\n   shape     rate\n1.615070 2.922565\n&gt; fit$sd\n    shape      rate\n0.2956650 0.6261151\n&gt; hist(x, prob = T, col = \"lightgrey\")\n&gt; curve(dgamma(x, shape = fit$estimate[1], rate = fit$estimate[2]),\n    col = \"red\", lwd=2, add = TRUE)\n\nIn the same example, we may have a one way classification with \\(n\\) groups and \\(m\\) observations per group. Then the example extends to \\[\\begin{eqnarray*}\n% \\nonumber to remove numbering (before each equation)\n  X_{ij} |\\lambda_i &\\sim& \\mbox{Poisson}(\\lambda_i),~~~i=1,2,\\cdots,n;~j=1,2,\\cdots,m;~~~\\mbox{independent}, \\\\\n  \\lambda_i &\\sim& \\mbox{gamma}(\\alpha, \\beta),~~~i=1,2,\\cdots,n;~~~~\\mbox{independent}.\n\\end{eqnarray*}\\] To estimate \\(\\lambda_i\\), we use the statistic \\(Y_i=\\sum_{j=1}^{m}X_{ij}\\), then \\(Y_i|\\lambda_i \\sim \\mbox{Poisson}(m\\lambda_i),~ i=1,2,\\cdots,n\\). The marginal pdf of \\(Y_i\\) is \\[\\begin{eqnarray*}\nf(y_i)&=&\\int_{0}^{\\infty}f(y_i|\\lambda_i)\\pi(\\lambda_i)\\mathrm{d}\\lambda_i\\\\\n&=&\\int_{0}^{\\infty}\\frac{e^{-m\\lambda_i}(m\\lambda_i)^{y_i}}{y_i!}\\frac{e^{-\\frac{\\lambda_i}{\\hat{\\beta}}}\\lambda_i^{\\alpha-1}}{\\Gamma(\\alpha) \\hat{\\beta}^\\alpha}\\mathrm{d}\\lambda_i\\\\\n&=&\\int_{0}^{\\infty}\\frac{e^{-\\lambda_i\\left(m+\\frac{1}{\\beta}\\right)}m^{y_i}\\lambda_i^{y_i+\\alpha-1}}{y_i!\\beta^{\\alpha}\\Gamma(\\alpha)}\\\\\n&=&\\frac{m^{y_i}\\left\\lbrace\\left(m+\\frac{1}{\\beta}\\right)^{-1}\\right\\rbrace^{y_i+\\alpha}\\Gamma(y_i+\\alpha)}{y_i!\\beta^{\\alpha}\\Gamma(\\alpha)}\\\\\n&=&\\frac{\\Gamma(y_i+\\alpha+1-1)}{\\Gamma(y_i+1)\\Gamma(\\alpha)}\\left(\\frac{m\\beta}{m\\beta+1}\\right)^{y_i}\\left(\\frac{1}{m\\beta+1}\\right)^\\alpha\\\\\n&=&\\binom{y_i +\\alpha-1}{y_i}\\left(\\frac{1}{m\\beta+1}\\right)^\\alpha\\left(1-\\frac{1}{m\\beta+1})\\right)^{y_i}.\n\\end{eqnarray*}\\] So, \\(Y_i\\sim \\mbox{NB}\\left(\\alpha,\\frac{1}{m\\beta+1}\\right),~ i=1,2,\\cdots,n\\). The posterior distribution of \\(\\lambda_i\\) is given by, \\[\\begin{eqnarray*}\n\\pi(\\lambda_i|y_i)&=&\\frac{f(y_i|\\lambda_i)\\pi(\\lambda_i)}{f(y_i)}\\\\\n&=&\\frac{\\frac{e^{m\\lambda_i}(m \\lambda_i)^{y_i}}{y_i!}\\frac{e^{-\\frac{\\lambda_i}{\\beta}}\\lambda_i^{\\alpha-1}}{\\beta^{\\alpha}\\Gamma(\\alpha)}}{f(y_i)}\\\\\n&=&\\frac{\\lambda_i^{y_i+\\alpha-1}e^{-\\lambda_i\\left(m+\\frac{1}{\\beta}\\right)}\\left(m+\\frac{1}{\\beta}\\right)^{y_i+\\alpha}}{\\Gamma(y_i+\\alpha)},\n\\end{eqnarray*}\\] which shows that \\(\\lambda_i|y_i \\sim \\mbox{gamma}\\left(y_i+\\alpha,\\left(m+\\frac{1}{\\beta}\\right)^{-1}\\right)\\). The posterior mean, that is the Bayes estimator of \\(\\lambda_i\\) under a squared error loss is \\[\\begin{eqnarray*}\n\\hat{\\lambda_i}_B&=& \\mbox{E}(\\lambda_i|y_i)\\\\\n&=&(y_i+\\alpha)\\left(m+\\frac{1}{\\beta}\\right)^{-1}\\\\\n&=&\\left(\\sum_{j=1}^{m}x_{ij} +\\alpha\\right) \\left(m+\\frac{1}{\\beta}\\right)^{-1},\\\\\n&=&\\left(\\frac{m\\beta}{1+m\\beta}\\right)\\bar{X_i} + \\left(\\frac{1}{1+m\\beta}\\right)(\\alpha \\beta).\n\\end{eqnarray*}\\] Thus, \\(\\hat{\\lambda_i}_B\\) is a linear combination of the prior mean and sample mean. The variance of \\(\\hat{\\lambda_i}_B\\) is given by \\[\\begin{eqnarray*}\n\\mbox{Var}_{\\lambda_i}(\\hat{\\lambda_i}_B)&=&\\left(\\frac{m\\beta}{1+m\\beta}\\right)^2\\mbox{Var}_{\\lambda_i}(\\bar{X_i}),\\\\\n&=& \\left(\\frac{m\\beta}{1+m\\beta}\\right)^2\\left(\\frac{\\lambda_i}{m}\\right).\n\\end{eqnarray*}\\] With no surprise, for large \\(m\\), the above expression is close to the variance of maximum likelihood estimator \\(\\left(\\frac{m\\beta}{m\\beta + 1} \\to 1 \\mbox{ for large } m\\right)\\)."
  },
  {
    "objectID": "Illustrative Examples in practice.html#example---v-gamma-prior-for-exponential-rate-parameter",
    "href": "Illustrative Examples in practice.html#example---v-gamma-prior-for-exponential-rate-parameter",
    "title": "2  Illustrative Examples in practice",
    "section": "2.5 Example - V (Gamma prior for Exponential rate parameter)",
    "text": "2.5 Example - V (Gamma prior for Exponential rate parameter)\nLet \\(Y_1\\),\\(Y_2\\),\\(\\cdots\\),\\(Y_n\\) be a observed random sample of size \\(n\\) such that \\[\\begin{equation*}\nY_i|\\lambda \\sim \\mbox{exponential}(\\lambda),\\ i=1,2,\\cdots,n,~~\\mbox{independent}.\n\\end{equation*}\\] Suppose that \\(\\lambda\\) has a \\(\\mbox{gamma}(\\alpha,\\beta)\\) distribution, which is the conjugate family for Exponential. So, the statistical model has the following hierarchy\n\\[\\begin{eqnarray}\nY_i|\\lambda &\\sim& \\mbox{exponential}(\\lambda),\\ i=1,2,\\cdots,n,\\\\\n\\lambda &\\sim& \\mbox{gamma}(\\alpha,\\beta),\n\\end{eqnarray}\\]\nwhere \\(\\alpha\\) and \\(\\beta\\) are known. Now, our interest is to estimate the \\(\\lambda\\) based on observed sample. The prior distribution of \\(\\lambda\\) is given by,\n\\[\\begin{equation}\n\\pi(\\lambda)=\\frac{e^{-\\frac{\\lambda}{\\beta}}\\lambda^{\\alpha -1}}{\\Gamma(\\alpha)\\beta^\\alpha }, \\lambda&gt;0,\n\\end{equation}\\]\nwhere \\(\\alpha\\) and \\(\\beta\\) are positive constants. First, we shall estimate the posterior distribution of \\(\\lambda\\). We start with statistic \\(Z\\) = \\(\\sum_{i=1}^n Y_i\\). Sampling distribution of \\(Z\\) is \\(\\mbox{gamma}(n,\\lambda)\\) and it is denoted by \\(f(z|\\lambda).\\) The posterior distribution of \\(\\lambda\\) given the sample \\(Y_1,Y_2,\\cdots,Y_n\\), that is given \\(Z = z,\\) is\n\\[\\begin{equation*}\n\\pi(\\lambda|z) = \\frac{f(z|\\lambda)\\pi(\\lambda)}{f(z)},\n\\end{equation*}\\]\nwhere \\(f(z)\\) is the marginal distribution of \\(Z\\), and calculated as follows;\n\\[\\begin{eqnarray*}\nf(z) &=& \\int_{0}^{\\infty}f(z|\\lambda)\\pi(\\lambda) \\mathrm{d}\\lambda \\\\\n&=& \\int_{0}^{\\infty} \\frac{z^{n-1}e^{-\\lambda z}\\lambda^n}{\\Gamma(n)}\\frac{\\lambda^{\\alpha-1}e^{-\\frac{\\lambda}{\\beta}}}{\\Gamma(\\alpha)\\beta^\\alpha} \\mathrm{d}\\lambda \\\\\n&=& \\frac{z^{n-1}}{\\Gamma(n)\\Gamma(\\alpha)\\beta^\\alpha}\\int_{0}^{\\infty} e^{-\\lambda(z+\\frac{1}{\\beta})} \\lambda^{\\alpha+n-1} \\mathrm{d}\\lambda  \\\\\n&=& \\frac{z^{n-1} \\Gamma(\\alpha + n)}{\\Gamma(n)\\Gamma(\\alpha)\\beta^\\alpha (z+\\frac{1}{\\beta})^{\\alpha+n}} \\int_{0}^{\\infty} \\frac{\\lambda^{\\alpha+n-1}e^{-\\lambda(z+ \\frac{1}{\\beta})}(z+\\frac{1}{\\beta})^{\\alpha+n}}{\\Gamma(\\alpha+n)} \\mathrm{d}\\lambda \\\\\n&=& \\frac{ z^{n-1} \\Gamma(\\alpha + n)}{\\Gamma(n)\\Gamma(\\alpha)\\beta^\\alpha (z+\\frac{1}{\\beta})^{\\alpha+n}},\n\\end{eqnarray*}\\] where,\\[\\begin{equation*}\n\\int_{0}^{\\infty} \\frac{\\lambda^{\\alpha+n-1}e^{-\\lambda(z+\\frac{1}{\\beta})}(z+\\frac{1}{\\beta})^{\\alpha+n}}{\\Gamma(\\alpha+n)} \\,d\\lambda = 1.\n\\end{equation*}\\]\nbecause it is pdf of \\(\\mbox{gamma}\\left(\\alpha+n,(z+\\frac{1}{\\beta})^{-1}\\right)\\) distribution. So integral reduces to 1. Now, posterior density \\(\\pi(\\lambda|z)\\) becomes,\n\\[\\begin{eqnarray*}\n\\pi(\\lambda|z) &=& \\frac{f(z|\\lambda)\\pi(\\lambda)}{f(z)} \\\\\n&=&  \\frac{z^{n-1}e^{-\\lambda z}\\lambda^n}{\\Gamma(n)}\\frac{e^{-\\frac{\\lambda}{\\beta}}\\lambda^{\\alpha -1}}{\\Gamma(\\alpha)\\beta^\\alpha}\\frac{\\Gamma(n)\\Gamma(\\alpha)(z+\\frac{1}{\\beta})^{\\alpha+n}\\beta^\\alpha}{ z^{n-1}\\Gamma(\\alpha+n)} \\\\\n&=& \\frac{e^{-\\lambda(z+\\frac{1}{\\beta})}\\lambda^{\\alpha+n-1}(z+\\frac{1}{\\beta})^{\\alpha+n}}{\\Gamma(\\alpha+n)}, 0&lt;\\lambda&lt;\\infty.\n\\end{eqnarray*}\\]\nHence, the posterior distribution of \\(\\lambda\\), \\(\\lambda|z\\sim \\mbox{gamma}\\left(\\alpha+n,(z+\\frac{1}{\\beta})^{-1}\\right)\\). So exact posterior mean \\(\\mbox{E}(\\lambda|z)\\) is \\[\\begin{equation*}\n\\frac{\\alpha+n}{z+\\frac{1}{\\beta}} = \\frac{\\alpha+n}{n\\bar{y}+\\frac{1}{\\beta}}\n\\end{equation*}\\] Now, we approximate the posterior mean of \\(\\lambda\\) using a Monte Carlo (MC) sample, given \\(m\\) independent values drawn directly from \\(\\mbox{gamma}(\\alpha+n,n\\bar{y}+\\frac{1}{\\beta})\\) posterior distribution. Further, the accuracy of the monte carlo approximation is compared with respect to the exact posterior distribution. First of all, we generated \\(y\\) of size \\(n=50\\) from exponential distribution with parameter \\(\\lambda\\), where \\(\\lambda\\) was generated from \\(\\mbox{gamma}(\\alpha = 8,\\beta = 4)\\) density function. To generate the independent values of \\(\\lambda\\) of the MC sample from the known posterior distribution the function \\(\\texttt{rgamma}\\) was utilized with posterior rate and shape parameter.\nR Code for Figure 2.6\n\nset.seed(123)\nn = 50                                          # sample size\nalpha = 8                                       # prior parameter\nbeta = 4                                        # prior parameter\nlambda = rgamma(1,alpha,beta)                   # true lambda\nlambda\ny = rexp(n = n,rate = lambda)                   # data (simulated here)\np_alpha = alpha + n; p_alpha                    # posterior alpha\np_beta = (1/beta) + n*mean(y); p_beta           # posterior beta\np_mean = p_alpha/p_beta; p_mean                 # exact posterior mean\npar(mfrow=c(1,3))\nM = c(100,500,1000)\nfor (m in M) {\n  z1 = rgamma(n=m, shape=p_alpha, rate=p_beta)\n  hist(z1, probability = TRUE,main=paste(\"m =\",m),\n       xlab=expression(lambda),col = \"light grey\", cex.lab = 1.5, cex.main = 1.5)\n  curve(dgamma(x, shape = p_alpha, rate = p_beta),\n        col =\"blue\", add=TRUE, lwd = 2)\n  points(lambda, 0, pch = 19,col = \"red\", cex = 2)}\n\n\n\n\nFigure 2.6: Histogram approximation of the posterior probability density of \\(\\lambda\\) for different posterior sample of size \\(m\\). Red dot indicates the exact posterior mean (given data). Blue colored curve represents the exact posterior density function."
  },
  {
    "objectID": "Illustrative Examples in practice.html#exercises",
    "href": "Illustrative Examples in practice.html#exercises",
    "title": "2  Illustrative Examples in practice",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\n\n(Casella and Berger 2002) If \\(S^2\\) is the sample variance based on a sample of size \\(n\\) from a normal population, we know that \\((n-1)S^2/\\sigma^2\\) has a \\(\\chi_{n-1}^2\\) distribution. The conjugate prior for \\(\\sigma^2\\) is the inverted gamma pdf, IG(\\(\\alpha, \\beta\\)), given by, \\[\\begin{equation*}\n     \\pi(\\sigma^2) = \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}}\\frac{1}{\\left(\\sigma^2 \\right)^{\\alpha+1}}e^{-1/(\\beta \\sigma^2)},~~0&lt;\\sigma^2 &lt;\\infty,\n   \\end{equation*}\\] where \\(\\alpha\\) and \\(\\beta\\) are positive constants. Show that the posterior distribution of \\(\\sigma^2\\) is \\[\\begin{equation*}\n     \\mbox{IG}\\left(\\alpha+\\frac{n-1}{2}, \\left[\\frac{(n-1)S^2}{2} + \\frac{1}{\\beta}\\right]^{-1}\\right).\n   \\end{equation*}\\]\nFind the mean of this distribution, the Bayes estimator of \\(\\sigma^2\\).\nSuppose that \\(X_1, X_2,\\cdots, X_n\\) is a random sample from the distribution with pdf \\[\\begin{eqnarray*}\n% \\nonumber to remove numbering (before each equation)\n   f(x|\\theta) &=& \\theta x^{\\theta - 1},~~~0 &lt; x&lt;1, \\\\\n    &=& 0,~~~~~~~~\\mbox{otherwise}.\n\\end{eqnarray*}\\] Suppose also that the value of the parameter \\(\\theta\\) is unknown (\\(\\theta&gt;0\\)) and that the prior distribution of \\(\\theta\\) is \\(\\mbox{gamma}(\\alpha, \\beta)\\), \\(\\alpha &gt;0\\) and \\(\\beta&gt;0\\). Determine the posterior distribution of \\(\\theta\\) and hence obtain the Bayes estimator of \\(\\theta\\) under a squared error loss function.\n(Casella and Berger 2002) Suppose that we observe \\(X_1, X_2,\\cdots, X_n\\) where \\[\\begin{eqnarray*}\n% \\nonumber to remove numbering (before each equation)\n  X_i|\\theta_i &\\sim & \\mathcal{N}(\\theta_i, \\sigma^2),~~i=1, 2,\\cdots,n, ~~~\\mbox{independent}\\\\\n  \\theta_i &\\sim & \\mathcal{N}(\\mu,\\tau^2),~~~i=1, 2,\\cdots,n, ~~~\\mbox{independent}\n\\end{eqnarray*}\\]\n\n\nShow that the marginal distribution of \\(X_i\\) is \\(\\mathcal{N}(\\mu, \\sigma^2)\\) and that, marginally, \\(X_1, X_2,\\cdots,X_n\\) are iid. Empirical Bayes analysis would use the marginal distribution of \\(X_i\\)’s to estimate the prior parameters \\(\\mu\\) and \\(\\tau^2\\).\nShow, in general, that if \\[\\begin{eqnarray*}\n% \\nonumber to remove numbering (before each equation)\n  X_i|\\theta_i &\\sim & f(\\theta_i, \\sigma^2),~~i=1, 2,\\cdots,n, ~~~\\mbox{independent}\\\\\n  \\theta_i &\\sim & \\pi(\\theta|\\tau),~~~~i=1, 2,\\cdots,n, ~~~\\mbox{independent}\n\\end{eqnarray*}\\] then \\(X_1, X_2,\\cdots,X_n\\) are iid.\n\n\n(Wasserman 2004) Suppose that we observe \\(X_1, X_2,\\cdots, X_n\\) from \\(\\mathcal{\\mu, 1}\\). (a)Simulate a data set (using \\(\\mu=5\\)) consisting of \\(n=100\\) observations. (b)Take \\(\\pi(\\mu)=1\\) and find the posterior density and plot the density. (c)Simulate 1000 draws from the posterior and plot the histogram of the simulated values and compare the histogram to the answer in (b). (d)Let \\(\\theta= \\exp(\\mu)\\), then find the posterior density for \\(\\theta\\) analytically and by simulation. (e)Obtain a 95 percent posterior interval for \\(\\mu\\) and \\(\\theta\\).\n(Wasserman 2004) Consider the Bernoulli(\\(p\\)) observations: \\[0~1~0~1~0~0~0~0~0~0\\] Plot the posterior for \\(p\\) using these prior distributions for \\(p\\): \\(\\mbox{beta}(1/2, 1/2)\\), \\(\\mbox{beta}(10, 10)\\) and \\(\\mbox{beta}(100, 100)\\).\n(Wasserman 2004) Let \\(X_1, X_2,\\cdots,X_n \\sim \\mbox{Poisson}(\\lambda)\\). Find the Jeffrey’s prior. Also, find the corresponding posterior density function.\n\n\n\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical Inference. Second. Duxbury Advanced Series. India Edition: Cengage Learning.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in Statistical Inference. New York: Springer-Verlag New York, Inc."
  },
  {
    "objectID": "Bayesian Estimation for Linear Regression Problem.html#simulation-study",
    "href": "Bayesian Estimation for Linear Regression Problem.html#simulation-study",
    "title": "3  Bayesian Estimation for Linear Regression Problem",
    "section": "3.1 Simulation study",
    "text": "3.1 Simulation study\nFor checking purpose using simulation, we have fixed population parameters of a linear regression model as \\(\\beta_0\\) = 3, \\(\\beta_1\\) = 1 and \\(\\phi\\) = \\(1\\). Since we do not have the real data in our hand, so we created artificial data using following model with these fixed values of parameters (Figure 4.1),\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\mathcal{N}(\\mbox{mean} = 0, ~\\mbox{sd} =\\sqrt{\\phi}), i \\in \\{1,2,\\cdots,n\\}.\\]\nWe have prior for \\(\\beta_0\\) and \\(\\beta_1\\) is Normal distribution with mean(\\(\\mu_0\\)), variance(\\(\\tau_0\\)) and mean(\\(\\mu_1\\)), variance(\\(\\tau_1)\\), respectively. Parameter \\(\\phi\\) have prior following the inverse gamma distribution with shape(\\(\\alpha\\)) and rate(\\(\\gamma\\)). The hyper parameters \\(\\mu_0\\), \\(\\tau_0\\), \\(\\mu_1\\), \\(\\tau_1\\), \\(\\alpha\\), \\(\\gamma\\) are assumed to be known. After that we computed the conditional posterior distribution of \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\phi\\) assuming the above priors. Conditional posterior of \\(f(\\beta_0|\\beta_1,\\phi)\\) and \\(f(\\beta_1|\\beta_0,\\phi)\\) do not follow any common know distributions. So we have used grid approximation method for generating samples from the conditional posterior of \\(\\beta_0\\) and \\(\\beta_1\\). From this process we get the unstandardized posterior values at the grid points, then we standardized this values by dividing the sum of all unstandardized posterior values. We randomly select one grid sample and we iterate this process for 100000 times but we consider starting 70000 random sample as burn-in period and after a burn-in period. we want the independent random sample, to obtain independent sample we used thinning by taking every sixteen sample after the burn-in period in posterior of \\(\\beta_0\\) and \\(\\beta_1\\). In posterior of \\(\\phi\\) we used thinning by taking every third sample after the burn-in period. However, the choice is good should be decided by plotting \\(\\texttt{ACF}\\) of after thinning sample. After checking autocorrelation of thinning simulated sample, we draw histogram of \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\phi\\). Histogram of conditional posterior distribution of \\(\\beta_0\\) and \\(\\beta_1\\) of normal distribution and conditional posterior distribution of \\(\\phi\\) is inverse gamma distribution approximated using the posterior means of \\(\\beta_0\\) and \\(\\beta_1\\). Population parameters are fall within the credible interval of posterior distribution \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\phi\\). In developing the Gibbs sampling algorithm for Bayesian regression, we have used the inbuilt function \\(\\texttt{rnorm}\\) and \\(\\texttt{rinvgamma}\\) from \\(\\texttt{R}\\) base and \\(\\texttt{invgamma}\\) package respectively. However by virtue of the probability integral transform both of them can be simulated from uniform random number.\n\nbeta_0 = 3                                 # population intercept\nbeta_1 = 1                                 # population slope\nphi = 1                                    # population error variance\nx = seq(1, 5, length.out = 50)             # fixation of the population parameter\nset.seed(123)                              # we fixing a randomness in output\ny = beta_0 + beta_1 * x + rnorm(length(x), 0, sqrt(phi))\ndata = data.frame(y, x)                    # data containing population size\nplot(x,y, col = \"red\", pch = 19)\n\n\n\n\nFigure 3.1: Plot of simulated data for the study of linear regression generated using the model \\(y_i = \\beta_0 + \\beta_1 x_i + \\mathcal{N}(\\mbox{mean} = 0, ~\\mbox{sd} =\\sqrt{\\phi}), i \\in \\{1,2,\\cdots,n\\}\\). The parameters values are fixed as \\(\\beta_0 = 3,~\\beta_1 = 1,~\\phi = 1\\), and generated data of length \\(n=50\\).\n\n\n\n\nIn the above code the population parameters have been fixed for simulation purpose. The parameters are set as \\(\\beta_0 = 3\\), \\(\\beta_1 = 1\\) and \\(\\phi = 1\\). The seed has been fixed so that the results can be reproduced later as well. The reader is encouraged to run the complete code step by step rather than running it completely in a single step. In the following section, we specify the prior distribution for the model parameters.\n\n# Prior for beta_0\nmu_0 = 2; tau_0 = 0.4                      # prior parameter\nprior_beta_0 = function(x){                # function for prior beta_1\n  dnorm(x, mean = mu_0, sd = sqrt(tau_0))\n}\n\n# Prior for beta_1\nmu_1 = 2; tau_1 = 0.5                      # prior parameter\nprior_beta_1 = function(x){                # function for prior beta_1\n  dnorm(x, mean = mu_1, sd = sqrt(tau_1))\n}\n# Prior for phi. \nlibrary(invgamma)                          #\nalpha = 2; gamma = 2                       # prior parameters\nprior_phi = function(x){                   # function for prior phi\n  dinvgamma(x, shape = alpha, rate = gamma)\n}\n\nThe following code describes the likelihood function. Recall that to compute the posterior we have to multiply the likelihood function with the prior distribution. So, here we write separate function for the likelihood function. The likelihood function is the joint distribution of the data values where the parameters are considered as variable. Also, note that the function directly compute the log-likelihood; this is is helpful to avoid truncation errors made by the software.\n\nlikelihood = function(data, params){   # likelihood function of data given parameters\n  y = data[,1]\n  x = data[,2]\n  n = nrow(data)\n  beta_0 = params[1]\n  beta_1 = params[2]\n  phi = params[3]\n  log_lik = -n*log(sqrt(2*pi)) - (n/2)*log(phi) - sum((y - beta_0-beta_1*x)^2)/(2*phi)\n  # log-likelihood of data given parameters\n  return(exp(log_lik))\n}\n\nThe following codes are self explanatory. Sufficient comments have been included. Note that we have created the grid values by dividing an interval with a step size of 0.01. Choice of this interval plays critical role for a successful grid approximation. This interval is essentially act as a support for the posterior distribution of the designated parameter. The simulation would start with an initial choice of the parameters \\(\\beta_0^{(0)}\\), \\(\\beta_1^{(0)}\\) and \\(\\phi^{(0)}\\) which have been simulated from the prior distribution.\n\n# storage for posterior samples\niteration = 100000                        # number of iterations\npost_beta_0 = rep(NA, iteration)          # storage for posterior of beta_0\npost_beta_1 = rep(NA, iteration)          # storage for posterior of beta_1\npost_phi = rep(NA, iteration)             # storage for posterior of phi\n# Specification of grids\nstep = 0.01                                      # grid step\ngrid_beta_0 = seq(from = -5, to = 10, by = step) # fixation of the grid of beta_0\ngrid_beta_1 = seq(from = -5, to = 5, by = step)  # fixation of the grid of beta_1\n# Initialization of the posterior samples\npost_beta_0[1] = rnorm(1, mean = mu_0, sd = sqrt(tau_0))   # posterior beta_0\npost_beta_1[1] = rnorm(1, mean = mu_1, sd = sqrt(tau_1))   # posterior beta_1\npost_phi[1] = rinvgamma(n = 1, shape = alpha, rate = gamma)# posterior phi\n\nThe following codes give the code for Gibbs sampling method to generate the posterior distribution from the marginal posterior distribution. In the text, it has been mentioned that we do not have idea about the joint posterior distribution of the parameters, so we simulate from the marginal posterior distribution. Since, the exact functional form of the marginal posterior is also not known, we approximate it by using grid approximation.\n\nfor(i in 2:iteration){\n  \n  # section for posterior sample of beta_0\n  tmp_post_beta_0 = rep(NA, length(grid_beta_0))\n  for(j in 1:length(grid_beta_0)){\n    params = c(grid_beta_0[j], post_beta_1[i-1], post_phi[i-1])\n    tmp_post_beta_0[j] = likelihood(data = data, params = params) \n                                * prior_beta_0(grid_beta_0[j])\n  }\n  prob = tmp_post_beta_0/sum(tmp_post_beta_0)\n  post_beta_0[i] = sample(grid_beta_0, 1, prob = prob) # sample of posterior\n  beta_0 after normalize\n  \n  # section for posterior sample of beta_1\n  tmp_post_beta_1 = rep(NA, length(grid_beta_1))\n  for(j in 1:length(grid_beta_1)){\n    params = c(post_beta_0[i], grid_beta_1[j], post_phi[i-1])\n    tmp_post_beta_1[j] = likelihood(data = data, params= params) \n                                * prior_beta_1(grid_beta_1[j])\n  }\n  prob = tmp_post_beta_1/(sum(tmp_post_beta_1))\n  post_beta_1[i] = sample(grid_beta_1, 1, prob = prob) # sample of posterior\n  beta_1 after normalize\n  \n  # section for posterior sample of phi\n  shape = alpha + nrow(data)/2\n  rate = (1/2)*sum((y - post_beta_0[i] - post_beta_1[i]*x)^2) + gamma\n  post_phi[i] = rinvgamma(1, shape = shape, rate = rate)  # sample of posterior phi\n}\n\nTrace plots are useful tools to convergence of the chains which are expected to converge the posterior distribution. After an initial burn in period, the values would act as samples from the target (desired posterior) distribution. In the following we have considered first 70% of the simulated values as the burn in step. There is no hard and fast rule for it.\n\n# Trace plots\npar(mfrow = c(2,2))\n# trace plot of posterior of beta_0\nplot(post_beta_0, type = \"l\", main = bquote(\"Trace plot of\"~ beta[0]),col = \"red\")\n# trace plot of posterior of beta_1\nplot(post_beta_1, type = \"l\", main = bquote(\"Trace plot of\"~ beta[1]),col = \"red\")\n# trace plot of posterior of phi\nplot(post_phi, type = \"l\", main = bquote(\"Trace plot of\"~ phi),col = \"red\")\n\nIt is important to note that simulation of \\(\\beta_0^{(j)}\\) depends on the value of \\(\\beta_0^{(j-1)}\\). Similarly for other parameters as well. Thus, the posterior values are identically distributed (after a sufficiently large step) but not independent. So, the test for autocorrelation has been performed as the values have been selected accordingly. For example, we may consider every 10th values after the burn in period. This is also known as thinning. The function \\(\\texttt{acf()}\\) has been utilized for this purpose.\n\n# Plot of conditional posterior density functions of beta_0, beta_1, phi\ncut = 0.7                                           # for cutting starting 70% sample\npar(mfrow = c(2,3))\n\n# Section for plot acf of posterior beta_0\nu = post_beta_0[ceiling(iteration*cut):iteration]   # after burn-in period\nindex = seq(1,length(u), by = 16)                   # thining\npost_beta_b0_thinning = u[index]                    # values after thinning\nacf(post_beta_b0_thinning,main = bquote(\"acf of posterior \"~beta[0])) #checking autocorrelation\n\n# Section for plot acf of posterior beta_1\nu = post_beta_1[ceiling(iteration*cut):iteration]   # after burn-in period\nindex = seq(1,length(u), by = 16)                   # thinning\npost_beta_b1_thinning = u[index]                    # values after thinnig\nacf(post_beta_b1_thinning,main = bquote(\"acf of posterior \"~beta[1]))# checking autocorrelation\n\n# Section for plot acf of posterior phi\nu = post_phi[ceiling(iteration*cut):iteration]      # after burn-in period\nindex = seq(1,length(u), by = 3)                    # thinning\npost_phi_thinning = u[index]                        # values after thinning\nacf(post_phi_thinning,main = bquote(\"acf of posterior \"~phi)) # checking autocorrelation\n\nAfter the thinning has been done, we visualize the approximated distribution by means of histogram. As we can see that the posterior density of \\(\\beta_0\\) and \\(\\beta_1\\) can be well approximated by the normal distribution. The posterior density of \\(\\phi\\) is positively skewed and can be well approximated by the inverted gamma density function. This is due to conjugacy which has been mentioned earlier. The exact posterior density for \\(\\phi\\) has been computed using the mean of the posterior samples of \\(\\beta_0\\) and \\(\\beta_1\\).\n\n# Section for plot histogram of posterior beta_0 after thinning\nhist(post_beta_b0_thinning, probability = TRUE, \n    main = bquote(\" Posterior of \"~ beta[0]),\n    xlab = expression(beta[0]),col = \"grey\")\ncredible_interval_beta_0 = quantile(post_beta_b0_thinning, c(2.5, 97.5)/100)\nabline(v = credible_interval_beta_0,col = \"blue\",lwd = 2)\nlegend(\"topright\",legend = \"Credible Interval\",lwd =2,col = \"blue\",lty = 2,cex = 0.5)\n\n# Section for plot histogram of posterior beta_1 after thinning\nhist(post_beta_b1_thinning,probability = TRUE,\n    main = bquote(\"Posterior of \"~beta[1]),\n    col = \"grey\",xlab = expression(beta[1]))\ncredible_interval_beta_1 = quantile(post_beta_b1_thinning, c(2.5, 97.5)/100)\nabline(v = credible_interval_beta_1, col = \"blue\", lwd = 2)\nlegend(\"topright\",legend = \"Credible Interval\",lwd =2,col = \"blue\",lty = 2,cex = 0.5)\n\n# Section for plot histogram of posterior phi after thinning\nhist(post_phi_thinning, probability = TRUE,\n    main = bquote(\"psterior of \"~ phi),\n    col =\"grey\",ylim = c(0,3), xlab = expression(phi))\ncredible_interval_phi = quantile(post_phi_thinning, c(2.5, 97.5)/100)\nabline(v=credible_interval_phi,col = \"blue\",lwd = 2)\nlegend(\"topright\",legend = c(\"Credible Interval\",\"True curve of  phi\"),\n    lwd = c(2,2), col = c(\"blue\",\"red\"), lty = 2, cex = 0.5)\n\n# Approximating the posterior distribution of phi using the posterior means of beta_0 and beta_1\nmean_post_beta_0 = mean(post_beta_b0_thinning)\nmean_post_beta_1 = mean(post_beta_b1_thinning)\nshape = alpha + nrow(data)/2\nrate = (1/2)*sum((y - mean_post_beta_0 - mean_post_beta_1*x)^2) + gamma\ncurve(dinvgamma(x, shape = shape, rate = rate),add = TRUE, lwd=2, col = \"red\")\n\n\n\n\nFigure 3.2: Trace plot of posterior \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\phi\\).\n\n\n Up to this point, we have developed a clear understanding of linear regression from scratch under the Bayesian framework and demonstrated it on simulated data for better comprehension. The Bayesian estimation of the linear regression parameters has been conducted using custom-written code, without relying on any R packages. This approach provides a transparent view of the underlying processes. Once the mechanism is well understood, the same analysis can be performed using built-in packages. Some of the packages available in R for Bayesian linear regression are \\(\\texttt{rstanarm}\\) (Goodrich et al. 2024), and \\(\\texttt{brms}\\) (Bürkner 2017). The \\(\\texttt{rstanarm}\\) package provides a user-friendly interface to fit Bayesian models using Stan, while \\(\\texttt{brms}\\) extends this by allowing complex model structures using formula syntax.\n\n\n\n\nBürkner, Paul-Christian. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2024. “Rstanarm: Bayesian Applied Regression Modeling via Stan.” https://mc-stan.org/rstanarm/.\n\n\nKahle, David, and James Stamey. 2017. Invgamma: The Inverse Gamma Distribution. https://CRAN.R-project.org/package=invgamma."
  },
  {
    "objectID": "Bayesian Estimation to Nonlinear Regression Problem.html#sec-estimation_of_parameters",
    "href": "Bayesian Estimation to Nonlinear Regression Problem.html#sec-estimation_of_parameters",
    "title": "4  Bayesian Estimation to Nonlinear Regression Problem",
    "section": "4.1 Estimation of Parameters",
    "text": "4.1 Estimation of Parameters\nConsider \\(\\{ N_1,N_2,\\cdots,N_{n+1}\\}\\) be the observed time series data. The absolute change in population size at time \\(t\\) is approximated as \\[\\frac{\\mathrm{d}N(t)}{\\mathrm{d}t} \\approx \\frac{N(t+\\Delta t)-N(t)}{\\Delta t}.\\] We assume the yearly changes in population size, so that \\(\\Delta t=1\\). The relative changes in population sizes is given by\n\\[\\begin{equation}\nR(t) = \\frac{1}{N}\\frac{\\mathrm{d}N(t)}{\\mathrm{d}t} = \\frac{\\mathrm{d}}{\\mathrm{d}t} \\log N \\approx \\log N(t+1)- \\log N(t).\n\\end{equation}\\]\nSo, for a given population time series data of size \\(n+1\\), we have \\(n\\) many observed per capita growth rates \\(\\{R(1),R(2),\\cdots,R(n)\\}\\). We would like to investigate the dynamics of the population when it is subject to environmental stochastic perturbations alone. The following stochastic differential equation has been used to describe to population changes that is exposed to environmental variability.\n\\[\\begin{equation}\\label{eq:theta logistic}\n\\mathrm{d}N = r_m N(t) \\left[ 1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right]\\mathrm{d}t + \\sigma_{e} N(t)\\mathrm{d}W(t) ,\n\\end{equation}\\]\nwhere \\(\\sigma_{e}\\) represents the intensity of the stochastic perturbation and \\(W(t)\\) is the one dimensional Brownian motion which satisfies \\(\\mathrm{d}W(t) \\approx W(t+\\mathrm{d}t)-W(t) \\sim \\mathcal{N}(0,\\mathrm{d}t)\\). Our goal is to estimate the parameters of the stochastic model by employing Bayesian statistical methods. First we compute the likelihood function of the model parameters given the population size. It is to be noted that the likelihood function can be written either by conditional on the population size \\(\\{N_t\\}\\) or on the growth rates \\(\\{R_t\\}\\). We can use the either observations on \\(\\{R_t\\}\\) or \\(\\{N_t\\}\\) to obtain the posterior distribution of the model parameters. Now, we shall write down the likelihood function given the population time series. Using the equation \\(\\eqref{eq:theta logistic}\\) and utilizing the distributional assumptions, we see that the absolute changes in the population size, conditional on the current size, is normally distributed. So we obtain the following (assuming \\(\\Delta t =1\\)):\n\\[\\begin{align}\nN(t+1)-N(t)|N(t) &\\sim \\mathcal{N}\\left(r_m N(t) \\left[ 1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right], \\sigma_{e}^2 N(t)^2 \\right) \\notag \\\\\n\\frac{N(t+1)-N(t)}{N(t)}| N(t) &\\sim \\mathcal{N}\\left(r_m  \\left[ 1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right], \\sigma_{e}^2 \\right) \\notag \\\\\nN(i+1)-N(i) &\\sim \\mathcal{N}\\left(r_m N(i)\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta} \\right], \\sigma_{e}^2 N(i)^2 \\right) \\notag \\\\\nN(i+1)|N(i) &\\sim \\mathcal{N}\\left(N(i) + r_m N(i)\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta} \\right], \\sigma_{e}^2 N(i)^2 \\right) \\label{eq:N_i}\n\\end{align}\\]\nWe can estimate the parameters using (\\(\\ref{eq:N_i}\\)) using the method of maximum likelihood. Therefore by using (\\(\\ref{eq:N_i}\\)), our goal is to estimate parameters \\(r_m\\), \\(\\theta\\), \\(K\\), \\(\\sigma_e^2\\). If we use \\(R_t\\) values, then the following equation will be used for computation of the likelihood.\n\\[\\begin{equation}\nR(t)|N(t)  \\sim \\mathcal{N}\\left(r_m\\left[1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right],\\sigma_{e} ^2\\right)  \\label{eq:R_T},\n\\end{equation}\\] so that the log-likelihood function will be,\n\\[\\begin{equation}\\label{eq:likelihood}\n\\mbox{likelihood} = -\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log(\\sigma_e^2)-\\frac{1}{2\\sigma_e^2}\\sum_{i=1}^{n}\\left\\lbrace R(i)- r_m\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta}\\right] \\right\\rbrace^2 .\n\\end{equation}\\]\nTo draw inference about \\(r_m\\), \\(\\theta\\), \\(K\\) and \\(\\sigma_e^2\\), we consider inverted gamma prior for variance term \\(\\sigma_e^2\\), gamma prior for \\(\\theta\\), normal prior for \\(K\\) and normal prior for \\(r_m\\). Then the complete Bayesian model for this data can be written as,\n\\[\\begin{align*}\nN(i+1)|N(i),r_m,\\theta,K,\\sigma_e^2 &\\sim \\mathcal{N}\\left(N(i) + r_m N(i)\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta} \\right], \\sigma_{e}^2 N(i)^2 \\right) \\\\\nr_m|\\alpha_0,\\beta_0 &\\sim \\mathcal{N}(\\alpha_0,\\beta_0)\\\\\nK| \\mu_K,\\sigma_K^2 &\\sim \\mathcal{N}(\\mu_K,\\sigma_K^2)\\\\\n\\theta|\\alpha_1,\\beta_1 &\\sim \\mathcal{G}(\\alpha_1,\\beta_1)  \\\\\n\\sigma_e^2|\\alpha_2,\\beta_2 &\\sim \\mathcal{IG}(\\alpha_2,\\beta_2),\n\\end{align*}\\]\nwhere \\(\\alpha_0,\\alpha_1,\\alpha_2,\\beta_0,\\beta_1,\\beta_2,\\mu_K\\), and \\(\\sigma_K^2\\) are hyper-parameters and assumed to be known. As the starting population size is known, \\(P(N(1)=N_0)=1\\). We take \\(N(1)=N_0\\) as the starting population size and assumed to be a fixed quantity. The joint posterior distribution of \\(r_m,\\theta,K,\\sigma_e^2\\) can be written as,\n\\[\\begin{equation*}\nf(r_m,\\theta,K,\\sigma_e ^2|\\boldsymbol{N})=\\frac{f(\\boldsymbol{N},r_m,\\theta,K,\\sigma_e ^2)}{f_{\\boldsymbol{N}}(\\boldsymbol{N})},\n\\end{equation*}\\] where \\(\\boldsymbol{N}=(N(1),N(2),\\cdots,N(n+1))'\\) represents the data and \\(f_{\\boldsymbol{N}}(\\boldsymbol{N})\\) is the marginal distribution of \\(\\boldsymbol{N}\\).\n\\[\\begin{equation}\\label{eq:27}\nf\\left(r_m,\\theta,K,\\sigma_e^2|\\boldsymbol{N}\\right) = \\frac{f\\left(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2\\right)\\cdot f\\left(r_m,\\theta,K,\\sigma_e^2\\right)}{f(\\boldsymbol{N})}.\n\\end{equation}\\] We assume that \\(r_m,\\theta,K,\\sigma_e^2\\) are independent variables, therefore \\(f(r_m,\\theta,K,\\sigma_e ^2)\\) can be written as,\n\\[\\begin{equation}\nf\\left(r_m,\\theta,K,\\sigma_e^2\\right)=f(r_m)\\cdot f(\\theta) \\cdot f(K) \\cdot f\\left(\\sigma_e^2\\right).\n\\end{equation}\\] So, equation (\\(\\ref{eq:27}\\)) becomes, \\[\\begin{eqnarray*}\nf\\left(r_m,\\theta,K,\\sigma_e^2|\\boldsymbol{N}\\right) &\\propto& f\\left(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2\\right)\\cdot f\\left(r_m,\\theta,K,\\sigma_e^2\\right) \\\\\n&\\propto& f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e ^2)\\cdot f(r_m)\\cdot f(\\theta) \\cdot f(K) \\cdot f(\\sigma_e ^2)~~[\\mbox{independence of priors}]  \\\\\n&\\propto&  \\left[\\prod_{i=1}^{n} f\\left(N(i+1)|N(i)\\right)\\right]\\cdot f(r_m)\\cdot f(\\theta) \\cdot f(K) \\cdot f(\\sigma_e ^2).\n\\end{eqnarray*}\\]\nThe right hand side of the above expression is product of the likelihood and the prior, which is nothing but unnormalized joint posterior distribution of parameters. Since, the distribution does not follow some common known distribution, we use the Gibbs sampling method that simulates samples from the conditional distribution. So we generate random sample from conditional posterior distribution of each parameters. However, it is observed that in this case also the conditional posterior does not follow any known distribution. So, we use grid approximation algorithm to approximate the posterior probability distribution.\nLet \\(\\boldsymbol{\\beta} = \\left(r_m,\\theta, K, \\sigma_e^2\\right)\\). Since, \\(N(i)\\)’s are not independent, so the likelihood can be written as,\n\\[\\begin{eqnarray*}\n\\mbox{likelihood} &=& f\\left(N(1),N(2),\\cdots , N(n+1) ; \\beta \\right)   \\\\\n&=& f\\left(N(n+1)|N(n) ; \\beta \\right)\\cdot f\\left(N(n)|N(n-1) ; \\beta \\right) \\cdots f\\left(N(2)|N(1) ; \\beta \\right)\\cdot f\\left(N(1) ; \\beta \\right) \\\\\n&=& \\prod_{i=1}^{n} f\\left(N(i+1)|N(i) ; \\beta \\right)\\cdot f\\left(N(1); \\beta \\right).\n\\end{eqnarray*}\\]\nWe assume that the initial population size to be known so that \\(f\\left(N(1);\\beta \\right) = 1\\).\n\\[\\begin{eqnarray*}\n\\mbox{likelihood} &=& \\prod_{i=1}^{n} f\\left(N(i+1)|N(i) ; \\beta \\right) \\\\\n&=& \\prod_{i=1}^{n}\\left[\\frac{1}{\\sqrt[]{2\\pi}\\sigma_e^2 N(i)} e^{-\\frac{\\left\\lbrace N(i+1)-N(i)- r_m N(i)\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta} \\right]\\right\\rbrace^2} {2\\sigma_e^2 N(i)^2}}\\right]\\\\\n&=&\\prod_{i=1}^{n} \\left[\\frac{1}{\\sqrt[]{2\\pi}\\sigma_e^2 N(i)} e^{{-\\frac{1}{2\\sigma_e^2}\\left\\lbrace \\frac{N(i+1)-N(i)}{N(i)}-r_m\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta}\\right] \\right\\rbrace}^{\\theta} }\\right]\\\\\n&=& \\frac{1}{(\\sqrt[]{2\\pi})^n}\\frac{1}{(\\sigma_e)^n}\\frac{1}{\\prod_{i=1}^{n} N(i)} e^{-\\frac{1}{2\\sigma_e^2}\\sum_{i=1}^{n}\\left\\lbrace R(i)- r_m\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta}\\right]  \\right\\rbrace^2}.\n\\end{eqnarray*}\\]\nThe log-likelihood can be given as,\n\\[\\begin{equation}\n\\mbox{log-likelihood} = -\\frac{n}{2}\\log(2\\pi)-\\frac{n}{2}\\log(\\sigma_e^2)-\\sum_{i=1}^{n}\\log(N(i))-\\frac{1}{2\\sigma_e^2}\\sum_{i=1}^{n}\\left\\lbrace R(i)- r_m\\left[1-\\left( \\frac{N(i)}{K}\\right)^{\\theta}\\right] \\right\\rbrace^2.\n\\end{equation}\\]\nTo compute the conditional posterior following process has been carried out,\n\\[\\begin{eqnarray*}\nf(r_m|\\theta,K,\\sigma_e^2,\\boldsymbol{N})&=&\\frac{f(\\boldsymbol{N},r_m,\\theta,K,\\sigma_e^2)}{f(\\theta,K,\\sigma_e^2,\\boldsymbol{N})}\\\\\n&=& \\frac{f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2)\\cdot f(r_m,\\theta,K,\\sigma_e^2)}{f(\\boldsymbol{N},\\theta,K,\\sigma_e^2)}\\\\\n&=&\\frac{f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2)\\cdot\\pi(r_m)\\pi(\\theta)\\pi(K)\\pi(\\sigma_e^2)}{f(\\boldsymbol{N}|\\theta,K,\\sigma_e^2)\\cdot f(\\theta,K,\\sigma_e^2)}\\\\\n&=&\\frac{f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2)\\cdot \\pi(r_m)\\cdot \\pi(\\theta)\\cdot \\pi(K)\\cdot\\pi(\\sigma_e^2)}{f(\\boldsymbol{N}|\\theta,K,\\sigma_e^2)\\cdot \\pi(\\theta)\\cdot \\pi(K)\\cdot\\pi(\\sigma_e^2)}\\\\\n&=&\\frac{f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2)\\cdot \\pi(r_m)}{f(\\boldsymbol{N}|\\theta,K,\\sigma_e^2)}.\n\\end{eqnarray*}\\] Let \\(m=f(\\boldsymbol{N}|\\theta,K,\\sigma_e^2)\\), be the joint marginal probability density function of the data. \\[\\begin{eqnarray}\nf(r_m|\\theta,K,\\sigma_e^2,\\boldsymbol{N}) &\\propto& f(\\boldsymbol{N}|r_m,\\theta,K,\\sigma_e^2)\\cdot \\pi(r_m) \\notag\\\\\n&\\propto& \\prod_{i=1}^{n} f(N_{i+1}|N_i ; r_m,\\theta,K,\\sigma_e^2)\\cdot  \\pi(r_m).\n\\end{eqnarray}\\] Similarly,\n\\[\\begin{eqnarray}\nf(\\theta|r_m,K,\\sigma_e^2,\\boldsymbol{N})&\\propto& \\prod_{i=1}^{n} f(N_{i+1}|N_i ; r_m,\\theta,K,\\sigma_e^2)\\cdot  \\pi(\\theta)\\\\\nf(K|r_m,\\theta,\\sigma_e^2,\\boldsymbol{N})&\\propto& \\prod_{i=1}^{n} f(N_{i+1}|N_i ; r_m,\\theta,K,\\sigma_e^2)\\cdot  \\pi(K)\\\\\nf(\\sigma_e^2|r_m,\\theta,K,\\boldsymbol{N}) &\\propto& \\prod_{i=1}^{n} f(N_{i+1}|N_i ; r_m,\\theta,K,\\sigma_e^2)\\cdot  \\pi(\\sigma_e^2).\n\\end{eqnarray}\\]\nIn Bayesian linear regression we already verify that the variance (specially \\(\\phi\\)) after observed data follows \\(\\mathcal{IG}\\left( \\mbox{shape} = \\alpha+\\frac{n}{2}, \\mbox{rate} =\\frac{1}{2} \\sum_{i=1}^{n}\\left(y_i-\\beta_0-\\beta_1x_i\\right)^2 + \\gamma\\right)\\). Similarly here we expect posterior distribution of variance(\\(\\sigma_e^2\\)) as follows:\n\\[\\begin{equation}\\label{eq:post_sigma_sq}\n\\sigma_e^2|\\boldsymbol{N} \\sim \\mathcal{IG}\\left( \\mbox{shape} = \\alpha_2 +\\frac{n}{2}, \\mbox{rate} =\\frac{1}{2}\\sum_{i=1}^{n}\\left(y_i- r_m \\left\\lbrace 1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right\\rbrace\\right)^2+\\beta_2\\right).\n\\end{equation}\\]\n\n\n\n\n\n\nNote\n\n\n\nIf the prior distribution \\(\\pi(r_m)\\) is considered to be \\(\\mbox{gammma}(\\alpha, \\beta)\\) which takes only positive values, then task of estimation of a declining population may be underestimated.\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo reduce the computational time a correct choice of priors is required. In this case, since \\(K\\) is a property of the environment, it should not change drastically. Posterior sample of k should not vary too much away from the prior mean of \\(K\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the prior distribution \\(\\pi(r_m)\\) is considered to be \\(\\text{gamma}(\\alpha, \\beta)\\) which takes only positive values, then the task of estimation of a declining population may be underestimated.\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo reduce the computational time, a correct choice of priors is required. In this case, since \\(K\\) is a property of the environment, it should not change drastically. The posterior sample of \\(K\\) should not vary too much from the prior mean of \\(K\\).\n\n\nThe above structure of the posterior density function of \\(\\sigma_e^2\\) given in the equation \\(\\eqref{eq:post_sigma_sq}\\) is verified by the simulation for the simulated data."
  },
  {
    "objectID": "Bayesian Estimation to Nonlinear Regression Problem.html#simulation-study",
    "href": "Bayesian Estimation to Nonlinear Regression Problem.html#simulation-study",
    "title": "4  Bayesian Estimation to Nonlinear Regression Problem",
    "section": "4.2 Simulation study",
    "text": "4.2 Simulation study\n\n4.2.1 Data Generation\nTime series data of population size is generated using \\(r_m=0.5,~~ \\theta=0.5,~~K=50,~~\\sigma_e^2=0.0025\\). The initial population size is set as \\(N_0 = 23\\) and the length of the simulated series is \\(n=30\\). The full Bayesian model is described as follows:\n\\[\\begin{align}\nR(t)|N(t) &\\sim  \\mathcal{N}\\left(r_m\\left[1-\\left( \\frac{N(t)}{K}\\right)^{\\theta} \\right],\\sigma_{e} ^2\\right)\\label{eq:logistic model} \\\\\nr_m|\\alpha_0,\\beta_0 &\\sim \\mathcal{N}(\\alpha_0,\\beta_0)\\notag\\\\\nK| \\mu_K,\\sigma_K^2 &\\sim  \\mathcal{N}(\\mu_K,\\sigma_K^2)\\notag\\\\\n\\theta|\\alpha_1,\\beta_1 &\\sim \\mathcal{G}(\\alpha_1,\\beta_1)\\notag\\\\\n\\sigma_e^2|\\alpha_2,\\beta_2 &\\sim \\mathcal{IG}(\\alpha_2,\\beta_2).\\notag\n\\end{align}\\]\n\n# parameters values for simulating the data\nrm = 0.5 # intrinsic growth rate\ntheta = 0.5 # parameter for strength of density dependence\nK = 50 # carrying capacity of the environment\nsig_2e = 0.0025 # variance of the environmental perturbation\nn = 30 # length of the time series = n+1\nN1 = 23 # initial population size\n\nset.seed(471)\nN = numeric(n + 1) # storage the simulated time series\nN[1] = N1 # initial population size.\nfor(i in 1:(length(N)-1)){\n  r = rnorm(n =1, mean = rm*(1-(N[i]/K)^theta), sd = sqrt(sig_2e)) #   simulating growth rate\n  N[i+1] = N[i]*exp(r) # simulate next generation\n}\n#print(N) # printing population size\nplot(1:(n+1), N, type= \"p\", lwd=2, col = \"red\", pch = 19)\n\nfun_logistic = function(x){ # Deterministic solution of the generalized logistic equation\nK/(1+((K/N1)^theta-1 )*exp(-rm*x*theta) )^(1/theta)\n}\ncurve(fun_logistic, 0, n+1, add= TRUE, lwd=2)\n\n\n\n\nFigure 4.1: Plot of simulated data for the study of Nonlinear regression generated using the model given in the equation (\\(\\ref{eq:logistic model}\\)). The parameters values are fixed as \\(r_m=0.5, \\theta=0.5, K=50, \\sigma_e^2=0.0025\\), and initial population size taken as \\(N_0=23\\), generated data of length \\(n=30\\).\n\n\n\n\n\n\n4.2.2 Define Prior distributions\nFor generated time series, we want to generate values from posterior of each parameter\\((r_m,K,\\theta,\\sigma_e^2)\\). For simulation purpose, we first define the prior functions for the prior distribution given in the equation (\\(\\ref{eq:logistic model}\\)) by considering the values of hyper-parameters as \\(\\beta_0=1, \\sigma_K^2=3, \\alpha_1=1, \\beta_1=1, \\alpha_2=1, \\beta_2= 0.0001\\). Prior mean of \\(K\\) and \\(r_m\\) that is \\(\\mu_K\\) and \\(\\alpha_0\\) are obtained by fitting the logistic growth equation using nonlinear least squares method (\\(\\texttt{nls}\\), in \\(\\texttt{R}\\)). The package \\(\\texttt{invgamma}\\) (Kahle and Stamey 2017) has been used for generating random numbers from the inverted gamma distribution.\n\nprior_mean_rm = coef(fit)[1]\nprior_sd_rm = 1\nfun_prior_rm = function(x){ # Prior specification for r_m\n  dnorm(x, prior_mean_rm, prior_sd_rm)\n}\n\nprior_mean_K = coef(fit)[2]\nprior_sd_K = 3\nfun_prior_K = function(x){ # Prior specification for K\n  dnorm(x, prior_mean_K, prior_sd_K)\n}\n\nalpha_1 = 1\nbeta_1 = 1\nfun_prior_theta = function(x){ # Prior specification for theta\n  dgamma(x, shape = alpha_1, rate = beta_1)\n}\n\nlibrary(invgamma)\nalpha_2 = 1\nbeta_2 = 0.0001\nfun_prior_sig_2e = function(x){ # Prior specification for inverted gama\n  dinvgamma(x, shape = alpha_2, rate = beta_2)\n}\n\nAs mentioned in the Section 4.1, the exact form of the conditional posterior distribution is not known, so grid approximation has been utilized to approximate the conditional posterior density function. In the following code, we have created grid within an interval specified for each parameter. This initial choice is important as this act as a support for the density function. Finer the grid is more accurate is the posterior approximation.\n\n# Specification of grid values for approximating the posterior\nstep = 0.01\ngrid_rm = seq(from = -3, to =3, by = step)\ngrid_K = seq(from = 40, to = 60, by = step)\ngrid_theta = seq(from = 0, to =10, by = step)\ngrid_sig_2e = seq(from = 0.0001 , to =0.01, by = 0.00001)\n\n\n\n4.2.3 Likelihood function\nIn the following code we define the likelihood function. Note that we have directly computed the log-likelihood values to avoid truncation error.\n\nlikelihood = function(data, param){\n  rm = param[1]\n  K = param[2]\n  theta = param[3]\n  sig_2e = param[4]\n\n  x = data[,1] # population size\n  y = data[,2] # per capita growth rates\n\n  log_lik = -n*log(sqrt(2*pi)) - (n/2)*log(sig_2e) - sum(((y-rm*(1-(x/K)^theta))^2/(2*sig_2e)))\n  return(exp(log_lik))\n}\n\n\n\n4.2.4 Calculation of Posterior distribution\nFirst of all we have set 100000 as a number Markov Chain samples to be generated. Then we have fixed the initial values of the parameters and defined the arrays for storing the posterior values of the parameters in \\(\\texttt{R}\\) as follows:\n\niter = 100000 # Length of the chain to be simulated\npost_rm = rep(NA, iter) # posterior values of r_m\npost_K = rep(NA, iter) # posterior values of K\npost_theta = rep(NA, iter) # posterior values of theta\npost_sig_2e = rep(NA, iter) # posterior values of simga_2e\n\nparam = c(0.2, 40, 1, 0.02) # starting of the chain\n\n# initialization of the chain\npost_rm[1] = param[1]\npost_K[1] = param[2]\npost_theta[1] = param[3]\npost_sig_2e[1] = param[4]\n\nNext we carried out the Gibbs sampling and grid approximation of the posterior distribution. At each Gibbs sampling step (outer loop), the conditional posterior has been approximated by grid approximation (four inner loop for four parameters).\n\nfor(i in 2:iter){ # loop for iteration of Gibbs sampling\n  # section for rm\n  tmp_post_rm = numeric(length = length(grid_rm))\n  for(j in 1:length(grid_rm)){\n      param = c(grid_rm[j], post_K[i-1], post_theta[i-1], post_sig_2e[i-1])\n      tmp_post_rm[j] = likelihood(data = data, param = param)*fun_prior_rm(grid_rm[j])\n    }\n  prob = tmp_post_rm/sum(tmp_post_rm)\n  post_rm[i] = sample(grid_rm, size = 1, prob = prob)\n\n  # section for K\n  tmp_post_K = numeric(length = length(grid_K))\n  for(j in 1:length(grid_K)){\n    param = c(post_rm[i], grid_K[j], post_theta[i-1], post_sig_2e[i-1])\n    tmp_post_K[j] = likelihood(data = data, param = param)*fun_prior_K(grid_K[j])\n    }\n  prob = tmp_post_K/sum(tmp_post_K)\n  post_K[i] = sample(grid_K, size = 1, prob = prob)\n\n  # section for theta\n  tmp_post_theta = numeric(length = length(grid_theta))\n  for(j in 1:length(grid_theta)){\n    param = c(post_rm[i], post_K[i], grid_theta[j], post_sig_2e[i-1])\n    tmp_post_theta[j] = likelihood(data = data, param = param)*fun_prior_theta(grid_theta[j])\n    }\n  prob = tmp_post_theta/sum(tmp_post_theta)\n\n  post_theta[i] = sample(grid_theta, size = 1, prob = prob)\n\n  # section for sig_2e\n  tmp_post_sig_2e = numeric(length = length(grid_sig_2e))\n  for(j in 1:length(grid_sig_2e)){\n    param = c(post_rm[i], post_K[i], post_theta[i], grid_sig_2e[j])\n    tmp_post_sig_2e[j] = likelihood(data = data, param = param)*fun_prior_sig_2e(grid_sig_2e[j])\n    }\n  prob = tmp_post_sig_2e/sum(tmp_post_sig_2e)\n  post_sig_2e[i] = sample(grid_sig_2e, size = 1, prob = prob)\n\n} # loop ends for Gibbs iteration\n\nBy executing the above code, we have obtained the 1,00,000 sample values from the posterior density of \\(r_m, K, \\theta\\), and \\(\\sigma_e^2\\). We stored them in the local directory on system for the further analysis.\n\npost_param_vals = data.frame(post_rm,post_K,post_theta,post_sig_2e)\nsetwd(\"specify path here\")\nfilename = paste0(\"output\",seed, \".txt\")\nwrite.table(post_param_vals, file = filename, col.names = TRUE)\n\nTrace plot is a key diagnostic tool used in Bayesian statistics to assess the behavior and convergence of MCMC chains. So we have plotted the traceplot of posterior values obtained using Grid and Gibbs approximation using following code:\n\nfilename = paste0(\"output\",seed, \".txt\")\npost_vals = read.table(file = filename, header = TRUE)\n\n#traceplot\npar(mfrow=c(2,2))\nplot(post_vals$post_rm, type = \"l\", main = bquote(\"Trace plot of\"~ r[m]),col = \"red\")\nplot(post_vals$post_K, type = \"l\", main = bquote(\"Trace plot of\"~ K),col = \"red\")\nplot(post_vals$post_theta, type = \"l\", main = bquote(\"Trace plot of\"~ theta),col = \"red\")\nplot(post_vals$post_sig_2e, type = \"l\", main = bquote(\"Trace plot of\"~ sigma[e]^2),col = \"red\")\n\nWe have kept initial half i.e 50,000 sample values as a burn-in period for each parameter. After the burn-in period from remaining 50,000 sample posterior values, using appropriate step for thinning (using auto-correlation function, $) we have reduced the autocorrelation between the successive samples to get reliable estimate of parameters. Next we have plot the histogram of posterior distribution of all parameters using following code and depicted in the Figure 4.2.\n\ncut = 0.5 # burn-in chain (first 50,000 values as burn-in period)\n\n# for rm\nu = post_vals$post_rm[ceiling(iter*cut):iter] # after burn in period\nindex = seq(1,length(u), by = 20) # thining\npost_rm_thinning = u[index] # values after thinning\nacf(post_rm_thinning,main = bquote(\"acf of posterior \"~rm)) # autocorrelation\npost_interval_rm = quantile(post_rm_thinning, c(2.5, 97.5)/100) #credible interval for rm\nhist(post_rm_thinning, probability = TRUE,main = bquote(\"Posterior of \"~ r[m]),\nxlab = expression(r[m]),col = \"grey\", breaks = 40)\narrows(post_interval_rm[1], 0, post_interval_rm[2], 0, lwd=3, angle = 90,col = \"red\", code = 1)\narrows(post_interval_rm[2], 0, post_interval_rm[1], 0, lwd=3, angle = 90,col = \"red\", code = 1)\nlegend(\"topright\",legend = c(\"Credible Interval\"),lwd = 3, col = \"red\",lty = 1,cex = 1, bty = \"n\")\n\n# for K\nu = post_vals$post_K[ceiling(iter*cut):iter] # after burn in period\nindex = seq(1,length(u), by = 10) # thining\npost_K_thinning = u[index] # values after thinning\nacf(post_K_thinning,main = bquote(\"acf of posterior \"~K)) # autocorrelation\npost_interval_K = quantile(post_K_thinning, c(2.5, 97.5)/100) #credible interval for K\nhist(post_K_thinning, probability = TRUE,main = bquote(\"Posterior of \"~ K),\nxlab = expression(K),col = \"grey\", breaks = 40)\narrows(post_interval_K[1], 0, post_interval_K[2], 0, lwd=3, angle = 90, col = \"red\", code = 1)\narrows(post_interval_K[2], 0, post_interval_K[1], 0, lwd=3, angle = 90, col = \"red\", code = 1)\nlegend(\"topright\",legend = c(\"Credible Interval\"),lwd = 3, col = \"red\",lty = 1,cex = 1, bty = \"n\")\n\n# for theta\nu = post_vals$post_theta[ceiling(iter*cut):iter] # after burn in period\nindex = seq(1,length(u), by = 20) # thining\npost_theta_thinning = u[index] # values after thinning\nacf(post_theta_thinning,main = bquote(\"acf of posterior \"~theta)) # autocorrelation\npost_interval_theta = quantile(post_theta_thinning, c(2.5, 97.5)/100) #credible interval for theta\nhist(post_theta_thinning, probability = TRUE,main = bquote(\"Posterior of \"~ theta),xlab = expression(theta),col = \"grey\", breaks = 40)\narrows(post_interval_theta[1], 0, post_interval_theta[2], 0, lwd=3, angle = 90, col = \"red\", code = 1)\narrows(post_interval_theta[2], 0, post_interval_theta[1], 0, lwd=3, angle = 90, col = \"red\", code = 1)\nlegend(\"topright\",legend = c(\"Credible Interval\"),lwd = 3, col = \"red\",lty = 1,cex = 1, bty = \"n\")\n\n# for sigma_e^2\nu = post_vals$post_sig_2e[ceiling(iter*cut):iter] # after burn in period\nindex = seq(1,length(u), by = 1) # thining\npost_sig_2e_thinning = u[index] # values after thinning\nacf(post_sig_2e_thinning,main = bquote(\"acf of posterior \"~sigma[e]^2))\npost_interval_sig_2e = quantile(post_sig_2e_thinning, c(2.5, 97.5)/100)\n#credible interval for sig_2e\nhist(post_sig_2e_thinning, probability = TRUE,main = bquote(\"Posterior of \"~ sigma[e]^2),xlab = expression(sigma[e]^2),col = \"grey\", breaks = 40, ylim = c(0,800))\narrows(post_interval_sig_2e[1], 0, post_interval_sig_2e[2], 0, lwd=3, angle = 90, col = \"red\", code = 1)\narrows(post_interval_sig_2e[2], 0, post_interval_sig_2e[1], 0, lwd=3, angle = 90, col = \"red\", code = 1)\nlegend(\"topright\",legend = c(\"Credible Interval\"),lwd = 3, col = \"red\",lty = 1,cex = 1, bty = \"n\")\n\nFor \\(\\sigma_e^2\\), due to conjugacy the posterior density is predictable, but good approximation would depend on the estimate of the shape parameter which involves other parameters. Here we show what choice of the point estimate for \\(r_m, K\\) and \\(\\theta\\) need to be used (Figure 4.2 (d)) and we observe that the posterior median values best approximate the posterior density function of \\(\\sigma_e^2\\).\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\nFigure 4.2: Posterior distribution of the parameters of theta-logistic growth equation. From the histogram of \\(\\sigma_e^2\\), it is evident that posterior median is more appropriate choice to obtain a point estimate of \\(r_m\\), \\(K\\) and \\(\\theta\\). Blue lines indicate the 95% of credible intervals.\n\n\n\n\n\n\nKahle, David, and James Stamey. 2017. Invgamma: The Inverse Gamma Distribution. https://CRAN.R-project.org/package=invgamma."
  },
  {
    "objectID": "Bayesian connection to Statistical Regularization.html",
    "href": "Bayesian connection to Statistical Regularization.html",
    "title": "5  Bayesian connection to Statistical Regularization",
    "section": "",
    "text": "First we outline the concepts of linear regression. We denote the response variable by \\(Y\\) and the set of predictor variables by \\(X_1, X_2,\\cdots, X_p\\), \\(p\\) being the number of predictors which are also synonymously written as explanatory variables, independent variables, covariates, regressors etc. The true relationship between the response and the predictors can be approximated by a regression function \\(f\\), so that the equation\n\\[\\begin{equation}\\label{eq1}\n    Y = f(X_1, X_2,\\cdots, X_p) + \\epsilon\n\\end{equation}\\]\nis a valid statistical model for the population of interest. Usually, \\(f\\) is a fixed but unknown function of \\(X_1, X_2,\\cdots, X_p\\) which represents the systematic variation of \\(Y\\) explained by the predictors. The unexplained component, denoted by \\(\\epsilon\\), is assumed to be a random error (independent of the predictors) with mean zero. This gives a measure of discrepancy of the approximation by the function \\(f\\).\nUnder the multiple regression set up, \\(f\\) is replaced by a linear function of the predictors, represented as\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\cdots+\\beta_p X_p + \\epsilon,\\]\nwhere \\(\\beta_0\\) is the intercept term and \\(\\beta_j\\) gives the contributions of \\(X_j\\) for \\(j=1,\\cdots,n\\) in explaining the variation of \\(Y\\). For convenience, we adopt the matrix notation and represent the data set up. We have the continuous response \\(\\mathbf{Y} = \\left(y_1,\\cdots,y_n\\right)' \\in \\mathbb{R}^n\\) and the \\(n\\) data values \\(\\left(x_{ij}\\right)_{i=1}^n\\) are available on each \\(X_j\\), \\(j=1,2,\\cdots, p\\). The regression equation in matrix form written as\n\\[\\begin{equation}\\label{eq2}\n    \\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{e}\n\\end{equation}\\]\nwhere \\(\\mathbf{X}\\) is an \\(n\\times (p+1)\\) design matrix with entries in the first column being 1 (if intercept included) otherwise it is \\(n\\times p\\) order matrix with \\(x_{ij}\\) denotes the \\(i\\)th observation corresponding to the \\(j\\)th variable. \\(\\boldsymbol{\\beta}\\) be the vector of regression coefficients of order \\((p+1)\\times 1\\) and \\(\\mathbf{e} = (e_1,e_2,\\cdots,e_n)' \\in \\mathbb{R}^n\\) is the vector of errors. By using the least squares theory, the residual sum of squares,\n\\[\\mathbf{e'e} = \\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_j x_{ij}\\right)^2 =\\mathbf{RSS}(\\boldsymbol{\\beta}),\\]\nis minimized with respect to \\(\\boldsymbol{\\beta}\\). Solving the normal equations, the estimates of \\(\\boldsymbol{\\beta}\\) is obtained as \\(\\hat{\\boldsymbol{\\beta}} = \\mathbf{\\left(X'X\\right)^{-1}X'Y}\\). Using the notion of \\(l_2-\\mbox{norm}\\), one can write as\n\\[\\begin{equation}\\label{est_beta}\n    \\hat{\\boldsymbol{\\beta}} = \\mbox{argmin}_{\\boldsymbol{\\beta}}\\left(\\|\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2\\right),\n\\end{equation}\\]\nwhere \\(\\|\\mathbf{u}\\|_2^2 = \\sum_{i=1}^n u_i^2\\) for a vector \\(\\mathbf{u} \\in \\mathbb{R}^n\\). \\(\\hat{\\boldsymbol{\\beta}}\\) is an unbiased and consistent estimator of \\(\\boldsymbol{\\beta}\\). The normal equations for the above minimization problem is given by\n\\[\\mathbf{\\left(X'X\\right)}\\boldsymbol{\\beta} = \\mathbf{X'Y}.\\]\nIf \\(\\mathbf{\\left(X'X\\right)}\\) has determinant zero, then the unique solution for the system of equations can not be obtained. If the matrix is ill-conditioned that is the determinant is very small, then variance for estimated coefficients are very large making the estimates unreliable. This is a common case in many real life data sets where high degree of correlation exits between two variables. If any particular predictor can be closely approximated by a linear combination of two or more other predictors, then also the matrix \\(\\mathbf{\\left(X'X\\right)}\\) is nearly singular. Such a situation is called multicollinearity and must be taken care of before any modeling assignment.\nTo tackle the multicollinearity problem, various shrinkage methods have been proposed in the literature. Ridge regression deals with solving the normal equations of the form\n\\[\\left(\\mathbf{X'X}+\\lambda \\mathbf{I}\\right)\\boldsymbol{\\beta} = \\mathbf{X'Y},\\]\nwhere \\(\\lambda\\ge 0\\) is called the shrinkage parameter. Because of the additional parameter \\(\\lambda\\) the coefficient estimates \\(\\hat{\\boldsymbol{\\beta}}\\) has lower variance but they are no longer unbiased. Basically, instead of minimizing the residual sum of squares, ridge regression minimizes a slightly different quantity, given by \\(\\mathbf{RSS}(\\boldsymbol{\\beta})+ \\lambda \\sum_{j=1}^p \\beta_j^2\\). The coefficient estimates can be written using \\(l_2-\\mbox{norm}\\) as\n\\[\\begin{equation}\\label{est_beta_ridge}\n    \\hat{\\boldsymbol{\\beta}_{\\lambda}^R} = \\mbox{argmin}_{\\boldsymbol{\\beta}} \\left(\\|\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2\\right).\n\\end{equation}\\]\nThe term \\(\\lambda \\sum_{j=1}^p \\beta_j^2\\), referred as shrinkage penalty, is small when \\(\\beta_1,\\cdots, \\beta_p\\) are close to zero. For \\(\\lambda = 0\\), the procedure is equivalent to the ordinary least squares (OLS) regression. For \\(\\lambda \\rightarrow \\infty\\), the influence of shrinkage penalty increases and the components of \\(\\hat{\\boldsymbol{\\beta}_{\\lambda}^R}\\) will approach zero and its successful implementation is guaranteed by the bias-variance tradeoff.\nModel selection methods such as forward, backward and mixed selection give the model that involves a subset of all the predictors. These techniques can be conveniently performed using R software for statistical computing. The packages \\(\\texttt{ISLR}\\) (James et al. 2021), \\(\\texttt{leaps}\\) (Fortran code by Alan Miller 2024), \\(\\texttt{MASS}\\) (Venables and Ripley 2002) can be used for this purpose and different criteria such as \\(R^2\\), AIC etc. can be utilized for the model selection. Unlike these methods, ridge regression includes all \\(p\\) predictors in the model; the penalty term reduces many coefficients to very small values, but will not set them exactly equal to zero. This is challenging for interpretation of the results in high dimension when the number of predictors is very large. This shortcoming is overcome by the method of lasso regression by minimizing the quantity \\(\\mathbf{RSS}(\\boldsymbol{\\beta})+ \\lambda \\sum_{j=1}^p|\\beta_j|\\), which considers an \\(l_1-\\)norm penalty instead of \\(l_2-\\)norm penalty. Thus using \\(l_1-\\mbox{norm}\\) notation, the coefficient estimates can be written as\n\\[\\begin{equation}\\label{est_beta_lasso}\n    \\hat{\\boldsymbol{\\beta}_{\\lambda}^L} = \\mbox{argmin}_{\\boldsymbol{\\beta}} \\left(\\|\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_1\\right)\n\\end{equation}\\]\nwhere \\(\\|\\mathbf{u}\\|_1 = \\sum_{i=1}^n |u_i|\\) for a vector \\(\\mathbf{u} \\in \\mathbb{R}^n\\). Because of the \\(l_1\\) penalty some of the coefficients are forced to be equal to zero for large \\(\\lambda\\). Thus, like the best subset selection methods, lasso also provides variable selection method. More precisely the lasso regression falls between best subset selection method and the ridge regression method and has some nice statistical properties from both techniques.\nRidge regression has a close connection to Bayesian linear regression. Bayesian linear regression assumes that the parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) (known) to be the random variables, while at the same time considering \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) as fixed. We shall show that if we assume the the prior distribution for \\(\\boldsymbol{\\beta}\\) as follows: \\(\\beta_1, \\beta_2,\\cdots,\\beta_p\\) are independent and identically distributed according to a normal distribution of mean zero and variance \\(c\\), then the posterior distribution of \\(\\boldsymbol{\\beta}\\) is also normal and the ridge regression estimate is both the mean and the mode for \\(\\boldsymbol{\\beta}\\) under this posterior distribution. We start with the linear regression setting again so that, the exact calculations follows naturally:\n\\[\\begin{equation}\n  Y_{i} = \\beta_{0} + \\sum_{j=1}^p\\beta_{j}X_{ij}+\\epsilon_{i},~~~i = 1,2,\\cdots,n.\n\\end{equation}\\]\n\\(\\epsilon_{i}\\sim \\mathcal{N}(0,\\sigma^2),~ i=1,2,\\cdots,n\\), \\(\\hat{\\boldsymbol{\\beta}} = (\\hat{\\beta_1},\\cdots,\\hat{\\beta_p})'\\), \\(\\mbox{E}_{\\boldsymbol{\\beta}}\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta}\\), \\(\\mbox{Var}_{\\boldsymbol{\\beta}}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2 (\\mathbf{X}'\\mathbf{X})^{-1}\\), where \\(\\hat{\\beta} =(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{Y}\\), \\(\\mathbf{Y} = (y_1,y_2,\\cdots,y_n)'\\). In matrix notation, we write it as\n\\[\\mathbf{Y} = \\beta_0 \\mathbf{1}_n + \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{e},\\]\nwhere \\(\\mathbf{1}_n\\) is a vector of order \\(n\\times 1\\) whose each entry is equal to 1. Note that we do not have any prior distribution on the intercept term \\(\\beta_0\\). Since \\(\\beta_{j}\\)’s (\\(j=1,2,\\cdots,p\\)) are independent and identically distributed normal random variables with mean \\(0\\) and variance \\(c\\), the the joint probability density function can be easily written as a multivariate normal distribution as follows:\n\\[\\begin{equation}\n\\pi(\\boldsymbol{\\beta}) = \\left(\\frac{1}{\\sqrt{2\\pi c}}\\right)^p e^{-\\frac{\\boldsymbol{\\beta}'\\boldsymbol{\\beta}}{2c}}, ~~\\boldsymbol{\\beta}\\in \\mathbb{R}^p,\n\\end{equation}\\]\nand the marginal prior densities are given by \\(\\pi(\\beta_j) = \\frac{1}{\\sqrt{2\\pi c}}e^{-\\frac{\\beta_j^2}{2c}},~ j =1,2,\\cdots,n\\). Using a Hierarchical Bayesian set up the regression model can be written as\n\\[\\begin{eqnarray*}\nY_i|\\boldsymbol{\\beta} &\\sim& \\mathcal{N}\\left(\\beta_0 + \\sum_{j=1}^p\\beta_{j}x_{ij},\\sigma^2\\right), i = 1,2,\\cdots,n,~\\mbox{independent},\\\\\n\\boldsymbol{\\beta} &\\sim & \\pi(\\boldsymbol{\\beta}).\n\\end{eqnarray*}\\]\nThe posterior distribution of \\(\\boldsymbol{\\beta}\\) is given by,\n\\[\\begin{eqnarray*}\n\\pi(\\boldsymbol{\\beta}|y) = \\frac{f(\\boldsymbol{\\beta},y)}{f_{\\mathbf{Y}}(y)} = \\frac{f(y|\\boldsymbol{\\beta})\\pi(\\boldsymbol{\\beta})}{f_{\\mathbf{Y}}(y)},\n\\end{eqnarray*}\\]\nThe marginal distribution of \\(\\mathbf{Y}\\) is given by,\n\\[\\begin{eqnarray}\\label{eq:one}\nf_{\\mathbf{Y}}(y) &=& \\int_{\\mathbb{R}^p} f(y|\\boldsymbol{\\beta}) \\pi(\\boldsymbol{\\beta})\\mathrm{d}\\boldsymbol{\\beta} \\\\\n&=&\\int_{\\mathbb{R}^p}\\frac{1}{(\\sqrt{2\\pi}\\sigma)^n}e^{-\\frac{\\sum_{i=1}^n\\left(y_{i}-\\beta_0-\\sum_{j=1}^p\\beta_{j}x_{ij}\\right)^2}{2\\sigma^2}}\\frac{1}{(\\sqrt{2\\pi c})^p}e^{-\\frac{\\sum_{j=1}^p\\beta_{j}^2}{2c}} \\mathrm{d}\\boldsymbol{\\beta} \\notag \\\\\n&=&\\frac{1}{(\\sqrt{2\\pi}\\sigma)^n}\\frac{1}{(\\sqrt{2\\pi c})^p}\\int_{\\mathbb{R}^p} e^{-\\frac{1}{2}\\left[\\frac{(y-\\beta_0 \\mathbf{1}_n - \\mathbf{X}\\boldsymbol{\\beta})'(y-\\beta_0 \\mathbf{1}_n - \\mathbf{X}\\boldsymbol{\\beta})}{\\sigma^2}+\\frac{\\boldsymbol{\\beta}'\\boldsymbol{\\beta}}{c}\\right]} \\mathrm{d}\\boldsymbol{\\beta}\n\\end{eqnarray}\\] Now consider the exponent part in the previous equation and simplify it, \\[\\begin{eqnarray}\n2\\times \\mbox{Exponent}&= &\\frac{(\\mathbf{Y}-\\beta_0\\mathbf{1}_n-\\mathbf{X}\\boldsymbol{\\beta})'(\\mathbf{Y}-\\beta_0\\mathbf{1}_n-\\mathbf{X}\\boldsymbol{\\beta})}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}'\\boldsymbol{\\beta}}{c}\\\\\n&=&\\frac{(\\mathbf{Y}'-\\beta_0\\mathbf{1}_n'-\\boldsymbol{\\beta}'\\mathbf{X}')(\\mathbf{Y}-\\beta_0\\mathbf{1}_n-\\mathbf{X}\\boldsymbol{\\beta}) }{\\sigma^2} + \\frac{\\boldsymbol{\\beta}' \\boldsymbol{\\beta}}{c} \\notag \\\\\n&=& \\frac{\\mathbf{Y}'\\mathbf{Y}-2\\beta_0\\mathbf{1}_n'\\mathbf{Y}-2\\boldsymbol{\\beta}'\\mathbf{X}'\\mathbf{Y}+\\beta_0^2+2\\boldsymbol{\\beta}'\\mathbf{X}'\\beta_0\\mathbf{1}_n+\\boldsymbol{\\beta}'\\mathbf{X}'\\mathbf{X}\\boldsymbol{\\beta}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}'\\boldsymbol{\\beta}}{c} \\notag \\\\\n&=&\\frac{\\mathbf{Y}'\\mathbf{Y}}{\\sigma^2}-\\frac{2\\beta_0\\mathbf{1}_n'\\mathbf{Y}}{\\sigma^2}+\\frac{\\beta_0^2}{\\sigma^2}+\\boldsymbol{\\beta}'\\left(\\frac{\\mathbf{X}'\\mathbf{X}}{\\sigma^2}+\\frac{1}{c}\\mathbf{I}_p\\right)\\beta -2\\boldsymbol{\\beta}'\\mathbf{X}'\\left(\\frac{\\mathbf{Y}}{\\sigma^2}-\\frac{\\beta_0\\mathbf{1}_n}{\\sigma^2}\\right)\\label{eq:two}\n\\end{eqnarray}\\]\nWe write the above expression in the form \\((\\boldsymbol{\\beta}-\\mu)'\\Sigma^{-1}(\\boldsymbol{\\beta}-\\mu) +\\) constant containing term \\(\\mathbf{Y}\\) and \\(\\mathbf{X}'s\\). To do that we utilize the following expression for the purpose of matching similar terms: \\[(\\mathbf{X}-\\mu)'\\Sigma^{-1}(\\mathbf{X}-\\mu) =\\mathbf{X}'\\Sigma^{-1}\\mathbf{X}-2\\mathbf{X}'\\Sigma^{-1}\\mu+\\mu'\\Sigma^{-1}\\mu.\\] Therefore by equation \\(\\eqref{eq:one}\\)\n\\[\\begin{eqnarray}\n& &\\frac{(\\mathbf{Y}-\\beta_0\\mathbf{1}_n-\\mathbf{X}\\boldsymbol{\\beta})'(\\mathbf{Y}-\\beta_0\\mathbf{1}_n-\\mathbf{X}\\boldsymbol{\\beta})}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}'\\boldsymbol{\\beta}}{c} \\\\\n&=& \\boldsymbol{\\beta}'\\left(\\frac{\\mathbf{X}'\\mathbf{X}}{\\sigma^2}+\\frac{1}{c}\\mathbf{I}_p\\right)\\boldsymbol{\\beta}-2\\boldsymbol{\\beta}'\\left(\\frac{\\mathbf{X}'\\mathbf{X}}{\\sigma^2}+\\frac{1}{c}\\mathbf{I}_p\\right)\\left(\\frac{\\mathbf{X}'\\mathbf{X}}{\\sigma^2}+\\frac{1}{c}\\mathbf{I}_p\\right)^{-1} \\left(\\frac{\\mathbf{X}'\\mathbf{Y}}{\\sigma^2}-\\frac{\\mathbf{X}'\\beta_0\\mathbf{1}_n}{\\sigma^2}\\right)+ \\notag \\\\\n&\\ &\\left[\\left(\\frac{\\mathbf{X}'\\mathbf{X}}{\\sigma^2}+\\frac{1}{c}\\mathbf{I}_p\\right)^{-1}\\left(\\frac{\\mathbf{X}'\\mathbf{Y}}{\\sigma^2}-\\frac{\\mathbf{X}'\\beta_0\\mathbf{1}_n}{\\sigma^2}\\right)\\right]'\\left(\\frac{\\mathbf{X}'\\mathbf{X}}{\\sigma^2}+\\frac{1}{c}\\mathbf{I}_p\\right)  \\notag \\\\\n&\\ & \\left[\\left(\\frac{\\mathbf{X}'\\mathbf{X}}{\\sigma^2}+\\frac{1}{c}\\mathbf{I}_p\\right)^{-1}\\left(\\frac{\\mathbf{X}'\\mathbf{Y}}{\\sigma^2}-\\frac{\\mathbf{X}'\\beta_0\\mathbf{1}_n}{\\sigma^2}\\right)\\right] \\notag \\\\\n&\\ & -\\left[\\left(\\frac{\\mathbf{X}'\\mathbf{X}}{\\sigma^2}+\\frac{1}{c}\\mathbf{I}_p\\right)^{-1}\\left(\\frac{\\mathbf{X}'\\mathbf{Y}}{\\sigma^2}-\\frac{\\mathbf{X}'\\beta_0\\mathbf{1}_n}{\\sigma^2}\\right)\\right]'\\left(\\frac{\\mathbf{X}'\\mathbf{X}}{\\sigma^2}+\\frac{1}{c}\\mathbf{I}_p\\right)  \\notag \\\\\n&\\ & \\left[\\left(\\frac{\\mathbf{X}'\\mathbf{X}}{\\sigma^2}+\\frac{1}{c}\\mathbf{I}_p\\right)^{-1}\\left(\\frac{\\mathbf{X}'\\mathbf{Y}}{\\sigma^2}-\\frac{\\mathbf{X}'\\beta_0\\mathbf{1}_n}{\\sigma^2}\\right)\\right] \\notag \\\\\n&\\ & + \\frac{\\mathbf{Y}'\\mathbf{Y}}{\\sigma^2}-\\frac{2\\beta_0\\mathbf{1}_n'\\mathbf{Y}}{\\sigma^2}+\\frac{\\beta_0^2}{\\sigma^2}. \\label{eq:four}\n\\end{eqnarray}\\] Now, let \\[\\begin{equation*}\n\\Sigma^{-1}=\\sigma^2\\left(\\mathbf{X}'\\mathbf{X}+\\frac{\\sigma^2}{c}\\mathbf{I}_p\\right)~~~\\mbox{and}~~~\\mu = \\Sigma\\mathbf{X}'\\left(\\mathbf{Y}-\\beta_0\\mathbf{1}_n\\right),\n\\end{equation*}\\]\nthen the Exponent becomes,\n\\[\\begin{eqnarray}\n2 \\times \\mbox{Exponent}&=&\\boldsymbol{\\beta}'\\Sigma^{-1}\\boldsymbol{\\beta}-2\\boldsymbol{\\beta}'\\Sigma^{-1}\\mu + \\mu'\\Sigma^{-1}\\mu -\\mu'\\Sigma^{-1}\\mu + \\frac{\\mathbf{Y}'\\mathbf{Y}}{\\sigma^2}-\\frac{2\\beta_0\\mathbf{1}_n'\\mathbf{Y}}{\\sigma^2}+\\frac{\\beta_0^2}{\\sigma^2} \\notag \\\\\n&=&(\\boldsymbol{\\beta}-\\mu)'\\Sigma^{-1}(\\boldsymbol{\\beta}-\\mu)-\\mu'\\Sigma^{-1}\\mu + \\frac{\\mathbf{Y}'\\mathbf{Y}}{\\sigma^2}-\\frac{2\\beta_0\\mathbf{1}_n'\\mathbf{Y}}{\\sigma^2}+\\frac{\\beta_0^2}{\\sigma^2}. \\label{eq:five}\n\\end{eqnarray}\\]\nThen equation \\(\\eqref{eq:one}\\) becomes,\n\\[\\begin{eqnarray}\nf_{\\mathbf{Y}}(y) &=&\\left(\\sqrt{2\\pi}\\right)^p|\\Sigma|^{p/2}\\frac{1}{\\left(\\sqrt{2\\pi}\\sigma\\right)^n}\\frac{1}{\\left(\\sqrt{2\\pi c}\\right)^p}e^{-\\frac{\\mathbf{Y}'\\mathbf{Y}}{2\\sigma^{2}}}e^{\\frac{\\beta_0\\mathbf{1}_n'\\mathbf{Y}}{2\\sigma^{2}}}e^{\\frac{\\beta_0^2}{2\\sigma^{2}}}e^{\\frac{\\mu'\\Sigma^{-1}\\mu}{2}} \\int_{\\mathbb{R}^p}\\frac{1}{(\\sqrt{2\\pi})^p}\\frac{1}{|\\Sigma|^{p/2}}e^{-\\frac{1}{2}(\\boldsymbol{\\beta}-\\mu)'\\Sigma^{-1}(\\boldsymbol{\\beta}-\\mu)} \\mathrm{d}\\boldsymbol{\\beta} \\notag \\\\\n&=&\\left(\\sqrt{2\\pi}\\right)^p|\\Sigma|^{p/2}\\frac{1}{\\left(\\sqrt{2\\pi}\\sigma\\right)^n}\\frac{1}{\\left(\\sqrt{2\\pi c}\\right)^p}e^{-\\frac{\\mathbf{Y}'\\mathbf{Y}}{2\\sigma^{2}}}e^{\\frac{\\beta_0\\mathbf{1}_n'\\mathbf{Y}}{2\\sigma^{2}}}e^{\\frac{\\beta_0^2}{2\\sigma^{2}}}e^{\\frac{\\mu'\\Sigma^{-1}\\mu}{2}}.\\label{eq:six}\n\\end{eqnarray}\\]\nNote that the integrand is the density function of \\(\\mbox{MVN}_p(\\mu,\\Sigma)\\), hence integral out to be \\(1\\), where,\n\\[\\begin{equation*}\n\\mu = \\left(\\frac{\\mathbf{X}'\\mathbf{X}}{\\sigma^2}+\\frac{1}{c}\\mathbf{I}_p\\right)^{-1}\\mathbf{X}'\\left(\\frac{\\mathbf{Y}-\\beta_0\\mathbf{1}_n}{\\sigma^2}\\right)~~~\\mbox{and}~~~ \\Sigma = \\sigma^2\\left(\\mathbf{X}'\\mathbf{X}+\\frac{\\sigma^2}{c} \\mathbf{I}_p\\right)^{-1}.\n\\end{equation*}\\]\nThe posterior distribution of \\(\\boldsymbol{\\beta}\\) is,\n\\[\\begin{eqnarray*}\n\\phi(\\boldsymbol{\\beta}|y)&=& \\frac{f(\\boldsymbol{\\beta},y)}{f_{\\mathbf{Y}}(y)} \\\\\n&=& \\frac{f(y|\\boldsymbol{\\beta})\\pi(\\boldsymbol{\\beta})}{f_{\\mathbf{Y}}(y)} \\\\\n&=& \\frac{\\frac{1}{(\\sqrt{2\\pi}\\sigma)^n}e^{-\\frac{(\\mathbf{Y}-\\beta_0\\mathbf{1}_n-\\mathbf{X}\\boldsymbol{\\beta})'(\\mathbf{Y}-\\beta_0\\mathbf{1}_n-\\mathbf{X}\\boldsymbol{\\beta})}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}'\\beta}{c}}\\frac{1}{(\\sqrt{2\\pi c})^p}e^{-\\frac{\\boldsymbol{\\beta}' \\boldsymbol{\\beta}}{2c}}}{\\left(\\sqrt{2\\pi}\\right)^p|\\Sigma|^{\\frac{p}{2}}\\frac{1}{\\left(\\sqrt{2\\pi}\\sigma\\right)^n}\\frac{1}{\\left(\\sqrt{2\\pi c}\\right)^p}e^{-\\frac{\\mathbf{Y}'\\mathbf{Y}}{2\\sigma^{2}}}e^{\\frac{\\beta_0\\mathbf{1}_n\\mathbf{Y}}{2\\sigma^{2}}}e^{\\frac{\\beta_0^2}{2\\sigma^2}}e^{\\frac{\\mu'\\Sigma^{-1}\\mu}{2}}} \\\\\n&=&\\frac{1}{\\left(\\sqrt{2\\pi}\\right)^p}\\frac{1}{|\\Sigma|^{p/2}}e^{-\\frac{1}{2}(\\boldsymbol{\\beta}-\\mu)'\\Sigma^{-1}(\\boldsymbol{\\beta}-\\mu)}.\n\\end{eqnarray*}\\]\nSo the posterior mean of \\(\\boldsymbol{\\beta}\\) is given by\n\\[\\begin{eqnarray*}\n\\mbox{E}(\\boldsymbol{\\beta}|y)\n&=&\\hat{\\boldsymbol{\\beta}}_B \\\\\n&=&\\left(\\mathbf{X}'\\mathbf{X} + \\frac{\\sigma^2}{c}\\mathbf{I}_p\\right)^{-1}\\mathbf{X}'(\\mathbf{Y}-\\beta_0\\mathbf{1}_n) \\label{eq:seven}\n\\end{eqnarray*}\\]\nThe main utility of ridge regression is to choose the predictors. We can start with the regression model in the following form:\n\\[\\begin{eqnarray*}\n\\tilde{\\mathbf{y}} = \\mathbf{X}\\boldsymbol{\\beta} + \\epsilon\n\\end{eqnarray*}\\]\nwhere \\(\\tilde{\\mathbf{y}} = \\mathbf{Y}-\\bar{y}\\mathbf{1}_n\\) and \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_i\\). Then\n\\[\\begin{equation*}\n\\hat{\\boldsymbol{\\beta}}_B = \\left(\\mathbf{X}'\\mathbf{X} + \\frac{\\sigma^2}{c}\\mathbf{I}_p\\right)^{-1}\\mathbf{X}'\\tilde{\\mathbf{y}}.\n\\end{equation*}\\]\n\n\n\n\nFortran code by Alan Miller, Thomas Lumley based on. 2024. Leaps: Regression Subset Selection. https://CRAN.R-project.org/package=leaps.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in r. https://CRAN.R-project.org/package=ISLR.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Conclusion",
    "section": "",
    "text": "This document serves as a comprehensive guide for students and educators, aiming to foster a solid foundation in Bayesian computation without relying on pre-built packages. By encouraging learning from scratch, the material ensures that students grasp the underlying principles and mechanics of Bayesian methods, which enhances the teaching experience in a classroom setting.\nThe comparison of Maximum Likelihood Estimation (MLE) and Bayesian estimators through simulation studies in the second chapter for various examples helps students understand how well an estimator performs by accounting for both the bias (accuracy) and the variance (consistency). The simulations provide practical experience, allowing students to compare the estimators and deepen their understanding of statistical inference beyond just theory.\nThrough Bayesian regression, students gain a deeper understanding of how to incorporate prior beliefs with data and carry out the inference process. Additionally, the emphasis on ecological modeling in this document allows learners to connect theory with real-world issues. By integrating ecological assumptions with simulation-based learning, students better understand how to apply Bayesian methods to address complex environmental challenges, underscoring the significance of ecological assumptions in model development.\nIn conclusion, this document is designed to enrich classroom teaching and provide students with a robust foundation in Bayesian methods, ecological problem-solving, and the nuances of simulation studies. The goal is to cultivate a deep, conceptual understanding of these topics while reinforcing practical application through problem-solving exercises.\nThis document is not in its final format. We will update it regularly to provide a more polished version. Please check for updates and additional chapters."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bürkner, Paul-Christian. 2017. “brms:\nAn R Package for Bayesian Multilevel Models\nUsing Stan.” Journal of Statistical\nSoftware 80 (1): 1–28. https://doi.org/10.18637/jss.v080.i01.\n\n\nCasella, George, and Roger L. Berger. 2002. Statistical\nInference. Second. Duxbury Advanced Series. India Edition: Cengage\nLearning.\n\n\nFortran code by Alan Miller, Thomas Lumley based on. 2024. Leaps:\nRegression Subset Selection. https://CRAN.R-project.org/package=leaps.\n\n\nGoodrich, Ben, Jonah Gabry, Imad Ali, and Sam Brilleman. 2024.\n“Rstanarm: Bayesian Applied Regression Modeling via\nStan.” https://mc-stan.org/rstanarm/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021.\nISLR: Data for an Introduction to Statistical Learning with\nApplications in r. https://CRAN.R-project.org/package=ISLR.\n\n\nKahle, David, and James Stamey. 2017. Invgamma: The Inverse Gamma\nDistribution. https://CRAN.R-project.org/package=invgamma.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics\nwith s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nWasserman, Larry. 2004. All of Statistics: A Concise Course in\nStatistical Inference. New York: Springer-Verlag New York, Inc."
  }
]